<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ | My-Thistledown</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$).">
<meta name="author" content="Thistledown">
<link rel="canonical" href="https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-2/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://fang-lansheng.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://fang-lansheng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://fang-lansheng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://fang-lansheng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://fang-lansheng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ" />
<meta property="og:description" content="Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-02-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ"/>
<meta name="twitter:description" content="Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://fang-lansheng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ",
      "item": "https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ",
  "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ",
  "description": "Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \\cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \\cdots, n$).",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \\cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \\cdots, n$).   Hypothesis: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$  Previously: $h_\\theta(x) = \\theta_0 + \\theta_1x$ Now: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4$ For convenience of notation, define $x_0 = 1 \\ (x_0^{(i)}=1,\\ i=1, 2, \\cdots, m)$. $x = [ \\begin{matrix}x_0 \u0026 x_1 \u0026 x_2 \u0026 \\cdots \u0026 x_n \\end{matrix}]^T \\in \\mathbb{R^{n+1}}$ $\\theta = [ \\begin{matrix}\\theta_0 \u0026 \\theta_1 \u0026 \\theta_2 \u0026 \\cdots \u0026 \\theta_n \\end{matrix}]^T \\in \\mathbb{R^{n+1}}$   Multivariate Linear Regression  $$\\begin{align} h_\\theta(x)\u0026=\\theta^Tx= \\begin{bmatrix}\\theta_0 \\ \\theta_1 \\ \\theta_2 \\ \\cdots \\ \\theta_n \\end{bmatrix} \\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ \\cdots \\ x_n \\end{bmatrix} = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\end{align}$$\n5.2 Gradient Descent for Multiple Variables   Hypothesis: $\\begin{align} h_\\theta(x)\u0026=\\theta^Tx = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\end{align}$\n  Parameters: $\\theta_0, \\theta_1, \\dots, \\theta_n$ ($\\theta = \\begin{bmatrix} \\theta_0 \u0026 \\theta_1 \u0026 \\cdots \u0026 \\theta_n \\end{bmatrix}^T$)\n  Cost function: $J(\\theta_0, \\theta_1, \\dots, \\theta_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 $\n  Gradient descent:\n$$\\left.\\begin{array}{l} \\text{repeat until convergence}\\ { \\ \\qquad \\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} \\ \\qquad (\\text{simultaneously update for every } \\theta_j \\text{ for } j := 0\\dots n) \\ } \\end{array}\\right.$$\n  The following image compares gradient descent with one variable to gradient descent with multiple variables:   5.3 Gradient Descent in Practice I - Feature Scaling  Feature scaling Idea: make sure features are on similar scale. Get every feature into approximately a $-1 \\leq x_i \\leq 1$ range.   Mean normalization: replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean.  $x_i:= \\frac{x_i-u_i}{s_i}$: $u_i$ is the average of all the values for the feature $(i)$ and $s_i$ is the range of the values $(\\text{max - min})$, or $s_i$ is the standard deviation.    5.4 Gradient Descent in Practice II - Learning Rate  Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(\\theta)$, over the number of iterations of gradient descent. if $J(\\theta)$ ever increases, then you probably need to decrease $\\alpha$. Automatic convergence test. Declare convergence if $J(\\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it‚Äôs difficult to choose this threshold value. Make sure gradient descent is working correctly.  For sufficient small $\\alpha$, $J(\\theta)$ should decrease on every iteration. If $\\alpha$ is too large: may not decrease on every iteration and thus may not converge. if $\\alpha$ is too small: slow convergence.    5.5 Features and Polynomial Regression  [Example] Housing prices prediction: $h_\\theta(x)=\\theta_0 + \\theta_1 \\times frontage + \\theta_2 \\times depth$ Polynomial regression: We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).  6. Computing Parameters Analytically 6.1 Normal Equation   Normal equation: Method to solve for $\\theta$ analytically.\n $\\theta \\in \\mathbb{R^{n+1}}\\qquad J(\\theta_0,\\theta_1,\\dots,\\theta_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$ $\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\cdots=0 \\qquad (\\text{for every }j)$ Solve for $\\theta_0, \\theta_1, \\dots,\\theta_n$    $m$ examples $(x^{(1)},y^{(1)}), \\dots,(x^{(m)}, y^{(m)})$; $n$ features.\n$$x^{(i)}=\\begin{bmatrix} x_0^{(i)} \\ x_1^{(i)} \\ x_2^{(i)} \\ \\vdots \\ x_n^{(i)} \\end{bmatrix} \\in \\mathbb{R^{n+1}}, \\qquad \\mathop{x}\\limits_{\\text{(design matrix)}} = \\begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \\vdots \\ (x^{(m)})^T \\end{bmatrix}_{m\\times(n+1)}$$\n  $\\theta=(X^TX)^{-1}X^Ty$\n  The following is a comparison of gradient descent and the normal equation:\n     Gradient Descent Normal Equation     Need to choose $\\alpha$. No need to choose $\\alpha$.   Needs many iterations. Don‚Äôt need to iterate.   $\\text{O}(kn^2)$ $\\text{O}(n^3)$, Need to compute $(X^TX)^{-1}$   Works well even when $n$ is large. Slow if $n$ is very large.    6.2 Normal Equation and Noninvertibility  Normal equation: $\\theta=(X^TX)^{-1}X^Ty$. But what if $X^TX$ is non-invertible (singular/degenerate)? If $X^TX$ is noninvertible, the common causes might be having:  Redundant features, where two features are very closely related (i.e. they are linearly dependent). Too many features (e.g. $m \\leq n$). In this case, delete some features, or use regularization.    Ex1: Linear Regressionüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex1/ex1.ipynb\n Ex1.1 Linear regression with one variable Instruction: In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\nCode:\nimport numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D  \"\"\" Part 1: Basic Function \"\"\" def computeCost(X, y, theta):  ''' Compute cost for linear regression :param X: input variables :param y: output variables :param theta: parameters :return: Cost function '''  m = len(y)  J = 0  for xi, yi in zip(X, y):  J = J + np.square(float(np.dot(xi, theta)) - yi)  return J / (2 * m)  def gradientDescent(X, y, theta, alpha, num_iters):  ''' updates theta by taking num_iters gradient steps with learning rate alpha :param X: input variables :param y: output variables :param theta: parameters :param alpha: learning rate :param num_iters: times of iteration :return: [theta, J_history] '''  # Initialize some useful values  m = len(y)  J_history = np.zeros(num_iters)  for i in range(num_iters):  sum0, sum1 = 0, 0  for j in range(m):  sum0 = sum0 + (np.dot(X[j], theta) - y[j]) * X[j][0]  sum1 = sum1 + (np.dot(X[j], theta) - y[j]) * X[j][1]  theta[0] = theta[0] - alpha * sum0 / m  theta[1] = theta[1] - alpha * sum1 / m   # Save the cost J in every iteration  J_history[i] = computeCost(X, y, theta)  return theta, J_history   \"\"\" Part 2: Plotting \"\"\" print('Plotting Data ...') data = np.loadtxt(\"ex1data1.txt\", dtype=float, delimiter=',') X = data[:, 0] y = data[:, 1] m = len(y) # number of training examples  # Plot Data fig0, ax0 = plt.subplots() ax0.scatter(X, y, marker='x', label='Training data') ax0.set_xlabel('Population of City in 10,000s') ax0.set_ylabel('Profit in $10,000s')  \"\"\" Part 3: Cost and Gradient Descent \"\"\" X = np.c_[np.ones((m, 1)), data[:,0]] # Add a column of ones to X theta = np.zeros((2, 1)) # initialize fitting parameters  # Some gradient descent settings iterations = 1500 alpha = 0.01  print('\\nTesting the cost function ... \\n') # compute and display initial cost J = computeCost(X, y, theta) print('With theta = [0 ; 0], cost computed = ', J) print('Expected cost value (approx) 32.07\\n')  # further testing of the cost function J = computeCost(X, y, np.array([[-1], [2]])) print('With theta = [-1 ; 2], cost computed = ', J) print('Expected cost value (approx) 54.24\\n')  print('Running Gradient Descent ...\\n') # run gradient descent theta, J_history = gradientDescent(X, y, theta, alpha, iterations)  # print theta to screen print('Theta found by gradient descent: Œ∏0 =', theta[0], ', Œ∏1 =', theta[1]) print('Expected theta values (approx): -3.6303 1.6664\\n')  # Plot the linear fit ax0.plot(X[:, 1], np.dot(X, theta), color='r', label='Linear regression') ax0.legend(loc='lower right') ax0.set_title('Linear regression with one variable')  # Predict values for population sizes of 35,000 and 70,000 predict1 = np.dot([1, 3.5], theta) predict2 = np.dot([1, 7], theta) print('For population = 35,000, we predict a profit of ', predict1 * 10000) print('For population = 70,000, we predict a profit of ', predict2 * 10000)  \"\"\" Part 4: Visualizing J(theta_0, theta_1) \"\"\" print('\\nVisualizing J(theta_0, theta_1) ... \\n')  # Plot the reducing of cost function during iteration fig1, ax1 = plt.subplots() ax1.plot(np.arange(iterations), J_history, 'b') ax1.set_xlabel('Iterations') ax1.set_ylabel(r'$J(\\theta_0, \\theta_1)$') ax1.set_title(r'Reducing of $J(\\theta_0, \\theta_1)$ during iteration')  # Grid over which we will calculate J theta0_vals = np.linspace(-10, 10, 100) theta1_vals = np.linspace(-1, 4, 100)  # Initialize J_vals to a matrix of 0's J_vals = np.zeros([len(theta0_vals), len(theta1_vals)])  # Fill out J_vals for i in range(len(theta0_vals)):  for j in range(len(theta1_vals)):  t = [theta0_vals[i], theta1_vals[j]]  J_vals[i][j] = computeCost(X, y, t)  x_contour, y_contour = theta0_vals, theta1_vals theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)  # Produce surface and contour plots of J(Œ∏) fig2 = plt.figure() ax2 = Axes3D(fig2) ax2.plot_surface(theta0_vals, theta1_vals, J_vals.T, rstride=1, cstride=1, cmap=cm.rainbow) ax2.set_xlabel(r'$\\theta_0$') ax2.set_ylabel(r'$\\theta_1$') ax2.set_title('Surface')  fig3, ax3 = plt.subplots() CS = ax3.contour(theta0_vals, theta1_vals, J_vals.T, levels=50) ax3.plot(theta[0], theta[1], 'bx') ax3.set_xlabel(r'$\\theta_0$') ax3.set_ylabel(r'$\\theta_1$') ax3.set_title('Contour')  plt.show() Output:\n Console   Training data with linear regression fit.   Reducing of $J(\\theta_0, \\theta_1)$ during iteration   Produce surface and contour plots of $J(Œ∏)$  Ex1.2 Linear regression with multiple variables Instruction: In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\nCode:\nimport numpy as np import matplotlib.pyplot as plt  \"\"\" Part 0: Basic Function \"\"\" def computeCost(X, y, theta):  ''' Compute cost for linear regression :param X: input variables :param y: output variables :param theta: parameters :return: Cost function '''  m = len(y)  J = np.sum(np.square(X.dot(theta) - y)) / (2 * m)  return J  def gradientDescentMulti(X, y, theta, alpha, num_iters):  ''' updates theta by taking num_iters gradient steps with learning rate alpha :param X: input variables :param y: output variables :param theta: parameters :param alpha: learning rate :param num_iters: times of iteration :return: [theta, J_history] '''  # Initialize some useful values  m, n = len(y), len(theta)  J_history = np.zeros(num_iters)  for i in range(num_iters):  temp = np.dot((np.dot(X, theta) - y.reshape(m, 1)).T, X)  theta = theta - alpha * temp.T / m  J_history[i] = computeCost(X, y, theta)  return theta, J_history  def featureNormalize(X):  ''' Normalize the features in X :param X: features :return: X_norm: normalized X; mean: mean value; sigma: standard deviation '''  mean = np.mean(X, 0)  sigma = np.std(X, 0)  X_norm = (X - mean) / sigma   return X_norm, mean, sigma  def normalEqn(X, y):  ''' Computes the closed-form solution to linear regression :param X: input variables :param y: output variables :return: parameters '''  theta = np.linalg.inv(X.T@X)@X.T@y  return theta  \"\"\" Part 1: Feature Normalization \"\"\" # Load data data = np.loadtxt('ex1data2.txt', dtype=float, delimiter=',') X = data[:, 0:2] y = data[:, 2:3] m = len(y)  # Scale features and set them to zero mean X_norm, mean, sigma = featureNormalize(X) y_norm, _, _ = featureNormalize(y)  # Add intercept term to X X_norm = np.c_[np.ones([m, 1]), X_norm]  \"\"\" Part 2: Gradient Descent \"\"\" print('Running gradient descent ... ')  # Choose some alpha value alpha = 0.01 num_iters = 1000  # Init theta and run gradient descent theta = np.zeros([3, 1]) theta, J_history = gradientDescentMulti(X_norm, y, theta, alpha, num_iters)  # Plot the convergence graph fig1, ax1 = plt.subplots() ax1.plot(np.arange(num_iters), J_history, 'b') ax1.set_xlabel('Iterations') ax1.set_ylabel(r'$J(\\theta)$') ax1.set_title(r'Convergence of $J(\\theta)$')  # Display gradient descent's result print('Theta computed from gradient descent: \\n', theta)  \"\"\" Part 3: Normal Equations \"\"\" print('\\nSolving with normal equations ... ')  # Add intercept term to X X = np.c_[np.ones([m, 1]), X]  # Calculate the parameters from the normal equation theta1 = normalEqn(X, y) print('Theta computed from normal equations: \\n', theta1)  plt.show() Output:\n Console (There is some differences between the two results)   The convergence graph  ",
  "wordCount" : "1828",
  "inLanguage": "en",
  "datePublished": "2020-02-28T00:00:00Z",
  "dateModified": "2020-02-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thistledown"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My-Thistledown",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fang-lansheng.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://fang-lansheng.github.io" accesskey="h" title="My-Thistledown (Alt + H)">My-Thistledown</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ
    </h1>
    <div class="post-meta"><span title='2020-02-28 00:00:00 +0000 UTC'>February 28, 2020</span>&nbsp;¬∑&nbsp;Thistledown

</div>
  </header> 
  <div class="post-content"><blockquote>
<p>Course Link Ôºö<a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2 - Linear Regression with Multiple Variables</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="5-multivariate-linear-regression">5. Multivariate Linear Regression<a hidden class="anchor" aria-hidden="true" href="#5-multivariate-linear-regression">#</a></h2>
<h4 id="51-multiple-features">5.1 Multiple Features<a hidden class="anchor" aria-hidden="true" href="#51-multiple-features">#</a></h4>
<ul>
<li>Training set:
<!-- raw HTML omitted --></li>
<li>Notation:
<ul>
<li>$m$ = the number of training examples</li>
<li>$n$ = number of features</li>
<li>$x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$).</li>
<li>$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$).</li>
</ul>
</li>
<li>Hypothesis: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$
<ul>
<li>Previously: $h_\theta(x) = \theta_0 + \theta_1x$</li>
<li>Now: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4$</li>
<li>For convenience of notation, define $x_0 = 1 \ (x_0^{(i)}=1,\ i=1, 2, \cdots, m)$.</li>
<li>$x = [ \begin{matrix}x_0 &amp; x_1 &amp; x_2 &amp; \cdots &amp; x_n \end{matrix}]^T \in \mathbb{R^{n+1}}$</li>
<li>$\theta = [ \begin{matrix}\theta_0 &amp; \theta_1 &amp; \theta_2 &amp; \cdots &amp; \theta_n \end{matrix}]^T \in \mathbb{R^{n+1}}$</li>
</ul>
</li>
<li><!-- raw HTML omitted -->Multivariate Linear Regression<!-- raw HTML omitted --></li>
</ul>
<p>$$\begin{align}
h_\theta(x)&amp;=\theta^Tx= \begin{bmatrix}\theta_0 \ \theta_1 \ \theta_2 \ \cdots \ \theta_n \end{bmatrix} \begin{bmatrix}x_0 \ x_1 \ x_2 \ \cdots \ x_n \end{bmatrix}
= \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
\end{align}$$</p>
<h4 id="52-gradient-descent-for-multiple-variables">5.2 Gradient Descent for Multiple Variables<a hidden class="anchor" aria-hidden="true" href="#52-gradient-descent-for-multiple-variables">#</a></h4>
<ul>
<li>
<p>Hypothesis: $\begin{align}
h_\theta(x)&amp;=\theta^Tx
= \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
\end{align}$</p>
</li>
<li>
<p>Parameters: $\theta_0, \theta_1, \dots, \theta_n$  ($\theta = \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n \end{bmatrix}^T$)</p>
</li>
<li>
<p>Cost function: $J(\theta_0, \theta_1, \dots, \theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 $</p>
</li>
<li>
<p>Gradient descent:</p>
<p>$$\left.\begin{array}{l} \text{repeat until convergence}\ { \ \qquad \theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \ \qquad (\text{simultaneously update for every } \theta_j \text{ for } j := 0\dots n) \ } \end{array}\right.$$</p>
</li>
<li>
<p>The following image compares gradient descent with one variable to gradient descent with multiple variables:
<img loading="lazy" src="https://i.loli.net/2020/02/29/tJXVdIn4EsCbDkf.png" alt="image.png"  />
</p>
</li>
</ul>
<h4 id="53-gradient-descent-in-practice-i---feature-scaling">5.3 Gradient Descent in Practice I - Feature Scaling<a hidden class="anchor" aria-hidden="true" href="#53-gradient-descent-in-practice-i---feature-scaling">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Feature scaling<!-- raw HTML omitted -->
<ul>
<li>Idea: make sure features are on similar scale.</li>
<li>Get every feature into approximately a $-1 \leq x_i \leq 1$ range.</li>
</ul>
</li>
<li><!-- raw HTML omitted -->Mean normalization<!-- raw HTML omitted -->: replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean.
<ul>
<li>$x_i:= \frac{x_i-u_i}{s_i}$: $u_i$ is the <em>average</em> of all the values for the feature $(i)$ and $s_i$ is the range of the values $(\text{max - min})$, or $s_i$ is the standard deviation.</li>
</ul>
</li>
</ul>
<h4 id="54-gradient-descent-in-practice-ii---learning-rate">5.4 Gradient Descent in Practice II - Learning Rate<a hidden class="anchor" aria-hidden="true" href="#54-gradient-descent-in-practice-ii---learning-rate">#</a></h4>
<ul>
<li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$, over the number of iterations of gradient descent. if $J(\theta)$ ever increases, then you probably need to decrease $\alpha$.</li>
<li><strong>Automatic convergence test.</strong> Declare convergence if $J(\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it&rsquo;s difficult to choose this threshold value.</li>
<li><strong>Make sure gradient descent is working correctly.</strong>
<ul>
<li>For sufficient small $\alpha$, $J(\theta)$ should decrease on every iteration.</li>
<li>If $\alpha$ is too large: may not decrease on every iteration and thus may not converge.</li>
<li>if $\alpha$ is too small: slow convergence.</li>
</ul>
</li>
</ul>
<h4 id="55-features-and-polynomial-regression">5.5 Features and Polynomial Regression<a hidden class="anchor" aria-hidden="true" href="#55-features-and-polynomial-regression">#</a></h4>
<ul>
<li>[<em>Example</em>] <strong>Housing prices prediction</strong>: $h_\theta(x)=\theta_0 + \theta_1 \times frontage + \theta_2 \times depth$</li>
<li><strong>Polynomial regression</strong>: We can <em>change the behavior or curve</em> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</li>
</ul>
<h2 id="6-computing-parameters-analytically">6. Computing Parameters Analytically<a hidden class="anchor" aria-hidden="true" href="#6-computing-parameters-analytically">#</a></h2>
<h4 id="61-normal-equation">6.1 Normal Equation<a hidden class="anchor" aria-hidden="true" href="#61-normal-equation">#</a></h4>
<ul>
<li>
<p><!-- raw HTML omitted -->Normal equation<!-- raw HTML omitted -->: Method to solve for $\theta$ analytically.</p>
<ul>
<li>$\theta \in \mathbb{R^{n+1}}\qquad J(\theta_0,\theta_1,\dots,\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$</li>
<li>$\frac{\partial}{\partial\theta_j}J(\theta)=\cdots=0 \qquad (\text{for every }j)$</li>
<li>Solve for $\theta_0, \theta_1, \dots,\theta_n$</li>
</ul>
</li>
<li>
<p>$m$ examples $(x^{(1)},y^{(1)}), \dots,(x^{(m)}, y^{(m)})$; $n$ features.</p>
<p>$$x^{(i)}=\begin{bmatrix} x_0^{(i)} \ x_1^{(i)} \ x_2^{(i)} \ \vdots \ x_n^{(i)}  \end{bmatrix} \in \mathbb{R^{n+1}}, \qquad  \mathop{x}\limits_{\text{(design matrix)}} = \begin{bmatrix} (x^{(1)})^T \ (x^{(2)})^T \ \vdots \  (x^{(m)})^T \end{bmatrix}_{m\times(n+1)}$$</p>
</li>
<li>
<p>$\theta=(X^TX)^{-1}X^Ty$</p>
</li>
<li>
<p>The following is a comparison of gradient descent and the normal equation:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><!-- raw HTML omitted -->Gradient Descent<!-- raw HTML omitted --></th>
<th><!-- raw HTML omitted -->Normal Equation<!-- raw HTML omitted --></th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose $\alpha$.</td>
<td>No need to choose $\alpha$.</td>
</tr>
<tr>
<td>Needs many iterations.</td>
<td>Don&rsquo;t need to iterate.</td>
</tr>
<tr>
<td>$\text{O}(kn^2)$</td>
<td>$\text{O}(n^3)$, Need to compute $(X^TX)^{-1}$</td>
</tr>
<tr>
<td>Works well even when $n$ is large.</td>
<td>Slow if $n$ is very large.</td>
</tr>
</tbody>
</table>
<h4 id="62-normal-equation-and-noninvertibility">6.2 Normal Equation and Noninvertibility<a hidden class="anchor" aria-hidden="true" href="#62-normal-equation-and-noninvertibility">#</a></h4>
<ul>
<li>Normal equation: $\theta=(X^TX)^{-1}X^Ty$. But what if $X^TX$ is non-invertible (singular/degenerate)?</li>
<li>If $X^TX$ is <em>noninvertible</em>, the common causes might be having:
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent).</li>
<li>Too many features (e.g. $m \leq n$). In this case, delete some features, or use regularization.</li>
</ul>
</li>
</ul>
<h2 id="ex1-linear-regression">Ex1: Linear Regressionüë®‚Äçüíª<a hidden class="anchor" aria-hidden="true" href="#ex1-linear-regression">#</a></h2>
<blockquote>
<p>See this exercise on <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python/blob/master/ex1/ex1.ipynb">Coursera-MachineLearning-Python/ex1/ex1.ipynb</a></p>
</blockquote>
<h4 id="ex11-linear-regression-with-one-variable">Ex1.1 Linear regression with one variable<a hidden class="anchor" aria-hidden="true" href="#ex11-linear-regression-with-one-variable">#</a></h4>
<p><strong>Instruction:</strong>
In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> cm
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> mpl_toolkits.mplot3d <span style="color:#f92672">import</span> Axes3D
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 1: Basic Function &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">computeCost</span>(X, y, theta):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Compute cost for linear regression
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param X: input variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param y: output variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param theta: parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :return: Cost function
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>    J <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> xi, yi <span style="color:#f92672">in</span> zip(X, y):
</span></span><span style="display:flex;"><span>        J <span style="color:#f92672">=</span> J <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>square(float(np<span style="color:#f92672">.</span>dot(xi, theta)) <span style="color:#f92672">-</span> yi)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> J <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> m)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradientDescent</span>(X, y, theta, alpha, num_iters):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    updates theta by taking num_iters gradient steps with learning rate alpha
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param X: input variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param y: output variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param theta: parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param alpha: learning rate
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param num_iters: times of iteration
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :return: [theta, J_history]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Initialize some useful values</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>    J_history <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(num_iters)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_iters):
</span></span><span style="display:flex;"><span>        sum0, sum1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(m):
</span></span><span style="display:flex;"><span>            sum0 <span style="color:#f92672">=</span> sum0 <span style="color:#f92672">+</span> (np<span style="color:#f92672">.</span>dot(X[j], theta) <span style="color:#f92672">-</span> y[j]) <span style="color:#f92672">*</span> X[j][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>            sum1 <span style="color:#f92672">=</span> sum1 <span style="color:#f92672">+</span> (np<span style="color:#f92672">.</span>dot(X[j], theta) <span style="color:#f92672">-</span> y[j]) <span style="color:#f92672">*</span> X[j][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        theta[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> theta[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> alpha <span style="color:#f92672">*</span> sum0 <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>        theta[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> theta[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> alpha <span style="color:#f92672">*</span> sum1 <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Save the cost J in every iteration</span>
</span></span><span style="display:flex;"><span>        J_history[i] <span style="color:#f92672">=</span> computeCost(X, y, theta)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> theta, J_history
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 2: Plotting &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Plotting Data ...&#39;</span>)
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#34;ex1data1.txt&#34;</span>, dtype<span style="color:#f92672">=</span>float, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>)
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> len(y)      <span style="color:#75715e"># number of training examples</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot Data</span>
</span></span><span style="display:flex;"><span>fig0, ax0 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>scatter(X, y, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Training data&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Population of City in 10,000s&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Profit in $10,000s&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 3: Cost and Gradient Descent &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[np<span style="color:#f92672">.</span>ones((m, <span style="color:#ae81ff">1</span>)), data[:,<span style="color:#ae81ff">0</span>]]   <span style="color:#75715e"># Add a column of ones to X</span>
</span></span><span style="display:flex;"><span>theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>))                <span style="color:#75715e"># initialize fitting parameters</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Some gradient descent settings</span>
</span></span><span style="display:flex;"><span>iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">1500</span>
</span></span><span style="display:flex;"><span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Testing the cost function ... </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># compute and display initial cost</span>
</span></span><span style="display:flex;"><span>J <span style="color:#f92672">=</span> computeCost(X, y, theta)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;With theta = [0 ; 0], cost computed = &#39;</span>, J)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost value (approx) 32.07</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># further testing of the cost function</span>
</span></span><span style="display:flex;"><span>J <span style="color:#f92672">=</span> computeCost(X, y, np<span style="color:#f92672">.</span>array([[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">2</span>]]))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;With theta = [-1 ; 2], cost computed = &#39;</span>, J)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost value (approx) 54.24</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Running Gradient Descent ...</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># run gradient descent</span>
</span></span><span style="display:flex;"><span>theta, J_history <span style="color:#f92672">=</span> gradientDescent(X, y, theta, alpha, iterations)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print theta to screen</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Theta found by gradient descent: Œ∏0 =&#39;</span>, theta[<span style="color:#ae81ff">0</span>], <span style="color:#e6db74">&#39;, Œ∏1 =&#39;</span>, theta[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected theta values (approx): -3.6303  1.6664</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the linear fit</span>
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>plot(X[:, <span style="color:#ae81ff">1</span>], np<span style="color:#f92672">.</span>dot(X, theta), color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Linear regression&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lower right&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Linear regression with one variable&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict values for population sizes of 35,000 and 70,000</span>
</span></span><span style="display:flex;"><span>predict1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3.5</span>], theta)
</span></span><span style="display:flex;"><span>predict2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>], theta)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;For population = 35,000, we predict a profit of &#39;</span>, predict1 <span style="color:#f92672">*</span> <span style="color:#ae81ff">10000</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;For population = 70,000, we predict a profit of &#39;</span>, predict2 <span style="color:#f92672">*</span> <span style="color:#ae81ff">10000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 4: Visualizing J(theta_0, theta_1) &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Visualizing J(theta_0, theta_1) ... </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the reducing of cost function during iteration</span>
</span></span><span style="display:flex;"><span>fig1, ax1 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>plot(np<span style="color:#f92672">.</span>arange(iterations), J_history, <span style="color:#e6db74">&#39;b&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Iterations&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$J(\theta_0, \theta_1)$&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;Reducing of $J(\theta_0, \theta_1)$ during iteration&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Grid over which we will calculate J</span>
</span></span><span style="display:flex;"><span>theta0_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>theta1_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize J_vals to a matrix of 0&#39;s</span>
</span></span><span style="display:flex;"><span>J_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros([len(theta0_vals), len(theta1_vals)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fill out J_vals</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(theta0_vals)):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(len(theta1_vals)):
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> [theta0_vals[i], theta1_vals[j]]
</span></span><span style="display:flex;"><span>        J_vals[i][j] <span style="color:#f92672">=</span> computeCost(X, y, t)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x_contour, y_contour <span style="color:#f92672">=</span> theta0_vals, theta1_vals
</span></span><span style="display:flex;"><span>theta0_vals, theta1_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(theta0_vals, theta1_vals)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Produce surface and contour plots of J(Œ∏)</span>
</span></span><span style="display:flex;"><span>fig2 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
</span></span><span style="display:flex;"><span>ax2 <span style="color:#f92672">=</span> Axes3D(fig2)
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>plot_surface(theta0_vals, theta1_vals, J_vals<span style="color:#f92672">.</span>T, rstride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, cstride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, cmap<span style="color:#f92672">=</span>cm<span style="color:#f92672">.</span>rainbow)
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta_0$&#39;</span>)
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta_1$&#39;</span>)
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Surface&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig3, ax3 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>CS <span style="color:#f92672">=</span> ax3<span style="color:#f92672">.</span>contour(theta0_vals, theta1_vals, J_vals<span style="color:#f92672">.</span>T, levels<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>ax3<span style="color:#f92672">.</span>plot(theta[<span style="color:#ae81ff">0</span>], theta[<span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#39;bx&#39;</span>)
</span></span><span style="display:flex;"><span>ax3<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta_0$&#39;</span>)
</span></span><span style="display:flex;"><span>ax3<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta_1$&#39;</span>)
</span></span><span style="display:flex;"><span>ax3<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Contour&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/uUr8wWJka7mXcHd.png" alt="image.png"  />
</p>
<ul>
<li>Training data with linear regression fit.</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/3SVAbhH5y78zXkc.png" alt="image.png"  />
</p>
<ul>
<li>Reducing of $J(\theta_0, \theta_1)$ during iteration</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/VWOMoKai5QcCAsn.png" alt="Figure_2.png"  />
</p>
<ul>
<li>Produce surface and contour plots of $J(Œ∏)$</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/3HfaXsZnPyuelWr.png" alt="Figure_3.png"  />
</p>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/wXyKdAUvBaHTWRY.png" alt="Figure_4.png"  />
</p>
<h4 id="ex12-linear-regression-with-multiple-variables">Ex1.2 Linear regression with multiple variables<a hidden class="anchor" aria-hidden="true" href="#ex12-linear-regression-with-multiple-variables">#</a></h4>
<p><strong>Instruction:</strong>
In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 0: Basic Function &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">computeCost</span>(X, y, theta):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Compute cost for linear regression
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param X: input variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param y: output variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param theta: parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :return: Cost function
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>    J <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>square(X<span style="color:#f92672">.</span>dot(theta) <span style="color:#f92672">-</span> y)) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> m)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> J
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradientDescentMulti</span>(X, y, theta, alpha, num_iters):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    updates theta by taking num_iters gradient steps with learning rate alpha
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param X: input variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param y: output variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param theta: parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param alpha: learning rate
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param num_iters: times of iteration
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :return: [theta, J_history]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Initialize some useful values</span>
</span></span><span style="display:flex;"><span>    m, n <span style="color:#f92672">=</span> len(y), len(theta)
</span></span><span style="display:flex;"><span>    J_history <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(num_iters)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_iters):
</span></span><span style="display:flex;"><span>        temp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot((np<span style="color:#f92672">.</span>dot(X, theta) <span style="color:#f92672">-</span> y<span style="color:#f92672">.</span>reshape(m, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>T, X)
</span></span><span style="display:flex;"><span>        theta <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> alpha <span style="color:#f92672">*</span> temp<span style="color:#f92672">.</span>T <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>        J_history[i] <span style="color:#f92672">=</span> computeCost(X, y, theta)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> theta, J_history
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">featureNormalize</span>(X):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Normalize the features in X
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param X: features
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :return: X_norm: normalized X; mean: mean value; sigma: standard deviation
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(X, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    sigma <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>std(X, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    X_norm <span style="color:#f92672">=</span> (X <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> sigma
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> X_norm, mean, sigma
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalEqn</span>(X, y):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Computes the closed-form solution to linear regression
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param X: input variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param y: output variables
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :return: parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(X<span style="color:#f92672">.</span>T<span style="color:#a6e22e">@X</span>)<span style="color:#a6e22e">@X</span><span style="color:#f92672">.</span>T<span style="color:#a6e22e">@y</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> theta
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 1: Feature Normalization &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load data</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;ex1data2.txt&#39;</span>, dtype<span style="color:#f92672">=</span>float, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>)
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Scale features and set them to zero mean</span>
</span></span><span style="display:flex;"><span>X_norm, mean, sigma <span style="color:#f92672">=</span> featureNormalize(X)
</span></span><span style="display:flex;"><span>y_norm, _, _ <span style="color:#f92672">=</span> featureNormalize(y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add intercept term to X</span>
</span></span><span style="display:flex;"><span>X_norm <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[np<span style="color:#f92672">.</span>ones([m, <span style="color:#ae81ff">1</span>]), X_norm]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 2: Gradient Descent &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Running gradient descent ... &#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Choose some alpha value</span>
</span></span><span style="display:flex;"><span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>num_iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Init theta and run gradient descent</span>
</span></span><span style="display:flex;"><span>theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>theta, J_history <span style="color:#f92672">=</span> gradientDescentMulti(X_norm, y, theta, alpha, num_iters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the convergence graph</span>
</span></span><span style="display:flex;"><span>fig1, ax1 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>plot(np<span style="color:#f92672">.</span>arange(num_iters), J_history, <span style="color:#e6db74">&#39;b&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Iterations&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$J(\theta)$&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;Convergence of $J(\theta)$&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display gradient descent&#39;s result</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Theta computed from gradient descent: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, theta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34; Part 3: Normal Equations &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Solving with normal equations ... &#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add intercept term to X</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[np<span style="color:#f92672">.</span>ones([m, <span style="color:#ae81ff">1</span>]), X]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate the parameters from the normal equation</span>
</span></span><span style="display:flex;"><span>theta1 <span style="color:#f92672">=</span> normalEqn(X, y)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Theta computed from normal equations: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, theta1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console (There is some differences between the two results)</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/04/HTaInqFUc7e9hPu.png" alt="image.png"  />
</p>
<ul>
<li>The convergence graph</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/04/EDNHosUvmA8kw13.png" alt="Figure_5.png"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://fang-lansheng.github.io/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/python/">Python</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://fang-lansheng.github.io">My-Thistledown</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
