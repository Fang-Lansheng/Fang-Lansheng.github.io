<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ | My-Thistledown</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ÔºöWeek8 - Unsupervised Learning &amp; Dimensionality Reduction
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning &amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook &ldquo;smartlists&rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24.">
<meta name="author" content="Thistledown">
<link rel="canonical" href="https://fang-lansheng.github.io/posts/2020-06-22-ml-ng-8/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://fang-lansheng.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://fang-lansheng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://fang-lansheng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://fang-lansheng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://fang-lansheng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ" />
<meta property="og:description" content="Course Link ÔºöWeek8 - Unsupervised Learning &amp; Dimensionality Reduction
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning &amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook &ldquo;smartlists&rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fang-lansheng.github.io/posts/2020-06-22-ml-ng-8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-22T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-06-22T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ"/>
<meta name="twitter:description" content="Course Link ÔºöWeek8 - Unsupervised Learning &amp; Dimensionality Reduction
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning &amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook &ldquo;smartlists&rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://fang-lansheng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ",
      "item": "https://fang-lansheng.github.io/posts/2020-06-22-ml-ng-8/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ",
  "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ",
  "description": "Course Link ÔºöWeek8 - Unsupervised Learning \u0026amp; Dimensionality Reduction\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning \u0026amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook \u0026ldquo;smartlists\u0026rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24.",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ÔºöWeek8 - Unsupervised Learning \u0026 Dimensionality Reduction\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning \u0026 unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook ‚Äúsmartlists‚Äù Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24.2 K-Means Algorithm  Algorithm overview    Randomly allocate two points as the cluster centroids   Have as many cluster centroids as clusters you want to do ($K$ cluster centroids, in fact) In our example we just have two clusters, so there‚Äôre two cluster centroids (red and blue point)    Cluster assignment step   Go through each example and depending on if it‚Äôs closer the red or blue centroid assign each point to one of the two clusters To demonstrate this, we‚Äôve gone through the data and ‚Äúcolor‚Äù each point red or blue    Move centroid step   Take each centroid and move to the average of the correspondingly assigned data-points    Repeat 2) and 3) until convergence     More formal definition  Input:  $K$: number of clusters in the data Training set ${x^{(1)}, x^{(n)}, \\dots,x^{(m)}}$, $x^{(i)} \\in \\Bbb{R}^n$ (drop $x_0 = 1$ convention)   Algorithm:  Randomly initialize $K$ cluster centroids as ${\\mu_1, \\mu_2, \\dots, u_K \\in \\Bbb{R}^n }$      K-means for non-separated clusters  24.3 Optimization Objective   Notations:\n $c^{(i)}$ = index of cluster ($1, 2, \\dots, K$) to which example $x^{(i)}$ is currently assigned  $c^{(i)} = k \\in {1, 2, \\dots, K}$ $i \\in {1, 2, \\dots, m}$   $\\mu_k$ = cluster centroid k ($\\mu_k \\in \\Bbb{R}^n$) $\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned    Optimization objective\n  $$ J(c^{(1)}, \\dots, c^{(m)}, \\mu_1, \\dots, \\mu_K) = \\frac{1}{m} \\sum_{i=1}^m |x^{(i)} - \\mu_{c^{(i)}} |^2 $$\n  $$ \\mathop{\\mathop{\\text{min}}\\limits{c^{(1)}, \\dots, c^{(m)}}}\\limits{\\mu_1, \\dots, \\mu_K} J(c^{(1)}, \\dots, c^{(m)},\\mu_1, \\dots, \\mu_K) $$\n    If we consider the K-means algorithm\n The cluster assigned step is minimizing $J(\\cdot)$ with respect to $c^{(i)}$  i.e. find the centroid closet to each example Doesn‚Äôt change the centroids themselves   The move centroid step  This step is choosing the values of $\\mu$ which minimizes $J(\\cdot)$ with respect to $\\mu_k$   So, we‚Äôre partitioning the algorithm into two parts:  First part minimizes the $c$ variables Second part minimizes the $J$ variables      24.4 Random Initialization  Have number of centroids set to less than number of examples ($K Randomly pick $K$ training examples Set $\\mu_1$ up to $\\mu_K$ to these examples values   K-means can converge to different solutions depending on the initialization setup  Risk of local optimum We can do multiple random initializations to see if we get the same result - many same result are likely to indicate a global optimum    24.5 Choosing the Number of Clusters  What is right value of $K$ ? Elbow method Vary $K$ and compute cost function at a range of $K$ values As $K$ increases, $J(\\cdot)$ minimum value should decrease (i.e. you decrease the granularity so centroids can better optimize) Plot $K$ vs. $J(\\cdot)$  Chose the ‚Äúelbow‚Äù number of clusters (if you get a nice plot this is a reasonable way of choosing $K$) Risks: normally you don‚Äôt get a a nice line ‚Üí no clear elbow on curve   Another method for choosing $K$  Sometimes, you‚Äôre, running K-¬≠means to get clusters to use for some later/downstream purpose. Evaluate K¬≠‚Äêmeans based on a metric for how well it performs for that later purpose.    25 Motivation 25.1 Motivation I: Data Compression  A second type of unsupervised learning algorithm: dimensionality reduction Data compression Speeds up algorithms Reduces space used by data for them    25.2 Motivation II: Visualization  It‚Äôs hard to visualize highly dimensional data  Dimensionality reduction can improve how we display information in a tractable manner for human consumption Why do we care?  Often helps to develop algorithms if we can understand our data better Dimensionality reduction helps us do this, see data in a helpful manner Good for explaining something to someone if you can ‚Äúshow‚Äù it in the data      26 Principal Component Analysis 26.1 Principal Component Analysis Problem Formulation   For the problem of dimensionality reduction the most commonly used algorithm is PCA  Say we have a 2D data set which we wish to reduce to 1D\n  In other words, find a single line onto which to project this data. How do we determine this line?\n  The distance between each point and the projected version should be small (blue lines below are short)\n  PCA tries to find a lower dimensional surface so the sum of squares onto that surface is minimized\n  The blue lines are sometimes called the projection error\n PCA tries to find the surface (a straight line in this case) which has the minimum projection error    As an aside, you should normally do mean normalization and feature scaling on your data before PCA\n      A more formal description is\n For 2D-1D, we must find a vector $u^{(1)}$, which is of some dimensionality Onto which you can project the data so as to minimize the projection error  $u^{(1)}$ can be positive or negative ($-u^{(1)}$) which makes no difference  Each of the vectors define the same red line      In the more general case, To reduce from $n$-D to $k$-D we\n Find $k$ vectors ($u^{(1)} , u^{(2)}, \\dots, u^{(k)}$) onto which to project the data to minimize the projection error So lots of vectors onto which we project the data Find a set of vectors which we project the data onto the linear subspace spanned by that set of vectors  We can define a point in a plane with $k$ vectors   e.g. 3D ‚Üí 2D  Find pair of vectors which define a 2D plane (surface) onto which you‚Äôre going to project your data Much like the ‚Äúshallow box‚Äù example in compression, we‚Äôre trying to create the shallowest box possible (by defining two of it‚Äôs three dimensions, so the box‚Äô depth is minimized)       PCA is NOT linear regression  For linear regression, fitting a straight line to minimize the straight line between a point and a squared line  Note: VERTICAL distance between point   For PCA minimizing the magnitude of the shortest orthogonal distance  Gives very different effects   More generally  With linear regression we‚Äôre trying to predict ‚Äú$y$‚Äù With PCA there is no ‚Äú$y$‚Äù - instead we have a list of features and all features are treated equally  If we have 3D dimensional data 3D-2D  Have 3 features treated symmetrically          26.2 Principal Component Analysis Algorithm   Before applying PCA must do data preprocessing\n Mean normalization Feature scaling (depending on data)    With preprocessing done, PCA finds the lower dimensional sub-space which minimizes the sum of the square  Need to compute two things: $\\mu$ vectors \u0026 $z$ vectors    Algorithm description\n  Reducing data from $n$-dimensional to $k$-dimensional\n  Compute the covariance matrix: $\\Sigma = \\frac{1}{m}\\sum_{i=1}^n \\left(x^{(i)} \\right)\\left(x^{(i)} \\right)^T$\n $\\Sigma$: This is commonly denoted as Œ£ (Greek upper case sigma) - NOT summation symbol $\\Sigma = \\frac{1}{m} x^Tx$  $\\Sigma$ is an $n \\times n$ matrix $x^{(i)}$ is an $n \\times 1$ matrix      Compute eigenvectors of matrix $\\Sigma$: [U, S, V] = svd(sigma)\n svd = singular value decomposition    $U$, $S$ and $V$ are matrices\n  $U$ matrix is also an $n \\times n$ matrix\n  Turns out the columns of $U$ are the $u$ vectors we want\n  So to reduce a system from $n$-D to $k$-D: just take the first $k$ vectors from $U$: $$ U=\\left[\\begin{array}{cccc} | \u0026 | \u0026 \u0026 | \\ u^{(1)} \u0026 u^{(2)} \u0026 \\ldots \u0026 u^{(n)} \\ | \u0026 | \u0026 \u0026 | \\end{array}\\right] \\in \\mathbb{R}^{n \\times n} $$\n      Next we need to find some way to change $x$ (which is $n$ dimensional) to $z$ (which is $k$ dimensional) - reduce the dimensionality\n  Take first $k$ columns of the $u$ matrix and stack in columns: $$ U_{\\text{reduce}}=\\left[\\begin{array}{cccc} | \u0026 | \u0026 \u0026 | \\ u^{(1)} \u0026 u^{(2)} \u0026 \\ldots \u0026 u^{(k)} \\ | \u0026 | \u0026 \u0026 | \\end{array}\\right] \\in \\mathbb{R}^{n \\times k} $$\n  $z = U_{\\text{reduce}}^T * x=[k \\times n] * [n \\times 1] = [k \\times 1]$\n    Exactly the same as with supervised learning except we‚Äôre now doing it with unlabeled data\n  So in summary\n Preprocessing Calculate $\\Sigma$ (covariance matrix) Calculate eigenvectors with svd Take $k$ vectors from $U$, i.e. $U_{\\text{reduce}} = U(:,1:k)$ Calculate $z$ ($z =U_{\\text{reduce}}^T * x$)      27 Applying PCA 27.1 Reconstruction from compressed Representation   Earlier spoke about PCA as a compression algorithm\n If this is the case, is there a way to decompress the data from low dimensionality back to a higher dimensionality format?    Reconstruction\n  Say we have an example as follows:\n  We have our examples ($x^{(1)}$, $x^{(2)}$ etc.)\n  Project onto $z$-surface\n  Given a point $z^{(1)}$, how can we go back to the 2D space?\n    Considering\n $z = U_{\\text{reduce}}^T * x$    To go in the opposite direction we must do\n $x_{\\text{approx}} = U_{\\text{reduce}} * z,(=[n\\times k] * [k * 1] = [n \\times 1])$    So this creates the following representation\n  We lose some of the information (i.e. everything is now perfectly on that line) but it is now projected into 2D space\n  27.2 Choosing the Number of Principal Components   PCA tries to minimize average squared projection error: $$ \\frac{1}{m} \\sum_{i=1}^m | x^{(i)} - x^{(i)}_{\\text{approx}} |^2 $$\n  Total variationin data can be defined as the average over data saying how far are the training examples from the origin: $$ \\frac{1}{m} \\sum_{i=1}^m | x^{(i)} |^2 $$\n  Typically, choose $k$ to be smallest value so that $$ \\frac{\\frac{1}{m} \\sum_{i=1}^m | x^{(i)} - x^{(i)}{\\text{approx}} |^2}{\\frac{1}{m} \\sum{i=1}^m | x^{(i)} |^2} \\le 0.01 , (=1%) $$\n means ‚Äú99% of variance is retained‚Äù    Algorithms:\n  Try PCA with $k = 1$\n  Compute $$ U_{\\text{reduce}}, z^{(1)}, \\dots,z^{(m)}, x^{(1)}{\\text{approx}}, \\dots,x^{(m)}{\\text{approx}} $$\n  check if the ratio mentioned above is less than (or equal to) 0.01?\n if not, try $k = 2, \\dots$      Easier way: [U, S, V] = svd(Sigma)\n  $S$ is an $n \\times n$ diagonal matrix: $$ S = \\begin{bmatrix} S_{11} \u0026 \u0026 \u0026 \\ \u0026 S_{22} \u0026 \u0026 \\ \u0026 \u0026 \\ddots \u0026 \\ \u0026 \u0026 \u0026 S_{nn} \\end{bmatrix} $$\n  And $$ \\frac{\\frac{1}{m} \\sum_{i=1}^m | x^{(i)} - x^{(i)}{\\text{approx}} |^2}{\\frac{1}{m} \\sum{i=1}^m | x^{(i)} |^2} = 1 - \\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} $$\n  Pick smallest value of $k$ which: $\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\ge 0.99$\n    27.3 Advice for Applying PCA  Speeding up supervised learning algorithm  Say you have a supervised learning problem  Input $x$ and $y$  $x$ is a 10,000 dimensional feature vector e.g. $100 \\times100$ images = 10,000 pixels Such a huge feature vector will make the algorithm slow   With PCA we can reduce the dimensionality and make it tractable How    Extract $x$   So we now have an unlabeled training set    Apply PCA to $x$ vectors   So we now have a reduced dimensional feature vector $z$    This gives you a new training set   Each vector can be re-associated with the label    Take the reduced dimensionality data set and feed to a learning algorithm   Use $y$ as labels and $z$ as feature vector    if you have a new example map from higher dimensionality vector to lower dimensionality vector, then feed into learning a algorithm       PCA maps one vector to a lower dimensionality vector  $x \\rightarrow z$ Defined by PCA only on the training set The mapping computes a set of parameters  Feature scaling values $U_{\\text{reduce}}$  Parameter learned by PCA Should be obtained only by determining PCA on your training set     So we use those learned parameters for our  Cross validation set Test set       Applications of PCA  Compression  Reduce memory/disk needed to store data Speed up learning algorithm   Visualization  Typically chose $k =2$ or $k = 3$ Because we can plot these values!   A bad use of PCA: Use it to prevent over-fitting  Reasoning  If we have $x^i$ we have $n$ features, $z^i$ has $k$ features which can be lower If we only have k features then maybe we‚Äôre less likely to over fit‚Ä¶   This doesn‚Äôt work  BAD APPLICATION Might work OK, but not a good way to address over fitting Better to use regularization   PCA throws away some data without knowing what the values it‚Äôs losing  Probably OK if you‚Äôre keeping most of the data But if you‚Äôre throwing away some crucial data bad So you have to go to like 95-99% variance retained So here regularization will give you AT LEAST as good a way to solve over fitting     A second PCA myth  Used for compression or visualization - good Sometimes used to  Design ML system with PCA from the outset  But, what if you did the whole thing without PCA? (try it) See how a system performs without PCA When to use PCA: ONLY if you have a reason to believe PCA will help should you then add PCA PCA is easy enough to add on as a processing step  Try without first!            Ex7: K-Means Clustering and PCAüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex7/ex7.ipynb\n ",
  "wordCount" : "2129",
  "inLanguage": "en",
  "datePublished": "2020-06-22T00:00:00Z",
  "dateModified": "2020-06-22T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thistledown"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fang-lansheng.github.io/posts/2020-06-22-ml-ng-8/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My-Thistledown",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fang-lansheng.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://fang-lansheng.github.io" accesskey="h" title="My-Thistledown (Alt + H)">My-Thistledown</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ
    </h1>
    <div class="post-meta"><span title='2020-06-22 00:00:00 +0000 UTC'>June 22, 2020</span>&nbsp;¬∑&nbsp;Thistledown

</div>
  </header> 
  <div class="post-content"><blockquote>
<p>Course Link Ôºö<a href="https://www.coursera.org/learn/machine-learning/home/week/8">Week8 - Unsupervised Learning &amp; Dimensionality Reduction</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="24-clustering">24 Clustering<a hidden class="anchor" aria-hidden="true" href="#24-clustering">#</a></h2>
<h4 id="241-unsupervised-learning-introduction">24.1 Unsupervised Learning: Introduction<a hidden class="anchor" aria-hidden="true" href="#241-unsupervised-learning-introduction">#</a></h4>
<ul>
<li>Unsupervised learning: learning from unlabeled data</li>
<li>Supervised learning &amp; unsupervised learning: Compare and contrast:
<ul>
<li>Supervised learning:
<ul>
<li>Given a set of labels, fit a hypothesis to it</li>
</ul>
</li>
<li>Unsupervised learning:
<ul>
<li>Try and determining structure in the data</li>
<li>Clustering algorithm groups data together based on data features</li>
</ul>
</li>
</ul>
</li>
<li>What is clustering good for
<ul>
<li>Market segmentation: group customers into different market segments</li>
<li>Social network analysis: Facebook &ldquo;smartlists&rdquo;</li>
<li>Organizing computer clustering and data centers for network layout and location</li>
<li>Astronomical data analysis: understanding galaxy formation</li>
</ul>
</li>
</ul>
<h4 id="242-k-means-algorithm">24.2 K-Means Algorithm<a hidden class="anchor" aria-hidden="true" href="#242-k-means-algorithm">#</a></h4>
<ul>
<li>Algorithm overview
<ul>
<li>
<ol>
<li>Randomly allocate two points as the <em>cluster centroids</em></li>
</ol>
<ul>
<li>Have as many cluster centroids as clusters you want to do ($K$ cluster centroids, in fact)</li>
<li>In our example we just have two clusters, so there&rsquo;re two cluster centroids (red and blue point)</li>
</ul>
</li>
<li>
<ol start="2">
<li>Cluster assignment step</li>
</ol>
<ul>
<li>Go through each example and depending on if it&rsquo;s closer the red or blue centroid assign each point to one of the two clusters</li>
<li>To demonstrate this, we&rsquo;ve gone through the data and &ldquo;color&rdquo; each point red or blue</li>
</ul>
</li>
<li>
<ol start="3">
<li>Move centroid step</li>
</ol>
<ul>
<li>Take each centroid and move to the average of the correspondingly assigned data-points</li>
</ul>
</li>
<li>
<ol start="4">
<li>Repeat 2) and 3) until convergence</li>
</ol>
</li>
</ul>
</li>
<li>More formal definition
<ul>
<li>Input:
<ul>
<li>$K$: number of clusters in the data</li>
<li>Training set ${x^{(1)}, x^{(n)}, \dots,x^{(m)}}$, $x^{(i)} \in \Bbb{R}^n$ (drop $x_0 = 1$ convention)</li>
</ul>
</li>
<li>Algorithm:
<ul>
<li>Randomly initialize $K$ cluster centroids as ${\mu_1, \mu_2, \dots, u_K \in \Bbb{R}^n }$</li>
<li><img loading="lazy" src="https://i.loli.net/2020/06/23/1reMxvKJ8EzN5Cq.png" alt="image-20200623093920179"  />
</li>
</ul>
</li>
</ul>
</li>
<li>K-means for non-separated clusters</li>
</ul>
<h4 id="243-optimization-objective">24.3 Optimization Objective<a hidden class="anchor" aria-hidden="true" href="#243-optimization-objective">#</a></h4>
<ul>
<li>
<p>Notations:</p>
<ul>
<li>$c^{(i)}$ = index of cluster ($1, 2, \dots, K$) to which example $x^{(i)}$ is currently assigned
<ul>
<li>$c^{(i)} = k \in {1, 2, \dots, K}$</li>
<li>$i \in {1, 2, \dots, m}$</li>
</ul>
</li>
<li>$\mu_k$ = cluster centroid k ($\mu_k \in \Bbb{R}^n$)</li>
<li>$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</li>
</ul>
</li>
<li>
<p>Optimization objective</p>
<ul>
<li>
<p>$$
J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K) = \frac{1}{m} \sum_{i=1}^m |x^{(i)} - \mu_{c^{(i)}} |^2
$$</p>
</li>
<li>
<p>$$
\mathop{\mathop{\text{min}}<em>\limits{c^{(1)}, \dots, c^{(m)}}}</em>\limits{\mu_1, \dots, \mu_K} J(c^{(1)}, \dots, c^{(m)},\mu_1, \dots, \mu_K)
$$</p>
</li>
</ul>
</li>
<li>
<p>If we consider the K-means algorithm</p>
<ul>
<li>The <em>cluster assigned step</em> is minimizing $J(\cdot)$ with respect to $c^{(i)}$
<ul>
<li>i.e. find the centroid closet to each example</li>
<li>Doesn&rsquo;t change the centroids themselves</li>
</ul>
</li>
<li>The <em>move centroid step</em>
<ul>
<li>This step is choosing the values of $\mu$ which minimizes $J(\cdot)$ with respect to $\mu_k$</li>
</ul>
</li>
<li>So, we&rsquo;re partitioning the algorithm into two parts:
<ul>
<li>First part minimizes the $c$ variables</li>
<li>Second part minimizes the $J$ variables</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="244-random-initialization">24.4 Random Initialization<a hidden class="anchor" aria-hidden="true" href="#244-random-initialization">#</a></h4>
<ul>
<li>Have number of centroids set to less than number of examples ($K &lt; m$)
<ul>
<li>Randomly pick $K$ training examples</li>
<li>Set $\mu_1$ up to $\mu_K$ to these examples values</li>
</ul>
</li>
<li>K-means can converge to different solutions depending on the initialization setup
<ul>
<li>Risk of local optimum<img loading="lazy" src="https://i.loli.net/2020/06/23/IivZnJ8WOaqtEUw.png" alt="image-20200623100857119"  />
</li>
<li>We can do multiple random initializations to see if we get the same result - many same result are likely to indicate a global optimum</li>
</ul>
</li>
</ul>
<h4 id="245-choosing-the-number-of-clusters">24.5 Choosing the Number of Clusters<a hidden class="anchor" aria-hidden="true" href="#245-choosing-the-number-of-clusters">#</a></h4>
<ul>
<li>What is right value of $K$ ?</li>
<li><!-- raw HTML omitted -->Elbow method<!-- raw HTML omitted -->
<ul>
<li>Vary $K$ and compute cost function at a range of $K$ values</li>
<li>As $K$ increases, $J(\cdot)$ minimum value should decrease (i.e. you decrease the granularity so centroids can better optimize)</li>
<li>Plot $K$ vs. $J(\cdot)$
<img loading="lazy" src="https://i.loli.net/2020/06/23/gRD4bvnV9iBKmE6.png" alt="image-20200623102031067"  />
</li>
<li>Chose the &ldquo;elbow&rdquo; number of clusters (if you get a nice plot this is a reasonable way of choosing $K$)</li>
<li>Risks: normally you don&rsquo;t get a a nice line ‚Üí no clear elbow on curve</li>
</ul>
</li>
<li>Another method for choosing $K$
<ul>
<li>Sometimes, you‚Äôre, running K-¬≠means to get clusters to use for some later/downstream purpose. Evaluate K¬≠‚Äêmeans based on a metric for how well it performs for that later purpose.</li>
</ul>
</li>
</ul>
<h2 id="25-motivation">25 Motivation<a hidden class="anchor" aria-hidden="true" href="#25-motivation">#</a></h2>
<h4 id="251-motivation-i-data-compression">25.1 Motivation I: Data Compression<a hidden class="anchor" aria-hidden="true" href="#251-motivation-i-data-compression">#</a></h4>
<ul>
<li>A second type of unsupervised learning algorithm: <!-- raw HTML omitted -->dimensionality reduction<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted -->Data compression<!-- raw HTML omitted -->
<ul>
<li>Speeds up algorithms</li>
<li>Reduces space used by data for them</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/23/C9BUjYqlJ76dkab.png" alt="image-20200623104217083"  />
</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/23/QLnbruCBDRmtPMI.png" alt="image-20200623104239132"  />
</p>
<h4 id="252-motivation-ii-visualization">25.2 Motivation II: Visualization<a hidden class="anchor" aria-hidden="true" href="#252-motivation-ii-visualization">#</a></h4>
<ul>
<li>It&rsquo;s hard to visualize highly dimensional data
<ul>
<li>Dimensionality reduction can improve how we display information in a tractable manner for human consumption</li>
<li>Why do we care?
<ul>
<li>Often helps to develop algorithms if we can understand our data better</li>
<li>Dimensionality reduction helps us do this, see data in a helpful manner</li>
<li>Good for explaining something to someone if you can &ldquo;show&rdquo; it in the data</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="26-principal-component-analysis">26 Principal Component Analysis<a hidden class="anchor" aria-hidden="true" href="#26-principal-component-analysis">#</a></h2>
<h4 id="261-principal-component-analysis-problem-formulation">26.1 Principal Component Analysis Problem Formulation<a hidden class="anchor" aria-hidden="true" href="#261-principal-component-analysis-problem-formulation">#</a></h4>
<ul>
<li>
<p>For the problem of dimensionality reduction the most commonly used algorithm is <!-- raw HTML omitted -->PCA<!-- raw HTML omitted --></p>
</li>
<li>
<p>Say we have a 2D data set which we wish to reduce to 1D</p>
<!-- raw HTML omitted -->
<ul>
<li>
<p>In other words, find a single line onto which to project this data. How do we determine this line?</p>
<ul>
<li>
<p>The distance between each point and the projected version should be small (blue lines below are short)</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>PCA tries to find a lower dimensional surface so the sum of squares onto that surface is minimized</p>
</li>
<li>
<p>The blue lines are sometimes called the <strong>projection error</strong></p>
<ul>
<li>PCA tries to find the surface (a straight line in this case) which has the minimum projection error</li>
</ul>
</li>
<li>
<p>As an aside, you should normally do <strong>mean normalization</strong> and <strong>feature scaling</strong> on your data before PCA</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>A more formal description is</p>
<ul>
<li>For 2D-1D, we must find a vector $u^{(1)}$, which is of some dimensionality</li>
<li>Onto which you can project the data so as to minimize the projection error
<img loading="lazy" src="https://i.loli.net/2020/06/23/fdeILlCABD5ZzaF.png" alt="image-20200623110347915"  />
</li>
<li>$u^{(1)}$ can be positive or negative ($-u^{(1)}$) which makes no difference
<ul>
<li>Each of the vectors define the same red line</li>
</ul>
</li>
</ul>
</li>
<li>
<p>In the more general case, To reduce from $n$-D to $k$-D we</p>
<ul>
<li>Find $k$ vectors ($u^{(1)} , u^{(2)}, \dots, u^{(k)}$) onto which to project the data to minimize the projection error</li>
<li>So lots of vectors onto which we project the data</li>
<li>Find a set of vectors which we project the data onto the linear subspace spanned by that set of vectors
<ul>
<li>We can define a point in a plane with $k$ vectors</li>
</ul>
</li>
<li>e.g. 3D ‚Üí 2D
<ul>
<li>Find pair of vectors which define a 2D plane (surface) onto which you&rsquo;re going to project your data</li>
<li>Much like the &ldquo;shallow box&rdquo; example in compression, we&rsquo;re trying to create the shallowest box possible (by defining two of it&rsquo;s three dimensions, so the box&rsquo; depth is minimized)
<img loading="lazy" src="https://i.loli.net/2020/06/23/jf9AJygxaoc64wp.png" alt="image-20200623111238380"  />
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>PCA is <strong>NOT</strong> linear regression
<img loading="lazy" src="https://i.loli.net/2020/06/23/7rZdRoMuJCemYUk.png" alt="image-20200623111504389"  />
</p>
<ul>
<li>For linear regression, fitting a straight line to minimize the <strong>straight line</strong> between a point and a squared line
<ul>
<li>Note: <strong>VERTICAL distance</strong> between point</li>
</ul>
</li>
<li>For PCA minimizing the magnitude of the shortest <strong>orthogonal distance</strong>
<ul>
<li>Gives very different effects</li>
</ul>
</li>
<li>More generally
<ul>
<li>With linear regression we&rsquo;re trying to predict &ldquo;$y$&rdquo;</li>
<li>With PCA there is no &ldquo;$y$&rdquo; - instead we have a list of features and all features are treated equally
<ul>
<li>If we have 3D dimensional data 3D-&gt;2D
<ul>
<li>Have 3 features treated symmetrically</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="262-principal-component-analysis-algorithm">26.2 Principal Component Analysis Algorithm<a hidden class="anchor" aria-hidden="true" href="#262-principal-component-analysis-algorithm">#</a></h4>
<ul>
<li>
<p>Before applying PCA must do data preprocessing</p>
<ul>
<li><strong>Mean normalization</strong></li>
<li><strong>Feature scaling (depending on data)</strong></li>
</ul>
</li>
<li>
<p>With preprocessing done, PCA finds the lower dimensional sub-space which minimizes the sum of the square
<img loading="lazy" src="https://i.loli.net/2020/06/23/4tIuUHmyFbi59nq.png" alt="image-20200623113159983"  />
</p>
<ul>
<li>Need to compute two things: $\mu$ vectors &amp; $z$ vectors</li>
</ul>
</li>
<li>
<p>Algorithm description</p>
<ul>
<li>
<p>Reducing data from $n$-dimensional to $k$-dimensional</p>
<ul>
<li>
<p>Compute the covariance matrix: $\Sigma = \frac{1}{m}\sum_{i=1}^n \left(x^{(i)} \right)\left(x^{(i)} \right)^T$</p>
<ul>
<li>$\Sigma$: This is commonly denoted as Œ£ (Greek upper case sigma) - NOT summation symbol</li>
<li>$\Sigma = \frac{1}{m} x^Tx$
<ul>
<li>$\Sigma$ is an $n \times n$ matrix</li>
<li>$x^{(i)}$ is an $n \times 1$ matrix</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Compute eigenvectors of matrix $\Sigma$: <code>[U, S, V] = svd(sigma)</code></p>
<ul>
<li><code>svd</code> = singular value decomposition</li>
</ul>
</li>
<li>
<p>$U$, $S$ and $V$ are matrices</p>
<ul>
<li>
<p>$U$ matrix is also an $n \times n$ matrix</p>
</li>
<li>
<p>Turns out the columns of $U$ are the $u$ vectors we want</p>
</li>
<li>
<p>So to reduce a system from $n$-D to $k$-D: just take the first $k$ vectors from $U$:
$$
U=\left[\begin{array}{cccc}
| &amp; | &amp; &amp; | \
u^{(1)} &amp; u^{(2)} &amp; \ldots &amp; u^{(n)} \
| &amp; | &amp; &amp; |
\end{array}\right] \in \mathbb{R}^{n \times n}
$$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Next we need to find some way to change $x$ (which is $n$ dimensional) to $z$ (which is $k$ dimensional) - reduce the dimensionality</p>
<ul>
<li>
<p>Take first $k$ columns of the $u$ matrix and stack in columns:
$$
U_{\text{reduce}}=\left[\begin{array}{cccc}
| &amp; | &amp; &amp; | \
u^{(1)} &amp; u^{(2)} &amp; \ldots &amp; u^{(k)} \
| &amp; | &amp; &amp; |
\end{array}\right] \in \mathbb{R}^{n \times k}
$$</p>
</li>
<li>
<p>$z = U_{\text{reduce}}^T * x=[k \times n] * [n \times 1] = [k \times 1]$</p>
</li>
</ul>
</li>
<li>
<p>Exactly the same as with supervised learning except we&rsquo;re now doing it with unlabeled data</p>
</li>
<li>
<p>So in summary</p>
<ul>
<li>Preprocessing</li>
<li>Calculate $\Sigma$ (covariance matrix)</li>
<li>Calculate eigenvectors with <code>svd</code></li>
<li>Take $k$ vectors from $U$, i.e. $U_{\text{reduce}} = U(:,1:k)$</li>
<li>Calculate $z$ ($z =U_{\text{reduce}}^T * x$)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="27-applying-pca">27 Applying PCA<a hidden class="anchor" aria-hidden="true" href="#27-applying-pca">#</a></h2>
<h4 id="271-reconstruction-from-compressed-representation">27.1 Reconstruction from compressed Representation<a hidden class="anchor" aria-hidden="true" href="#271-reconstruction-from-compressed-representation">#</a></h4>
<ul>
<li>
<p>Earlier spoke about PCA as a compression algorithm</p>
<ul>
<li>If this is the case, is there a way to <strong>decompress</strong> the data from low dimensionality back to a higher dimensionality format?</li>
</ul>
</li>
<li>
<p>Reconstruction</p>
<ul>
<li>
<p>Say we have an example as follows:</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>We have our examples ($x^{(1)}$, $x^{(2)}$ etc.)</p>
</li>
<li>
<p>Project onto $z$-surface</p>
</li>
<li>
<p>Given a point $z^{(1)}$, how can we go back to the 2D space?</p>
</li>
</ul>
</li>
<li>
<p>Considering</p>
<ul>
<li>$z = U_{\text{reduce}}^T * x$</li>
</ul>
</li>
<li>
<p>To go in the opposite direction we must do</p>
<ul>
<li>$x_{\text{approx}} = U_{\text{reduce}} * z,(=[n\times k] * [k * 1] = [n \times 1])$</li>
</ul>
</li>
<li>
<p>So this creates the following representation</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>We lose some of the information (i.e. everything is now perfectly on that line) but it is now projected into 2D space</p>
</li>
</ul>
<h4 id="272-choosing-the-number-of-principal-components">27.2 Choosing the Number of Principal Components<a hidden class="anchor" aria-hidden="true" href="#272-choosing-the-number-of-principal-components">#</a></h4>
<ul>
<li>
<p>PCA tries to minimize <!-- raw HTML omitted -->average squared projection error<!-- raw HTML omitted -->:
$$
\frac{1}{m} \sum_{i=1}^m | x^{(i)} - x^{(i)}_{\text{approx}} |^2
$$</p>
</li>
<li>
<p><!-- raw HTML omitted -->Total variation<!-- raw HTML omitted --> in data can be defined as the average over data saying how far are the training examples from the origin:
$$
\frac{1}{m} \sum_{i=1}^m | x^{(i)} |^2
$$</p>
</li>
<li>
<p>Typically, choose $k$ to be smallest value so that
$$
\frac{\frac{1}{m} \sum_{i=1}^m | x^{(i)} - x^{(i)}<em>{\text{approx}} |^2}{\frac{1}{m} \sum</em>{i=1}^m | x^{(i)} |^2} \le 0.01 , (=1%)
$$</p>
<ul>
<li>means &ldquo;99% of variance is retained&rdquo;</li>
</ul>
</li>
<li>
<p>Algorithms:</p>
<ul>
<li>
<p>Try PCA with $k = 1$</p>
</li>
<li>
<p>Compute
$$
U_{\text{reduce}}, z^{(1)}, \dots,z^{(m)}, x^{(1)}<em>{\text{approx}}, \dots,x^{(m)}</em>{\text{approx}}
$$</p>
</li>
<li>
<p>check if the ratio mentioned above is less than (or equal to) 0.01?</p>
<ul>
<li>if not, try $k = 2, \dots$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Easier way: <code>[U, S, V] = svd(Sigma)</code></p>
<ul>
<li>
<p>$S$ is an $n \times n$ diagonal matrix:
$$
S = \begin{bmatrix} S_{11} &amp; &amp; &amp; \  &amp; S_{22} &amp;  &amp;  \   &amp;  &amp; \ddots &amp; \  &amp;  &amp;  &amp; S_{nn} \end{bmatrix}
$$</p>
</li>
<li>
<p>And
$$
\frac{\frac{1}{m} \sum_{i=1}^m | x^{(i)} - x^{(i)}<em>{\text{approx}} |^2}{\frac{1}{m} \sum</em>{i=1}^m | x^{(i)} |^2}  = 1 -
\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}
$$</p>
</li>
<li>
<p>Pick smallest value of $k$ which: $\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \ge 0.99$</p>
</li>
</ul>
</li>
</ul>
<h4 id="273-advice-for-applying-pca">27.3 Advice for Applying PCA<a hidden class="anchor" aria-hidden="true" href="#273-advice-for-applying-pca">#</a></h4>
<ul>
<li>Speeding up supervised learning algorithm
<ul>
<li>Say you have a supervised learning problem
<ul>
<li>Input $x$ and $y$
<ul>
<li>$x$ is a 10,000 dimensional feature vector</li>
<li>e.g. $100 \times100$ images = 10,000 pixels</li>
<li>Such a huge feature vector will make the algorithm slow</li>
</ul>
</li>
<li>With PCA we can reduce the dimensionality and make it tractable</li>
<li>How
<ul>
<li>
<ol>
<li>Extract $x$</li>
</ol>
<ul>
<li>So we now have an unlabeled training set</li>
</ul>
</li>
<li>
<ol start="2">
<li>Apply PCA to $x$ vectors</li>
</ol>
<ul>
<li>So we now have a reduced dimensional feature vector $z$</li>
</ul>
</li>
<li>
<ol start="3">
<li>This gives you a new training set</li>
</ol>
<ul>
<li>Each vector can be re-associated with the label</li>
</ul>
</li>
<li>
<ol start="4">
<li>Take the reduced dimensionality data set and feed to a learning algorithm</li>
</ol>
<ul>
<li>Use $y$ as labels and $z$ as feature vector</li>
</ul>
</li>
<li>
<ol start="5">
<li>if you have a new example map from higher dimensionality vector to lower dimensionality vector, then feed into learning a algorithm</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>PCA maps one vector to a lower dimensionality vector
<ul>
<li>$x \rightarrow z$</li>
<li>Defined by PCA <strong>only</strong> on the training set</li>
<li>The mapping computes a set of parameters
<ul>
<li>Feature scaling values</li>
<li>$U_{\text{reduce}}$
<ul>
<li>Parameter learned by PCA</li>
<li>Should be obtained only by determining PCA on your training set</li>
</ul>
</li>
</ul>
</li>
<li>So we use those learned parameters for our
<ul>
<li>Cross validation set</li>
<li>Test set</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Applications of PCA
<ul>
<li>Compression
<ul>
<li>Reduce memory/disk needed to store data</li>
<li>Speed up learning algorithm</li>
</ul>
</li>
<li>Visualization
<ul>
<li>Typically chose $k =2$ or $k = 3$</li>
<li>Because we can plot these values!</li>
</ul>
</li>
<li>A bad use of PCA: Use it to prevent over-fitting
<ul>
<li>Reasoning
<ul>
<li>If we have $x^i$ we have $n$ features, $z^i$ has $k$ features which can be lower</li>
<li>If we <em>only</em> have k features then maybe we&rsquo;re less likely to over fit&hellip;</li>
</ul>
</li>
<li>This doesn&rsquo;t work
<ul>
<li>BAD APPLICATION</li>
<li>Might work OK, but not a good way to address over fitting</li>
<li>Better to use regularization</li>
</ul>
</li>
<li>PCA throws away some data without knowing what the values it&rsquo;s losing
<ul>
<li>Probably OK if you&rsquo;re keeping most of the data</li>
<li>But if you&rsquo;re throwing away some crucial data bad</li>
<li>So you have to go to like 95-99% variance retained</li>
<li>So here regularization will give you AT LEAST as good a way to solve over fitting</li>
</ul>
</li>
</ul>
</li>
<li>A second PCA myth
<ul>
<li>Used for compression or visualization - good</li>
<li>Sometimes used to
<ul>
<li>Design ML system with PCA from the outset
<ul>
<li>But, what if you did the whole thing without PCA? (try it)</li>
<li>See how a system performs without PCA</li>
<li>When to use PCA: ONLY if you have a reason to believe PCA will help should you then add PCA</li>
<li>PCA is easy enough to add on as a processing step
<ul>
<li>Try without first!</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ex7-k-means-clustering-and-pca">Ex7: K-Means Clustering and PCAüë®‚Äçüíª<a hidden class="anchor" aria-hidden="true" href="#ex7-k-means-clustering-and-pca">#</a></h2>
<blockquote>
<p>See this exercise on <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python/blob/master/ex7/ex7.ipynb">Coursera-MachineLearning-Python/ex7/ex7.ipynb</a></p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://fang-lansheng.github.io/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/python/">Python</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://fang-lansheng.github.io">My-Thistledown</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
