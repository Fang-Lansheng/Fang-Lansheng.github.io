<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ | My-Thistledown</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ÔºöWeek 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&rsquo;re trying to predict">
<meta name="author" content="Thistledown">
<link rel="canonical" href="https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://fang-lansheng.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://fang-lansheng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://fang-lansheng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://fang-lansheng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://fang-lansheng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ" />
<meta property="og:description" content="Course Link ÔºöWeek 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&rsquo;re trying to predict" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-04T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-03-04T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ"/>
<meta name="twitter:description" content="Course Link ÔºöWeek 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&rsquo;re trying to predict"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://fang-lansheng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ",
      "item": "https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ",
  "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ",
  "description": "Course Link ÔºöWeek 3 - Logistic Regression\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n  For now, we will focus on the binary classification problem.\n  $y \\in {0,1}$ - the variable that we\u0026rsquo;re trying to predict",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ÔºöWeek 3 - Logistic Regression\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n  For now, we will focus on the binary classification problem.\n  $y \\in {0,1}$ - the variable that we‚Äôre trying to predict\n $0: \\text{‚ÄúNegative Class‚Äù}$ (e.g., benign tumor) $1: \\text{‚ÄúPositive Class‚Äù}$ (e.g., malignant tumor)    Applying linear regression to a classification problem often isn‚Äôt a great idea.\n  Logistic Regression: $0\\leq h_\\theta(x) \\leq 1$ (BTW, this is actually a classification algorithm)\n  7.2 Hypothesis Representation  Logistic Regression Model: want $0 \\leq h_\\theta(x) \\leq 1$ $h_\\theta(x) = g(\\theta^Tx)$  Sigmoid function (or logistic function): $g(z) = \\frac{1}{1 + e^{-z}}$. The following image shows us what the sigmoid function looks like:  $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$   Interpretation of hypothesis output:  $h_\\theta(x) = \\text{estimated probability that } y=1 \\text{ on input }x$ $h_\\theta(x)= P(y=1 \\vert x;\\theta)$ : probability that $y = 1$, given $x$, parameterized by $\\theta$ $P(y=0 \\vert x;\\theta) + P(y=1 \\vert x;\\theta) = 1 \\\\ P(y=0 \\vert x;\\theta) = 1 - P(y=1 \\vert x;\\theta)$    7.3 Decision Boundary  Logistic regression: $h_\\theta(x) = g(\\theta^Tx), \\quad g(z)=\\frac{1}{1+e^{-z}}$  Suppose predict $y=\\begin{cases} 0, \u0026 \\text {if $h_\\theta(x) \\geq$ 0.5} \\newline 1, \u0026 \\text{if $h_\\theta(x)   Decision boundary: the decision boundary is the line that separates the area where $y=0$ and where $y=1$.  It is created by our hypothesis function The input to the sigmoid function of $g(z)$ doesn‚Äôt need to be linear, and could be a function that describes a circle or any shape to fit the data    8 Logistic Regression Model 8.1 Cost Function  Logistic regression cost function:  $$ J(\\theta) = \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}(h_\\theta(x^{(i)}, y^{(i)} ) $$\n $\\text{Cost}(h_\\theta(x), y)=\\begin{cases} -\\log(h_\\theta(x)), \u0026 \\text {if } y =1 \\newline -\\log(1-h_\\theta(x)), \u0026 \\text{if } y=0 \\end{cases}$  $\\text{Cost}(h_\\theta(x), y) = 0, \\text{ if } h_\\theta(x) = y$ $\\text{Cost}(h_\\theta(x), y) \\rightarrow \\infty, \\text{ if $y=0$ and $h_\\theta(x) \\rightarrow 1$} $ $\\text{Cost}(h_\\theta(x), y) \\rightarrow \\infty, \\text{ if $y=1$ and $h_\\theta(x) \\rightarrow 0$} $   When $y=1$, we get the following plot for $J(\\theta) \\text{ vs } h_\\theta(x)$  Similarly, when $y=0$, we get the following plot for $J(\\theta) \\text{ vs } h_\\theta(x)$   8.2 Simplified Cost Function and Gradient Descent   Logistic regression cost function\n $J(\\theta) = \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}(h_\\theta(x^{(i)}, y^{(i)} )$ $\\text{Cost}(h_\\theta(x), y)=\\begin{cases} -\\log(h_\\theta(x)), \u0026 \\text {if } y =1 \\newline -\\log(1-h_\\theta(x)), \u0026 \\text{if } y=0 \\end{cases}$ $\\text{Note: $y=0$ or $ 1$ always}$    Then, $\\text{Cost}(h_\\theta(x), y)=-y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))$\n  $$ \\begin{align} J(\\theta) \u0026= \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}\\left(h_\\theta(x^{(i)}, y^{(i)}\\right) \\ \u0026= -\\frac{1}{m}\\sum_{i =1}^{m}\\left[ y^{(i)}\\log h_\\theta(x^{(i)})+(1-y^{(i)})\\log \\left(1-h_\\theta(x^{(i)})\\right) \\right] \\end{align} $$\n  A vectorized implementation is:\n $h = g(X\\theta)$ $J(\\theta) = \\frac{1}{m}\\cdot \\left(-y^T\\log(h)-(1-y)^T\\log(1-h)\\right)$    To fit parameters $\\theta : \\mathop{\\text{min}}\\limits_\\theta J(\\theta)$\n  To make a prediction given new $x$ : output $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n  Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\newline \\qquad (\\text{simultaneously update all $\\theta_j$) } \\newline } \\end{array} \\right. $$\n(Algorithm looks identical to linear regression!)\n  A vectorized implementation is: $\\theta := \\theta - \\frac{\\alpha}{m}X^T\\left( g(X\\theta)- \\vec{y} \\right)$\n  8.3 Advanced Optimization   Cost function $J(\\theta)$. Want $\\mathop{\\text{min}}\\limits_\\theta J(\\theta)$.\n  Given $\\theta$, we have code that can compute\n $J(\\theta)$ $\\frac{\\partial}{\\partial \\theta_j}J(\\theta), \\ \\text{(for $j = 0, 1, \\dots,n$)} $    Optimization algorithms:\n (In this class) Gradient descent Conjugate gradient, BFGS, L-BFGS  Advantages:  No need to manually pick $\\alpha$ Often faster that gradient descent   Disadvantages:  More sophisticated        9 Multiclass classification 9.1 Multiclass Classification: One-vs-all   $y = \\lbrace 0, 1, \\dots, n \\rbrace$\n$$ h_\\theta^{(0)}(x) = P(y=0 \\vert x;\\theta) \\ h_\\theta^{(1)}(x) = P(y=1 \\vert x;\\theta) \\ \\cdots \\ h_\\theta^{(n)}(x) = P(y=n \\vert x;\\theta) \\ \\text{prediction} = \\mathop{\\text{max}}\\limits_i h_\\theta^{(i)}(x) $$\n  The following image shows how one could classify 3 classes:   One-vs-all (one-vs-rest):\n $h_\\theta^{(i)} = P(y=i \\vert x;\\theta)\\quad (i=1,2,\\dots,n)$ Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y = i$. On a new input $x$, to make a prediction, pick the class i that maximizes $h_\\theta(x)$ : $\\mathop{\\text{max}}\\limits_i h_\\theta^{(i)}(x)$    10 Solving the Problem of Overfitting 10.1 The Problem of Overfitting  Overfitting: If we have too many features, the learned hypothesis may fit the training set well ($J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left( h_\\theta(x^{(i)}-y^{(i)}) \\right)^2\\approx 0$), but fail to generalize to new new examples (predict prices on new examples).  Options of addressing overfitting  Reduce number of features  Manually select which features to keep Model selection algorithm   Regularization  Keep all the features, but reduce magnitude/values of parameters $\\theta_j$ Regularization works well when we have a lot of slightly useful features, each of which contributes a bit to predicting $y$      10.2 Cost Function   Regularization: small values for parameters $\\theta_0, \\theta_1, \\dots, \\theta_n$\n ‚ÄúSimpler‚Äù hypothesis Less prone to overfitting    Cost function after regularization:\n$$ J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m\\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda\\sum_{j=1}^n \\theta_j^2 \\right] $$\n $\\lambda: \\text{regularization parameter}$    10.3 Regularized Linear Regression   Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_0^{(i)} \\newline \\qquad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\left[ \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)} + \\lambda\\theta_j \\right] \\quad j\\in\\lbrace 1,2,\\dots,n \\rbrace \\newline } \\end{array} \\right. $$\n When $j=1, 2, \\dots, n$, you can also write $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}$ $1-\\alpha\\frac{\\lambda}{m}    Normal equaiton\n$$ X=\\begin{bmatrix} (x^{(1)})^T \\newline \\vdots \\newline (x^{(m)})^T \\end{bmatrix}{m\\times(n+1)} \\quad y=\\begin{bmatrix} y^{(1)} \\newline \\vdots \\newline x^{(m)} \\end{bmatrix}\\in\\mathbb{R^{m}} \\\\ \\text{To } \\mathop{\\text{min}}\\limits\\theta J(\\theta), \\ \\theta=\\left(X^TX+\\lambda \\begin{bmatrix} 0 \u0026 \u0026 \u0026 \\ \u0026 1 \u0026 \u0026 \\ \u0026 \u0026 \\ddots \\ \u0026 \u0026 \u0026 1 \\end{bmatrix} \\right)^{-1}X^Ty $$\n  Non-invertibility\n Suppose $\\mathop{m}\\limits_{\\text{#examples}} \\leq \\mathop{n}\\limits_{\\text{#features}} $, then $X^TX$ will be non-invertible/singluar. $\\text{If } \\lambda  0, \\ \\theta=\\left(\\underbrace{X^TX+\\lambda \\begin{bmatrix} 0 \u0026 \\newline \u0026 I_n \\end{bmatrix} }_{\\text{invertible}} \\right)^{-1}X^Ty$    10.4 Regularized Logistic Regression   We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pick line, is less likely to overfit than the non-regularized function represented by the blue line:   Regularized cost function:\n$$ J(\\theta)= -\\frac{1}{m}\\sum_{i =1}^{m}\\left[ y^{(i)}\\log h_\\theta(x^{(i)})+(1-y^{(i)})\\log \\left(1-h_\\theta(x^{(i)})\\right) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2 $$\n $\\sum_{j=1}^{n}\\theta_j^2$ means to explicitly exclude the bias term, $\\theta_0$.    Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_0^{(i)} \\newline \\qquad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\left[ \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)} + \\lambda\\theta_j \\right] \\quad j\\in\\lbrace 1,2,\\dots,n \\rbrace \\newline } \\end{array} \\right. $$\n  Ex2: Logistic Regressionüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex2/ex2.ipynb\n Ex2.1 Logistic Regression Instruction: In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\nCode:\nimport numpy as np import matplotlib.pyplot as plt import scipy.optimize as op  ''' Part 0: Loading Data and Defining Functions ''' # Hypothesis function def h(X, theta):  return 1 / (1 + np.exp(-np.dot(X, theta)))  # Logistic regression cost function def costFunction(theta, X, y):  m = len(y)  cost = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m  grad = np.dot(X.T, h(X, theta) - y) / m  return cost, grad  # The objective function to be minimized def costFunc(params, *args):  X, y = args  [m, n] = X.shape  theta = params.reshape([n, 1])  cost = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m  return cost  # Method for computing the gradient vector def gradFunc(params, *args):  X, y = args  [m, n] = X.shape  theta = params.reshape([n, 1])  grad = np.dot(X.T, h(X, theta) - y) / m  return grad.flatten()  # The first two columns contains the exam scores and the third column contains the label print('Loading Data ... ') data = np.loadtxt('ex2data1.txt', dtype=float, delimiter=',') X, y = data[:, 0:2], data[:, 2:3]  ''' Part 1: Plotting ''' fig0, ax0 = plt.subplots() label1, label0 = np.where(y.ravel() == 1), np.where(y.ravel() == 0) ax0.scatter(X[label1, 0], X[label1, 1], marker='+', color='g', label='Admitted') ax0.scatter(X[label0, 0], X[label0, 1], marker='x', color='r', label='Not admitted') ax0.legend(loc='upper right') ax0.set_xlabel('Exam 1 Score') ax0.set_ylabel('Exam 2 Score')  ''' Part 2: Compute Cost and Gradient ''' [m, n] = X.shape X = np.c_[np.ones([m, 1]), X] # Add intercept term to x and X_test initial_theta = np.zeros([n + 1, 1]) # Initialize fitting parameters  # Compute and display initial cost and gradient cost, grad = costFunction(initial_theta, X, y)  print('\\nCost at initial theta : ', cost.ravel()) print('Expected cost (approx): 0.693') print('Gradient at initial theta :', grad.ravel()) print('Expected gradients (approx): \\t-0.1000\\t-12.0092\\t-11.2628')  # Compute and display cost and gradient with non-zero theta test_theta = np.array(([-24], [0.2], [0.2])) cost, grad = costFunction(test_theta, X, y)  print('\\nCost at test theta : ', cost.ravel()) print('Expected cost (approx): 0.218') print('Gradient at test theta :', grad.ravel()) print('Expected gradients (approx): \\t0.043\\t2.5662\\t2.647')  ''' Part 3: Optimizing using fminunc ''' params = np.zeros([n + 1, 1]) args = (X, y)  # uUse Newton Conjugate Gradient algorithm to obtain the optimal theta res = op.minimize(fun=costFunc, x0=params, args=args, method='TNC', jac=gradFunc) cost, theta = res.fun, res.x  print('\\nCost at theta found by fminunc: ', cost) print('Expected cost (approx): 0.203') print('theta: ', theta) print('Expected theta (approx): \\t-25.161\\t0.206\\t0.201')  # Plot boundary x1 = np.arange(min(X[:, 1]), max(X[:, 1]), 1) x2 = (-theta[0] - theta[1] * x1) / theta[2] plt.plot(x1, x2, color='blue') plt.show()  ''' Part 4: Predict and Accuracies ''' prob = h(np.array(([1, 45, 85])), theta) print('\\nFor a student with scores 45 and 85, we predict an adimission probability of ', prob) print('Expected value: 0.775 +/- 0.002')  p = np.where(h(X, theta)  0.5, 1.0, 0.0) print('Train accuracy: ', np.mean(p == y.flatten()) * 100, '%') print('Expected accuracy (approx): 89.0 %\\n') Output:\n Console  Training data with decision boundary   Ex2.2 Regularized Logistic Regression Instruction: In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nCode:\nimport numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures  ''' Part 0: Loading Data and Defining Functions ''' # Feature mapping function to polynomial features def mapFeature(X1, X2):  degree = 6  X = np.ones([len(X1), 1])  for i in np.arange(1, degree + 1, 1):  for j in range(i + 1):  X = np.c_[X, X1**(i-j) * X2**(j)]  return X  # Hypothesis function def h(X, theta):  return 1 / (1 + np.exp(-np.dot(X, theta)))  # Compute cost and gradient for logistic regression with regularization def costFunctionReg(theta, X, y, reg_param):  m = len(y)  cost1 = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m  cost2 = 0.5 * reg_param * np.dot(theta[1:].T, theta[1:]) / m # Don't penalize theta_0  cost = cost1 + cost2  grad = np.dot(X.T, h(X, theta) - y) / m  grad[1:] += (reg_param * theta / m)[1:]  return cost, grad  # Use Batch Gradient Descent algorithm to minimize cost def batchGradientDescent(X, y, theta, alpha=0.1, iters = 2000, reg=1):  J_history = np.zeros(iters)  for i in range(iters):  cost, grad = costFunctionReg(theta, X, y, reg)  theta = theta - alpha * grad  J_history[i] = cost  return theta, J_history  # The first two columns contains the exam scores and the third column contains the label print('Loading Data ... ') data = np.loadtxt('ex2data2.txt', dtype=float, delimiter=',') X, y = data[:, 0:2], data[:, 2:3]  # Plot data fig0, ax0 = plt.subplots() label1, label0 = np.where(y.ravel() == 1), np.where(y.ravel() == 0) ax0.scatter(X[label1, 0], X[label1, 1], marker='+', color='k', label='y = 1') ax0.scatter(X[label0, 0], X[label0, 1], marker='x', color='y', label='y = 0')  ''' Part 1: Regularized Logistic Regression ''' m = len(y) X = mapFeature(X[:, 0], X[:, 1])  # Initialize fitting parameters initial_theta = np.zeros([X.shape[1], 1])  # Set regularization parameter lambda to 1 reg_param = 1  # Compute and display inital cost gradient for regularized logistic regression cost0, grad0 = costFunctionReg(initial_theta, X, y, reg_param) print('\\nCost at initial theta (zeros): ', cost0.flatten()) print('Expected cost (approx): 0.693') print('Gradient at initial theta (zeros) - first five values only: '  , np.around(grad0[0:5], 4).flatten()) print('Expected gradients (approx) - first five values only: ',  '0.0085 0.0188 0.0001 0.0503, 0.0115')  # Compute and display cost and gradient with all-ones theta and lambda = 10 test_theta = np.ones([X.shape[1], 1]) cost1, grad1 = costFunctionReg(test_theta, X, y, reg_param=10)  print('\\nCost at test theta (with lambda = 10): ', cost1.flatten()) print('Expected cost (approx): 3.16') print('Gradient at test theta - first five values only: '  , np.around(grad1[0:5], 4).flatten()) print('Expected gradients (approx) - first five values only: ',  '0.3460 0.1614 0.1948 0.2269, 0.0922')  ''' Part 2: Regularization and Accuracies ''' # Optimize theta, J_history = batchGradientDescent(X, y, initial_theta, reg=reg_param) # fig1, ax1 = plt.subplots() # ax1.plot(np.arange(2000), J_history, 'c')  # Plot boundary poly = PolynomialFeatures(6) x1min, x1max, x2min, x2max = X[:, 1].min(), X[:, 1].max(), X[:, 2].min(), X[:, 2].max() xx1, xx2 = np.meshgrid(np.linspace(x1min, x1max), np.linspace(x2min, x2max)) bd = 1 / (1 + np.exp(-poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(theta))) bd = bd.reshape(xx2.shape) CS = ax0.contour(xx1, xx2, bd, [0.5], colors='c') CS.collections[0].set_label('Decision\\nBoundary') ax0.set_title(r'$\\lambda$ = '+str(reg_param)) ax0.legend(loc='upper right') ax0.set_xlabel('Microchip Test 1') ax0.set_ylabel('Microchip Test 2') plt.show()  # Compute accuracy on our training set p = np.where(h(X, theta) = 0.5, 1.0, 0.0) print('\\nTrain Accuracy: ', np.mean(p == y) * 100, '%') print('Expected accuracy (with lambda = 1): 83.1 % (approx)') Output:\n Console ((Œª = 1)  Training data with decision boundary (Œª = 1)  Too much regularization (Underfitting) (Œª = 100)   ",
  "wordCount" : "2144",
  "inLanguage": "en",
  "datePublished": "2020-03-04T00:00:00Z",
  "dateModified": "2020-03-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thistledown"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My-Thistledown",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fang-lansheng.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://fang-lansheng.github.io" accesskey="h" title="My-Thistledown (Alt + H)">My-Thistledown</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ
    </h1>
    <div class="post-meta"><span title='2020-03-04 00:00:00 +0000 UTC'>March 4, 2020</span>&nbsp;¬∑&nbsp;Thistledown

</div>
  </header> 
  <div class="post-content"><blockquote>
<p>Course Link Ôºö<a href="https://www.coursera.org/learn/machine-learning/home/week/3">Week 3 - Logistic Regression</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="7-classification-and-representation">7 Classification and Representation<a hidden class="anchor" aria-hidden="true" href="#7-classification-and-representation">#</a></h2>
<h4 id="71-classification">7.1 Classification<a hidden class="anchor" aria-hidden="true" href="#71-classification">#</a></h4>
<ul>
<li>
<p>The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.</p>
</li>
<li>
<p>For now, we will focus on the <!-- raw HTML omitted -->binary classification problem<!-- raw HTML omitted -->.</p>
</li>
<li>
<p>$y \in {0,1}$ - the variable that we&rsquo;re trying to predict</p>
<ul>
<li>$0: \text{&ldquo;Negative Class&rdquo;}$ (e.g., benign tumor)</li>
<li>$1: \text{&ldquo;Positive Class&rdquo;}$ (e.g., malignant tumor)</li>
</ul>
</li>
<li>
<p>Applying linear regression to a classification problem often isn&rsquo;t a great idea.</p>
</li>
<li>
<p><!-- raw HTML omitted -->Logistic Regression<!-- raw HTML omitted -->: $0\leq h_\theta(x) \leq 1$ (BTW, this is actually a classification algorithm)</p>
</li>
</ul>
<h4 id="72-hypothesis-representation">7.2 Hypothesis Representation<a hidden class="anchor" aria-hidden="true" href="#72-hypothesis-representation">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Logistic Regression Model<!-- raw HTML omitted -->: want $0 \leq h_\theta(x) \leq 1$</li>
<li>$h_\theta(x) = g(\theta^Tx)$
<ul>
<li>Sigmoid function (or logistic function): $g(z) = \frac{1}{1 + e^{-z}}$. The following image shows us what the sigmoid function looks like:
<img loading="lazy" src="https://i.loli.net/2020/03/05/1UmdDKhyOuVRXgx.png" alt="image.png"  />
</li>
<li>$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$</li>
</ul>
</li>
<li>Interpretation of hypothesis output:
<ul>
<li>$h_\theta(x) = \text{estimated probability that } y=1 \text{ on input }x$</li>
<li>$h_\theta(x)= P(y=1 \vert   x;\theta)$ : probability that $y = 1$, given $x$, parameterized by $\theta$</li>
<li>$P(y=0 \vert x;\theta) + P(y=1 \vert x;\theta) = 1 \\ P(y=0 \vert x;\theta) = 1 - P(y=1 \vert x;\theta)$</li>
</ul>
</li>
</ul>
<h4 id="73-decision-boundary">7.3 Decision Boundary<a hidden class="anchor" aria-hidden="true" href="#73-decision-boundary">#</a></h4>
<ul>
<li>Logistic regression: $h_\theta(x) = g(\theta^Tx), \quad g(z)=\frac{1}{1+e^{-z}}$
<ul>
<li>Suppose predict $y=\begin{cases} 0, &amp; \text {if $h_\theta(x) \geq$ 0.5} \newline 1, &amp; \text{if $h_\theta(x) &lt; $ 0.5} \end{cases}$</li>
</ul>
</li>
<li>Decision boundary: the decision boundary is the line that separates the area where $y=0$ and where $y=1$.
<ul>
<li>It is created by our hypothesis function</li>
<li>The input to the sigmoid function of $g(z)$ doesn&rsquo;t need to be linear, and could be a function that describes a circle or any shape to fit the data</li>
</ul>
</li>
</ul>
<h2 id="8-logistic-regression-model">8 Logistic Regression Model<a hidden class="anchor" aria-hidden="true" href="#8-logistic-regression-model">#</a></h2>
<h4 id="81-cost-function">8.1 Cost Function<a hidden class="anchor" aria-hidden="true" href="#81-cost-function">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Logistic regression cost function<!-- raw HTML omitted -->:</li>
</ul>
<p>$$
J(\theta) = \frac{1}{m}\sum_{i =1}^{m}\text{Cost}(h_\theta(x^{(i)}, y^{(i)} )
$$</p>
<ul>
<li>$\text{Cost}(h_\theta(x), y)=\begin{cases} -\log(h_\theta(x)), &amp; \text {if } y =1 \newline -\log(1-h_\theta(x)), &amp; \text{if } y=0 \end{cases}$
<ul>
<li>$\text{Cost}(h_\theta(x), y) = 0, \text{ if } h_\theta(x) = y$</li>
<li>$\text{Cost}(h_\theta(x), y) \rightarrow \infty, \text{ if $y=0$ and $h_\theta(x) \rightarrow 1$} $</li>
<li>$\text{Cost}(h_\theta(x), y) \rightarrow \infty, \text{ if $y=1$ and $h_\theta(x) \rightarrow 0$} $</li>
</ul>
</li>
<li>When $y=1$, we get the following plot for $J(\theta) \text{ vs } h_\theta(x)$
<img loading="lazy" src="https://i.loli.net/2020/03/05/gJH7oC9dVLhsljz.png" alt="image.png"  />
</li>
<li>Similarly, when $y=0$, we get the following plot for $J(\theta) \text{ vs } h_\theta(x)$
<img loading="lazy" src="https://i.loli.net/2020/03/05/ZTPtRHfNSLVJvWI.png" alt="image.png"  />
</li>
</ul>
<h4 id="82-simplified-cost-function-and-gradient-descent">8.2 Simplified Cost Function and Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#82-simplified-cost-function-and-gradient-descent">#</a></h4>
<ul>
<li>
<p>Logistic regression cost function</p>
<ul>
<li>$J(\theta) = \frac{1}{m}\sum_{i =1}^{m}\text{Cost}(h_\theta(x^{(i)}, y^{(i)} )$</li>
<li>$\text{Cost}(h_\theta(x), y)=\begin{cases} -\log(h_\theta(x)), &amp; \text {if } y =1 \newline -\log(1-h_\theta(x)), &amp; \text{if } y=0 \end{cases}$</li>
<li>$\text{Note: $y=0$ or $ 1$ always}$</li>
</ul>
</li>
<li>
<p>Then, $\text{Cost}(h_\theta(x), y)=-y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))$</p>
</li>
</ul>
<p>$$
\begin{align} J(\theta) &amp;= \frac{1}{m}\sum_{i =1}^{m}\text{Cost}\left(h_\theta(x^{(i)}, y^{(i)}\right) \
&amp;= -\frac{1}{m}\sum_{i =1}^{m}\left[ y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log \left(1-h_\theta(x^{(i)})\right) \right]
\end{align}
$$</p>
<ul>
<li>
<p>A vectorized implementation is:</p>
<ul>
<li>$h = g(X\theta)$</li>
<li>$J(\theta) = \frac{1}{m}\cdot \left(-y^T\log(h)-(1-y)^T\log(1-h)\right)$</li>
</ul>
</li>
<li>
<p>To fit parameters $\theta : \mathop{\text{min}}\limits_\theta J(\theta)$</p>
</li>
<li>
<p>To make a prediction given new $x$ : output $h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$</p>
</li>
<li>
<p>Gradient descent:</p>
<p>$$
\left.
\begin{array}{l}
\text{repeat } {\
\qquad \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \newline
\qquad (\text{simultaneously update all $\theta_j$) } \newline
}
\end{array}
\right.
$$</p>
<p>(Algorithm <em>looks</em> identical to linear regression!)</p>
</li>
<li>
<p>A vectorized implementation is: $\theta := \theta - \frac{\alpha}{m}X^T\left( g(X\theta)- \vec{y} \right)$</p>
</li>
</ul>
<h4 id="83-advanced-optimization">8.3 Advanced Optimization<a hidden class="anchor" aria-hidden="true" href="#83-advanced-optimization">#</a></h4>
<ul>
<li>
<p>Cost function $J(\theta)$. Want $\mathop{\text{min}}\limits_\theta J(\theta)$.</p>
</li>
<li>
<p>Given $\theta$, we have code that can compute</p>
<ul>
<li>$J(\theta)$</li>
<li>$\frac{\partial}{\partial \theta_j}J(\theta), \ \text{(for $j = 0, 1, \dots,n$)} $</li>
</ul>
</li>
<li>
<p>Optimization algorithms:</p>
<ul>
<li>(In this class) Gradient descent</li>
<li>Conjugate gradient, BFGS, L-BFGS
<ul>
<li>Advantages:
<ul>
<li>No need to manually pick $\alpha$</li>
<li>Often faster that gradient descent</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>More sophisticated</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="9-multiclass-classification">9 Multiclass classification<a hidden class="anchor" aria-hidden="true" href="#9-multiclass-classification">#</a></h2>
<h4 id="91-multiclass-classification-one-vs-all">9.1 Multiclass Classification: One-vs-all<a hidden class="anchor" aria-hidden="true" href="#91-multiclass-classification-one-vs-all">#</a></h4>
<ul>
<li>
<p>$y = \lbrace 0, 1, \dots, n \rbrace$</p>
<p>$$     h_\theta^{(0)}(x) = P(y=0 \vert x;\theta) \ h_\theta^{(1)}(x) = P(y=1 \vert x;\theta) \     \cdots \     h_\theta^{(n)}(x) = P(y=n \vert x;\theta) \     \text{prediction} = \mathop{\text{max}}\limits_i h_\theta^{(i)}(x) $$</p>
</li>
<li>
<p>The following image shows how one could classify 3 classes:
<img loading="lazy" src="https://i.loli.net/2020/03/06/wnGJg5QLbKmoN6V.png" alt="image.png"  />
</p>
</li>
<li>
<p>One-vs-all (one-vs-rest):</p>
<ul>
<li>$h_\theta^{(i)} = P(y=i \vert x;\theta)\quad (i=1,2,\dots,n)$</li>
<li>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y = i$.</li>
<li>On a new input $x$, to make a prediction, pick the class i that maximizes $h_\theta(x)$ : $\mathop{\text{max}}\limits_i h_\theta^{(i)}(x)$</li>
</ul>
</li>
</ul>
<h2 id="10-solving-the-problem-of-overfitting">10 Solving the Problem of Overfitting<a hidden class="anchor" aria-hidden="true" href="#10-solving-the-problem-of-overfitting">#</a></h2>
<h4 id="101-the-problem-of-overfitting">10.1 The Problem of Overfitting<a hidden class="anchor" aria-hidden="true" href="#101-the-problem-of-overfitting">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Overfitting<!-- raw HTML omitted -->: If we have too many features, the learned hypothesis may fit the training set well ($J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}\left( h_\theta(x^{(i)}-y^{(i)}) \right)^2\approx 0$), but fail to generalize to new new examples (predict prices on new examples).
<img loading="lazy" src="https://i.loli.net/2020/03/07/kHOjCN48IvSs9Ec.png" alt="image.png"  />
 <img loading="lazy" src="https://i.loli.net/2020/03/07/O5buMDzp1KPeNik.png" alt="image.png"  />
</li>
<li>Options of addressing overfitting
<ul>
<li>Reduce number of features
<ul>
<li>Manually select which features to keep</li>
<li>Model selection algorithm</li>
</ul>
</li>
<li>Regularization
<ul>
<li>Keep all the features, but reduce magnitude/values of parameters $\theta_j$</li>
<li>Regularization works well when we have a lot of slightly useful features, each of which contributes a bit to predicting $y$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="102-cost-function">10.2 Cost Function<a hidden class="anchor" aria-hidden="true" href="#102-cost-function">#</a></h4>
<ul>
<li>
<p><!-- raw HTML omitted -->Regularization<!-- raw HTML omitted -->: small values for parameters $\theta_0, \theta_1, \dots, \theta_n$</p>
<ul>
<li>&ldquo;Simpler&rdquo; hypothesis</li>
<li>Less prone to overfitting</li>
</ul>
</li>
<li>
<p>Cost function after regularization:</p>
<p>$$
J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^m\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n \theta_j^2 \right]
$$</p>
<ul>
<li>$\lambda: \text{regularization parameter}$</li>
</ul>
</li>
</ul>
<h4 id="103-regularized-linear-regression">10.3 Regularized Linear Regression<a hidden class="anchor" aria-hidden="true" href="#103-regularized-linear-regression">#</a></h4>
<ul>
<li>
<p>Gradient descent:</p>
<p>$$
\left.
\begin{array}{l}
\text{repeat } {\
\qquad \theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)} \newline
\qquad  \theta_j := \theta_j - \alpha \frac{1}{m} \left[ \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} + \lambda\theta_j \right] \quad j\in\lbrace 1,2,\dots,n \rbrace \newline
}
\end{array}
\right.
$$</p>
<ul>
<li>When $j=1, 2, \dots, n$, you can also write $\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}$</li>
<li>$1-\alpha\frac{\lambda}{m} &lt; 1$</li>
</ul>
</li>
<li>
<p>Normal equaiton</p>
<p>$$
X=\begin{bmatrix} (x^{(1)})^T \newline \vdots \newline (x^{(m)})^T \end{bmatrix}<em>{m\times(n+1)}
\quad
y=\begin{bmatrix} y^{(1)} \newline \vdots \newline x^{(m)} \end{bmatrix}\in\mathbb{R^{m}}
\\
\text{To } \mathop{\text{min}}\limits</em>\theta J(\theta), \ \theta=\left(X^TX+\lambda
\begin{bmatrix}
0 &amp;  &amp;  &amp; \
&amp; 1 &amp; &amp; \
&amp;   &amp; \ddots \
&amp;   &amp;  &amp; 1
\end{bmatrix} \right)^{-1}X^Ty
$$</p>
</li>
<li>
<p>Non-invertibility</p>
<ul>
<li>Suppose $\mathop{m}\limits_{\text{#examples}} \leq \mathop{n}\limits_{\text{#features}} $, then $X^TX$ will be <em>non-invertible/singluar</em>.</li>
<li>$\text{If } \lambda &gt; 0, \  \theta=\left(\underbrace{X^TX+\lambda \begin{bmatrix} 0 &amp;  \newline  &amp; I_n \end{bmatrix} }_{\text{invertible}}  \right)^{-1}X^Ty$</li>
</ul>
</li>
</ul>
<h4 id="104-regularized-logistic-regression">10.4 Regularized Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#104-regularized-logistic-regression">#</a></h4>
<ul>
<li>
<p>We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pick line, is less likely to overfit than the non-regularized function represented by the blue line:
<img loading="lazy" src="https://i.loli.net/2020/03/07/TWQj2DiGIBwxOnz.png" alt="image.png"  />
</p>
</li>
<li>
<p>Regularized cost function:</p>
<p>$$
J(\theta)= -\frac{1}{m}\sum_{i =1}^{m}\left[ y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log \left(1-h_\theta(x^{(i)})\right) \right] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$</p>
<ul>
<li>$\sum_{j=1}^{n}\theta_j^2$ means to explicitly exclude the bias term, $\theta_0$.</li>
</ul>
</li>
<li>
<p>Gradient descent:</p>
<p>$$
\left.  \begin{array}{l}  \text{repeat } {\  \qquad \theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)} \newline  \qquad  \theta_j := \theta_j - \alpha \frac{1}{m} \left[ \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} + \lambda\theta_j \right] \quad j\in\lbrace 1,2,\dots,n \rbrace \newline  }  \end{array}  \right.
$$</p>
</li>
</ul>
<h2 id="ex2-logistic-regression">Ex2: Logistic Regressionüë®‚Äçüíª<a hidden class="anchor" aria-hidden="true" href="#ex2-logistic-regression">#</a></h2>
<blockquote>
<p>See this exercise on <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python/blob/master/ex2/ex2.ipynb">Coursera-MachineLearning-Python/ex2/ex2.ipynb</a></p>
</blockquote>
<h4 id="ex21-logistic-regression">Ex2.1 Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#ex21-logistic-regression">#</a></h4>
<p><strong>Instruction:</strong>
In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> scipy.optimize <span style="color:#66d9ef">as</span> op
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 0: Loading Data and Defining Functions &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Hypothesis function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">h</span>(X, theta):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>dot(X, theta)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Logistic regression cost function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">costFunction</span>(theta, X, y):
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(h(X, theta)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> h(X, theta))) <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>    grad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X<span style="color:#f92672">.</span>T, h(X, theta) <span style="color:#f92672">-</span> y) <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cost, grad
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The objective function to be minimized</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">costFunc</span>(params, <span style="color:#f92672">*</span>args):
</span></span><span style="display:flex;"><span>    X, y <span style="color:#f92672">=</span> args
</span></span><span style="display:flex;"><span>    [m, n] <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>    theta <span style="color:#f92672">=</span> params<span style="color:#f92672">.</span>reshape([n, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>    cost <span style="color:#f92672">=</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(h(X, theta)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> h(X, theta))) <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cost
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Method for computing the gradient vector</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradFunc</span>(params, <span style="color:#f92672">*</span>args):
</span></span><span style="display:flex;"><span>    X, y <span style="color:#f92672">=</span> args
</span></span><span style="display:flex;"><span>    [m, n] <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>    theta <span style="color:#f92672">=</span> params<span style="color:#f92672">.</span>reshape([n, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>    grad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X<span style="color:#f92672">.</span>T, h(X, theta) <span style="color:#f92672">-</span> y) <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> grad<span style="color:#f92672">.</span>flatten()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The first two columns contains the exam scores and the third column contains the label</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Loading Data ... &#39;</span>)
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;ex2data1.txt&#39;</span>, dtype<span style="color:#f92672">=</span>float, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>)
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>], data[:, <span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 1: Plotting &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>fig0, ax0 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>label1, label0 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(y<span style="color:#f92672">.</span>ravel() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>), np<span style="color:#f92672">.</span>where(y<span style="color:#f92672">.</span>ravel() <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>scatter(X[label1, <span style="color:#ae81ff">0</span>], X[label1, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;+&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Admitted&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>scatter(X[label0, <span style="color:#ae81ff">0</span>], X[label0, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Not admitted&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Exam 1 Score&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Exam 2 Score&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 2: Compute Cost and Gradient &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>[m, n] <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[np<span style="color:#f92672">.</span>ones([m, <span style="color:#ae81ff">1</span>]), X]           <span style="color:#75715e"># Add intercept term to x and X_test</span>
</span></span><span style="display:flex;"><span>initial_theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros([n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>])    <span style="color:#75715e"># Initialize fitting parameters</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute and display initial cost and gradient</span>
</span></span><span style="display:flex;"><span>cost, grad <span style="color:#f92672">=</span> costFunction(initial_theta, X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Cost at initial theta : &#39;</span>, cost<span style="color:#f92672">.</span>ravel())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost (approx): 0.693&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Gradient at initial theta :&#39;</span>, grad<span style="color:#f92672">.</span>ravel())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected gradients (approx): </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">-0.1000</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> -12.0092</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> -11.2628&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute and display cost and gradient with non-zero theta</span>
</span></span><span style="display:flex;"><span>test_theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(([<span style="color:#f92672">-</span><span style="color:#ae81ff">24</span>], [<span style="color:#ae81ff">0.2</span>], [<span style="color:#ae81ff">0.2</span>]))
</span></span><span style="display:flex;"><span>cost, grad <span style="color:#f92672">=</span> costFunction(test_theta, X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Cost at test theta : &#39;</span>, cost<span style="color:#f92672">.</span>ravel())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost (approx): 0.218&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Gradient at test theta :&#39;</span>, grad<span style="color:#f92672">.</span>ravel())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected gradients (approx): </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">0.043</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> 2.5662</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> 2.647&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 3: Optimizing using fminunc &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>params <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros([n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>args <span style="color:#f92672">=</span> (X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># uUse Newton Conjugate Gradient algorithm to obtain the optimal theta</span>
</span></span><span style="display:flex;"><span>res <span style="color:#f92672">=</span> op<span style="color:#f92672">.</span>minimize(fun<span style="color:#f92672">=</span>costFunc, x0<span style="color:#f92672">=</span>params, args<span style="color:#f92672">=</span>args, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;TNC&#39;</span>, jac<span style="color:#f92672">=</span>gradFunc)
</span></span><span style="display:flex;"><span>cost, theta <span style="color:#f92672">=</span> res<span style="color:#f92672">.</span>fun, res<span style="color:#f92672">.</span>x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Cost at theta found by fminunc: &#39;</span>, cost)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost (approx): 0.203&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;theta: &#39;</span>, theta)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected theta (approx): </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">-25.161</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> 0.206</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> 0.201&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot boundary</span>
</span></span><span style="display:flex;"><span>x1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(min(X[:, <span style="color:#ae81ff">1</span>]), max(X[:, <span style="color:#ae81ff">1</span>]), <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>x2 <span style="color:#f92672">=</span> (<span style="color:#f92672">-</span>theta[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> theta[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> x1) <span style="color:#f92672">/</span> theta[<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x1, x2, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 4: Predict and Accuracies &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>prob <span style="color:#f92672">=</span> h(np<span style="color:#f92672">.</span>array(([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">45</span>, <span style="color:#ae81ff">85</span>])), theta)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">For a student with scores 45 and 85, we predict an adimission probability of &#39;</span>, prob)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected value: 0.775 +/- 0.002&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(h(X, theta) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Train accuracy: &#39;</span>, np<span style="color:#f92672">.</span>mean(p <span style="color:#f92672">==</span> y<span style="color:#f92672">.</span>flatten()) <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#39;%&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected accuracy (approx): 89.0 %</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console
<img loading="lazy" src="https://i.loli.net/2020/03/09/kEjqygun1US25Xc.png" alt="image.png"  />
</li>
<li>Training data with decision boundary
<img loading="lazy" src="https://i.loli.net/2020/03/09/8PeaQu5iAqz3kFm.png" alt="Figure_1.png"  />
</li>
</ul>
<h4 id="ex22-regularized-logistic-regression">Ex2.2 Regularized Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#ex22-regularized-logistic-regression">#</a></h4>
<p><strong>Instruction:</strong>
In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> PolynomialFeatures
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 0: Loading Data and Defining Functions &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Feature mapping function to polynomial features</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mapFeature</span>(X1, X2):
</span></span><span style="display:flex;"><span>    degree <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>    X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones([len(X1), <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, degree <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[X, X1<span style="color:#f92672">**</span>(i<span style="color:#f92672">-</span>j) <span style="color:#f92672">*</span> X2<span style="color:#f92672">**</span>(j)]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> X
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Hypothesis function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">h</span>(X, theta):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>dot(X, theta)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute cost and gradient for logistic regression with regularization</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">costFunctionReg</span>(theta, X, y, reg_param):
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>    cost1 <span style="color:#f92672">=</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(h(X, theta)) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> h(X, theta))) <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>    cost2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> reg_param <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>dot(theta[<span style="color:#ae81ff">1</span>:]<span style="color:#f92672">.</span>T, theta[<span style="color:#ae81ff">1</span>:]) <span style="color:#f92672">/</span> m    <span style="color:#75715e"># Don&#39;t penalize theta_0</span>
</span></span><span style="display:flex;"><span>    cost <span style="color:#f92672">=</span> cost1 <span style="color:#f92672">+</span> cost2
</span></span><span style="display:flex;"><span>    grad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X<span style="color:#f92672">.</span>T, h(X, theta) <span style="color:#f92672">-</span> y) <span style="color:#f92672">/</span> m
</span></span><span style="display:flex;"><span>    grad[<span style="color:#ae81ff">1</span>:] <span style="color:#f92672">+=</span> (reg_param <span style="color:#f92672">*</span> theta <span style="color:#f92672">/</span> m)[<span style="color:#ae81ff">1</span>:]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cost, grad
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Use Batch Gradient Descent algorithm to minimize cost</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batchGradientDescent</span>(X, y, theta, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">2000</span>, reg<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    J_history <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(iters)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(iters):
</span></span><span style="display:flex;"><span>        cost, grad <span style="color:#f92672">=</span> costFunctionReg(theta, X, y, reg)
</span></span><span style="display:flex;"><span>        theta <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> alpha <span style="color:#f92672">*</span> grad
</span></span><span style="display:flex;"><span>        J_history[i] <span style="color:#f92672">=</span> cost
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> theta, J_history
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The first two columns contains the exam scores and the third column contains the label</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Loading Data ... &#39;</span>)
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;ex2data2.txt&#39;</span>, dtype<span style="color:#f92672">=</span>float, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>)
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>], data[:, <span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot data</span>
</span></span><span style="display:flex;"><span>fig0, ax0 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>label1, label0 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(y<span style="color:#f92672">.</span>ravel() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>), np<span style="color:#f92672">.</span>where(y<span style="color:#f92672">.</span>ravel() <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>scatter(X[label1, <span style="color:#ae81ff">0</span>], X[label1, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;+&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y = 1&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>scatter(X[label0, <span style="color:#ae81ff">0</span>], X[label0, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;y = 0&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 1: Regularized Logistic Regression &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> mapFeature(X[:, <span style="color:#ae81ff">0</span>], X[:, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize fitting parameters</span>
</span></span><span style="display:flex;"><span>initial_theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros([X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set regularization parameter lambda to 1</span>
</span></span><span style="display:flex;"><span>reg_param <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute and display inital cost gradient for regularized logistic regression</span>
</span></span><span style="display:flex;"><span>cost0, grad0 <span style="color:#f92672">=</span> costFunctionReg(initial_theta, X, y, reg_param)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Cost at initial theta (zeros): &#39;</span>, cost0<span style="color:#f92672">.</span>flatten())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost (approx): 0.693&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Gradient at initial theta (zeros) - first five values only: &#39;</span>
</span></span><span style="display:flex;"><span>      , np<span style="color:#f92672">.</span>around(grad0[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">5</span>], <span style="color:#ae81ff">4</span>)<span style="color:#f92672">.</span>flatten())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected gradients (approx) - first five values only: &#39;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#39;0.0085 0.0188 0.0001 0.0503, 0.0115&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute and display cost and gradient with all-ones theta and lambda = 10</span>
</span></span><span style="display:flex;"><span>test_theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones([X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>cost1, grad1 <span style="color:#f92672">=</span> costFunctionReg(test_theta, X, y, reg_param<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Cost at test theta (with lambda = 10): &#39;</span>, cost1<span style="color:#f92672">.</span>flatten())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected cost (approx): 3.16&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Gradient at test theta - first five values only: &#39;</span>
</span></span><span style="display:flex;"><span>      , np<span style="color:#f92672">.</span>around(grad1[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">5</span>], <span style="color:#ae81ff">4</span>)<span style="color:#f92672">.</span>flatten())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected gradients (approx) - first five values only: &#39;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#39;0.3460 0.1614 0.1948 0.2269, 0.0922&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39; Part 2: Regularization and Accuracies &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Optimize</span>
</span></span><span style="display:flex;"><span>theta, J_history <span style="color:#f92672">=</span> batchGradientDescent(X, y, initial_theta, reg<span style="color:#f92672">=</span>reg_param)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># fig1, ax1 = plt.subplots()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ax1.plot(np.arange(2000), J_history, &#39;c&#39;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot boundary</span>
</span></span><span style="display:flex;"><span>poly <span style="color:#f92672">=</span> PolynomialFeatures(<span style="color:#ae81ff">6</span>)
</span></span><span style="display:flex;"><span>x1min, x1max, x2min, x2max <span style="color:#f92672">=</span> X[:, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>min(), X[:, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>max(), X[:, <span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>min(), X[:, <span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>max()
</span></span><span style="display:flex;"><span>xx1, xx2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(np<span style="color:#f92672">.</span>linspace(x1min, x1max), np<span style="color:#f92672">.</span>linspace(x2min, x2max))
</span></span><span style="display:flex;"><span>bd <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>poly<span style="color:#f92672">.</span>fit_transform(np<span style="color:#f92672">.</span>c_[xx1<span style="color:#f92672">.</span>ravel(), xx2<span style="color:#f92672">.</span>ravel()])<span style="color:#f92672">.</span>dot(theta)))
</span></span><span style="display:flex;"><span>bd <span style="color:#f92672">=</span> bd<span style="color:#f92672">.</span>reshape(xx2<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>CS <span style="color:#f92672">=</span> ax0<span style="color:#f92672">.</span>contour(xx1, xx2, bd, [<span style="color:#ae81ff">0.5</span>], colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;c&#39;</span>)
</span></span><span style="display:flex;"><span>CS<span style="color:#f92672">.</span>collections[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_label(<span style="color:#e6db74">&#39;Decision</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Boundary&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\lambda$ = &#39;</span><span style="color:#f92672">+</span>str(reg_param))
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Microchip Test 1&#39;</span>)
</span></span><span style="display:flex;"><span>ax0<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Microchip Test 2&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute accuracy on our training set</span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(h(X, theta) <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Train Accuracy: &#39;</span>, np<span style="color:#f92672">.</span>mean(p <span style="color:#f92672">==</span> y) <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#39;%&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Expected accuracy (with lambda = 1): 83.1 % (approx)&#39;</span>)
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console ((Œª = 1)
<img loading="lazy" src="https://i.loli.net/2020/03/10/mL5Uq16nKgifRuB.png" alt="image.png"  />
</li>
<li>Training data with decision boundary (Œª = 1)
<img loading="lazy" src="https://i.loli.net/2020/03/10/K29fFZimhoAuxrb.png" alt="image.png"  />
</li>
<li>Too much regularization (Underfitting) (Œª = 100)
<img loading="lazy" src="https://i.loli.net/2020/03/10/eG36QAIqsiwbZ45.png" alt="image.png"  />
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://fang-lansheng.github.io/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/python/">Python</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://fang-lansheng.github.io">My-Thistledown</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
