<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ | Jifan&#39;s Blog</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ÔºöWeek 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&rsquo;re trying to predict">
<meta name="author" content="Thistledown">
<link rel="canonical" href="https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://fang-lansheng.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://fang-lansheng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://fang-lansheng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://fang-lansheng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://fang-lansheng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ" />
<meta property="og:description" content="Course Link ÔºöWeek 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&rsquo;re trying to predict" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-04T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-03-04T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ"/>
<meta name="twitter:description" content="Course Link ÔºöWeek 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&rsquo;re trying to predict"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://fang-lansheng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ",
      "item": "https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ",
  "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ",
  "description": "Course Link ÔºöWeek 3 - Logistic Regression\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n  For now, we will focus on the binary classification problem.\n  $y \\in {0,1}$ - the variable that we\u0026rsquo;re trying to predict",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ÔºöWeek 3 - Logistic Regression\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n  For now, we will focus on the binary classification problem.\n  $y \\in {0,1}$ - the variable that we‚Äôre trying to predict\n $0: \\text{‚ÄúNegative Class‚Äù}$ (e.g., benign tumor) $1: \\text{‚ÄúPositive Class‚Äù}$ (e.g., malignant tumor)    Applying linear regression to a classification problem often isn‚Äôt a great idea.\n  Logistic Regression: $0\\leq h_\\theta(x) \\leq 1$ (BTW, this is actually a classification algorithm)\n  7.2 Hypothesis Representation  Logistic Regression Model: want $0 \\leq h_\\theta(x) \\leq 1$ $h_\\theta(x) = g(\\theta^Tx)$  Sigmoid function (or logistic function): $g(z) = \\frac{1}{1 + e^{-z}}$. The following image shows us what the sigmoid function looks like:  $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$   Interpretation of hypothesis output:  $h_\\theta(x) = \\text{estimated probability that } y=1 \\text{ on input }x$ $h_\\theta(x)= P(y=1 \\vert x;\\theta)$ : probability that $y = 1$, given $x$, parameterized by $\\theta$ $P(y=0 \\vert x;\\theta) + P(y=1 \\vert x;\\theta) = 1 \\\\ P(y=0 \\vert x;\\theta) = 1 - P(y=1 \\vert x;\\theta)$    7.3 Decision Boundary  Logistic regression: $h_\\theta(x) = g(\\theta^Tx), \\quad g(z)=\\frac{1}{1+e^{-z}}$  Suppose predict $y=\\begin{cases} 0, \u0026 \\text {if $h_\\theta(x) \\geq$ 0.5} \\newline 1, \u0026 \\text{if $h_\\theta(x)   Decision boundary: the decision boundary is the line that separates the area where $y=0$ and where $y=1$.  It is created by our hypothesis function The input to the sigmoid function of $g(z)$ doesn‚Äôt need to be linear, and could be a function that describes a circle or any shape to fit the data    8 Logistic Regression Model 8.1 Cost Function  Logistic regression cost function:  $$ J(\\theta) = \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}(h_\\theta(x^{(i)}, y^{(i)} ) $$\n $\\text{Cost}(h_\\theta(x), y)=\\begin{cases} -\\log(h_\\theta(x)), \u0026 \\text {if } y =1 \\newline -\\log(1-h_\\theta(x)), \u0026 \\text{if } y=0 \\end{cases}$  $\\text{Cost}(h_\\theta(x), y) = 0, \\text{ if } h_\\theta(x) = y$ $\\text{Cost}(h_\\theta(x), y) \\rightarrow \\infty, \\text{ if $y=0$ and $h_\\theta(x) \\rightarrow 1$} $ $\\text{Cost}(h_\\theta(x), y) \\rightarrow \\infty, \\text{ if $y=1$ and $h_\\theta(x) \\rightarrow 0$} $   When $y=1$, we get the following plot for $J(\\theta) \\text{ vs } h_\\theta(x)$  Similarly, when $y=0$, we get the following plot for $J(\\theta) \\text{ vs } h_\\theta(x)$   8.2 Simplified Cost Function and Gradient Descent   Logistic regression cost function\n $J(\\theta) = \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}(h_\\theta(x^{(i)}, y^{(i)} )$ $\\text{Cost}(h_\\theta(x), y)=\\begin{cases} -\\log(h_\\theta(x)), \u0026 \\text {if } y =1 \\newline -\\log(1-h_\\theta(x)), \u0026 \\text{if } y=0 \\end{cases}$ $\\text{Note: $y=0$ or $ 1$ always}$    Then, $\\text{Cost}(h_\\theta(x), y)=-y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))$\n  $$ \\begin{align} J(\\theta) \u0026= \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}\\left(h_\\theta(x^{(i)}, y^{(i)}\\right) \\ \u0026= -\\frac{1}{m}\\sum_{i =1}^{m}\\left[ y^{(i)}\\log h_\\theta(x^{(i)})+(1-y^{(i)})\\log \\left(1-h_\\theta(x^{(i)})\\right) \\right] \\end{align} $$\n  A vectorized implementation is:\n $h = g(X\\theta)$ $J(\\theta) = \\frac{1}{m}\\cdot \\left(-y^T\\log(h)-(1-y)^T\\log(1-h)\\right)$    To fit parameters $\\theta : \\mathop{\\text{min}}\\limits_\\theta J(\\theta)$\n  To make a prediction given new $x$ : output $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n  Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\newline \\qquad (\\text{simultaneously update all $\\theta_j$) } \\newline } \\end{array} \\right. $$\n(Algorithm looks identical to linear regression!)\n  A vectorized implementation is: $\\theta := \\theta - \\frac{\\alpha}{m}X^T\\left( g(X\\theta)- \\vec{y} \\right)$\n  8.3 Advanced Optimization   Cost function $J(\\theta)$. Want $\\mathop{\\text{min}}\\limits_\\theta J(\\theta)$.\n  Given $\\theta$, we have code that can compute\n $J(\\theta)$ $\\frac{\\partial}{\\partial \\theta_j}J(\\theta), \\ \\text{(for $j = 0, 1, \\dots,n$)} $    Optimization algorithms:\n (In this class) Gradient descent Conjugate gradient, BFGS, L-BFGS  Advantages:  No need to manually pick $\\alpha$ Often faster that gradient descent   Disadvantages:  More sophisticated        9 Multiclass classification 9.1 Multiclass Classification: One-vs-all   $y = \\lbrace 0, 1, \\dots, n \\rbrace$\n$$ h_\\theta^{(0)}(x) = P(y=0 \\vert x;\\theta) \\ h_\\theta^{(1)}(x) = P(y=1 \\vert x;\\theta) \\ \\cdots \\ h_\\theta^{(n)}(x) = P(y=n \\vert x;\\theta) \\ \\text{prediction} = \\mathop{\\text{max}}\\limits_i h_\\theta^{(i)}(x) $$\n  The following image shows how one could classify 3 classes:   One-vs-all (one-vs-rest):\n $h_\\theta^{(i)} = P(y=i \\vert x;\\theta)\\quad (i=1,2,\\dots,n)$ Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y = i$. On a new input $x$, to make a prediction, pick the class i that maximizes $h_\\theta(x)$ : $\\mathop{\\text{max}}\\limits_i h_\\theta^{(i)}(x)$    10 Solving the Problem of Overfitting 10.1 The Problem of Overfitting  Overfitting: If we have too many features, the learned hypothesis may fit the training set well ($J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left( h_\\theta(x^{(i)}-y^{(i)}) \\right)^2\\approx 0$), but fail to generalize to new new examples (predict prices on new examples).  Options of addressing overfitting  Reduce number of features  Manually select which features to keep Model selection algorithm   Regularization  Keep all the features, but reduce magnitude/values of parameters $\\theta_j$ Regularization works well when we have a lot of slightly useful features, each of which contributes a bit to predicting $y$      10.2 Cost Function   Regularization: small values for parameters $\\theta_0, \\theta_1, \\dots, \\theta_n$\n ‚ÄúSimpler‚Äù hypothesis Less prone to overfitting    Cost function after regularization:\n$$ J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m\\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda\\sum_{j=1}^n \\theta_j^2 \\right] $$\n $\\lambda: \\text{regularization parameter}$    10.3 Regularized Linear Regression   Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_0^{(i)} \\newline \\qquad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\left[ \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)} + \\lambda\\theta_j \\right] \\quad j\\in\\lbrace 1,2,\\dots,n \\rbrace \\newline } \\end{array} \\right. $$\n When $j=1, 2, \\dots, n$, you can also write $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}$ $1-\\alpha\\frac{\\lambda}{m}    Normal equaiton\n$$ X=\\begin{bmatrix} (x^{(1)})^T \\newline \\vdots \\newline (x^{(m)})^T \\end{bmatrix}{m\\times(n+1)} \\quad y=\\begin{bmatrix} y^{(1)} \\newline \\vdots \\newline x^{(m)} \\end{bmatrix}\\in\\mathbb{R^{m}} \\\\ \\text{To } \\mathop{\\text{min}}\\limits\\theta J(\\theta), \\ \\theta=\\left(X^TX+\\lambda \\begin{bmatrix} 0 \u0026 \u0026 \u0026 \\ \u0026 1 \u0026 \u0026 \\ \u0026 \u0026 \\ddots \\ \u0026 \u0026 \u0026 1 \\end{bmatrix} \\right)^{-1}X^Ty $$\n  Non-invertibility\n Suppose $\\mathop{m}\\limits_{\\text{#examples}} \\leq \\mathop{n}\\limits_{\\text{#features}} $, then $X^TX$ will be non-invertible/singluar. $\\text{If } \\lambda  0, \\ \\theta=\\left(\\underbrace{X^TX+\\lambda \\begin{bmatrix} 0 \u0026 \\newline \u0026 I_n \\end{bmatrix} }_{\\text{invertible}} \\right)^{-1}X^Ty$    10.4 Regularized Logistic Regression   We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pick line, is less likely to overfit than the non-regularized function represented by the blue line:   Regularized cost function:\n$$ J(\\theta)= -\\frac{1}{m}\\sum_{i =1}^{m}\\left[ y^{(i)}\\log h_\\theta(x^{(i)})+(1-y^{(i)})\\log \\left(1-h_\\theta(x^{(i)})\\right) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2 $$\n $\\sum_{j=1}^{n}\\theta_j^2$ means to explicitly exclude the bias term, $\\theta_0$.    Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_0^{(i)} \\newline \\qquad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\left[ \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)} + \\lambda\\theta_j \\right] \\quad j\\in\\lbrace 1,2,\\dots,n \\rbrace \\newline } \\end{array} \\right. $$\n  Ex2: Logistic Regressionüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex2/ex2.ipynb\n Ex2.1 Logistic Regression Instruction: In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\nCode:\nimport numpy as np import matplotlib.pyplot as plt import scipy.optimize as op ''' Part 0: Loading Data and Defining Functions ''' # Hypothesis function def h(X, theta): return 1 / (1 + np.exp(-np.dot(X, theta))) # Logistic regression cost function def costFunction(theta, X, y): m = len(y) cost = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m grad = np.dot(X.T, h(X, theta) - y) / m return cost, grad # The objective function to be minimized def costFunc(params, *args): X, y = args [m, n] = X.shape theta = params.reshape([n, 1]) cost = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m return cost # Method for computing the gradient vector def gradFunc(params, *args): X, y = args [m, n] = X.shape theta = params.reshape([n, 1]) grad = np.dot(X.T, h(X, theta) - y) / m return grad.flatten() # The first two columns contains the exam scores and the third column contains the label print('Loading Data ... ') data = np.loadtxt('ex2data1.txt', dtype=float, delimiter=',') X, y = data[:, 0:2], data[:, 2:3] ''' Part 1: Plotting ''' fig0, ax0 = plt.subplots() label1, label0 = np.where(y.ravel() == 1), np.where(y.ravel() == 0) ax0.scatter(X[label1, 0], X[label1, 1], marker='+', color='g', label='Admitted') ax0.scatter(X[label0, 0], X[label0, 1], marker='x', color='r', label='Not admitted') ax0.legend(loc='upper right') ax0.set_xlabel('Exam 1 Score') ax0.set_ylabel('Exam 2 Score') ''' Part 2: Compute Cost and Gradient ''' [m, n] = X.shape X = np.c_[np.ones([m, 1]), X] # Add intercept term to x and X_test initial_theta = np.zeros([n + 1, 1]) # Initialize fitting parameters # Compute and display initial cost and gradient cost, grad = costFunction(initial_theta, X, y) print('\\nCost at initial theta : ', cost.ravel()) print('Expected cost (approx): 0.693') print('Gradient at initial theta :', grad.ravel()) print('Expected gradients (approx): \\t-0.1000\\t-12.0092\\t-11.2628') # Compute and display cost and gradient with non-zero theta test_theta = np.array(([-24], [0.2], [0.2])) cost, grad = costFunction(test_theta, X, y) print('\\nCost at test theta : ', cost.ravel()) print('Expected cost (approx): 0.218') print('Gradient at test theta :', grad.ravel()) print('Expected gradients (approx): \\t0.043\\t2.5662\\t2.647') ''' Part 3: Optimizing using fminunc ''' params = np.zeros([n + 1, 1]) args = (X, y) # uUse Newton Conjugate Gradient algorithm to obtain the optimal theta res = op.minimize(fun=costFunc, x0=params, args=args, method='TNC', jac=gradFunc) cost, theta = res.fun, res.x print('\\nCost at theta found by fminunc: ', cost) print('Expected cost (approx): 0.203') print('theta: ', theta) print('Expected theta (approx): \\t-25.161\\t0.206\\t0.201') # Plot boundary x1 = np.arange(min(X[:, 1]), max(X[:, 1]), 1) x2 = (-theta[0] - theta[1] * x1) / theta[2] plt.plot(x1, x2, color='blue') plt.show() ''' Part 4: Predict and Accuracies ''' prob = h(np.array(([1, 45, 85])), theta) print('\\nFor a student with scores 45 and 85, we predict an adimission probability of ', prob) print('Expected value: 0.775 +/- 0.002') p = np.where(h(X, theta)  0.5, 1.0, 0.0) print('Train accuracy: ', np.mean(p == y.flatten()) * 100, '%') print('Expected accuracy (approx): 89.0 %\\n') Output:\n Console  Training data with decision boundary   Ex2.2 Regularized Logistic Regression Instruction: In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nCode:\nimport numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures ''' Part 0: Loading Data and Defining Functions ''' # Feature mapping function to polynomial features def mapFeature(X1, X2): degree = 6 X = np.ones([len(X1), 1]) for i in np.arange(1, degree + 1, 1): for j in range(i + 1): X = np.c_[X, X1**(i-j) * X2**(j)] return X # Hypothesis function def h(X, theta): return 1 / (1 + np.exp(-np.dot(X, theta))) # Compute cost and gradient for logistic regression with regularization def costFunctionReg(theta, X, y, reg_param): m = len(y) cost1 = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m cost2 = 0.5 * reg_param * np.dot(theta[1:].T, theta[1:]) / m # Don't penalize theta_0 cost = cost1 + cost2 grad = np.dot(X.T, h(X, theta) - y) / m grad[1:] += (reg_param * theta / m)[1:] return cost, grad # Use Batch Gradient Descent algorithm to minimize cost def batchGradientDescent(X, y, theta, alpha=0.1, iters = 2000, reg=1): J_history = np.zeros(iters) for i in range(iters): cost, grad = costFunctionReg(theta, X, y, reg) theta = theta - alpha * grad J_history[i] = cost return theta, J_history # The first two columns contains the exam scores and the third column contains the label print('Loading Data ... ') data = np.loadtxt('ex2data2.txt', dtype=float, delimiter=',') X, y = data[:, 0:2], data[:, 2:3] # Plot data fig0, ax0 = plt.subplots() label1, label0 = np.where(y.ravel() == 1), np.where(y.ravel() == 0) ax0.scatter(X[label1, 0], X[label1, 1], marker='+', color='k', label='y = 1') ax0.scatter(X[label0, 0], X[label0, 1], marker='x', color='y', label='y = 0') ''' Part 1: Regularized Logistic Regression ''' m = len(y) X = mapFeature(X[:, 0], X[:, 1]) # Initialize fitting parameters initial_theta = np.zeros([X.shape[1], 1]) # Set regularization parameter lambda to 1 reg_param = 1 # Compute and display inital cost gradient for regularized logistic regression cost0, grad0 = costFunctionReg(initial_theta, X, y, reg_param) print('\\nCost at initial theta (zeros): ', cost0.flatten()) print('Expected cost (approx): 0.693') print('Gradient at initial theta (zeros) - first five values only: ' , np.around(grad0[0:5], 4).flatten()) print('Expected gradients (approx) - first five values only: ', '0.0085 0.0188 0.0001 0.0503, 0.0115') # Compute and display cost and gradient with all-ones theta and lambda = 10 test_theta = np.ones([X.shape[1], 1]) cost1, grad1 = costFunctionReg(test_theta, X, y, reg_param=10) print('\\nCost at test theta (with lambda = 10): ', cost1.flatten()) print('Expected cost (approx): 3.16') print('Gradient at test theta - first five values only: ' , np.around(grad1[0:5], 4).flatten()) print('Expected gradients (approx) - first five values only: ', '0.3460 0.1614 0.1948 0.2269, 0.0922') ''' Part 2: Regularization and Accuracies ''' # Optimize theta, J_history = batchGradientDescent(X, y, initial_theta, reg=reg_param) # fig1, ax1 = plt.subplots() # ax1.plot(np.arange(2000), J_history, 'c') # Plot boundary poly = PolynomialFeatures(6) x1min, x1max, x2min, x2max = X[:, 1].min(), X[:, 1].max(), X[:, 2].min(), X[:, 2].max() xx1, xx2 = np.meshgrid(np.linspace(x1min, x1max), np.linspace(x2min, x2max)) bd = 1 / (1 + np.exp(-poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(theta))) bd = bd.reshape(xx2.shape) CS = ax0.contour(xx1, xx2, bd, [0.5], colors='c') CS.collections[0].set_label('Decision\\nBoundary') ax0.set_title(r'$\\lambda$ = '+str(reg_param)) ax0.legend(loc='upper right') ax0.set_xlabel('Microchip Test 1') ax0.set_ylabel('Microchip Test 2') plt.show() # Compute accuracy on our training set p = np.where(h(X, theta) = 0.5, 1.0, 0.0) print('\\nTrain Accuracy: ', np.mean(p == y) * 100, '%') print('Expected accuracy (with lambda = 1): 83.1 % (approx)') Output:\n Console ((Œª = 1)  Training data with decision boundary (Œª = 1)  Too much regularization (Underfitting) (Œª = 100)   ",
  "wordCount" : "2144",
  "inLanguage": "en",
  "datePublished": "2020-03-04T00:00:00Z",
  "dateModified": "2020-03-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thistledown"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jifan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fang-lansheng.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://fang-lansheng.github.io" accesskey="h" title="Jifan&#39;s Blog (Alt + H)">Jifan&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://fang-lansheng.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://fang-lansheng.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://fang-lansheng.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ
    </h1>
    <div class="post-meta"><span title='2020-03-04 00:00:00 +0000 UTC'>March 4, 2020</span>&nbsp;¬∑&nbsp;Thistledown

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#7-classification-and-representation" aria-label="7 Classification and Representation">7 Classification and Representation</a><ul>
                        <ul>
                        
                <li>
                    <a href="#71-classification" aria-label="7.1 Classification">7.1 Classification</a></li>
                <li>
                    <a href="#72-hypothesis-representation" aria-label="7.2 Hypothesis Representation">7.2 Hypothesis Representation</a></li>
                <li>
                    <a href="#73-decision-boundary" aria-label="7.3 Decision Boundary">7.3 Decision Boundary</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#8-logistic-regression-model" aria-label="8 Logistic Regression Model">8 Logistic Regression Model</a><ul>
                        <ul>
                        
                <li>
                    <a href="#81-cost-function" aria-label="8.1 Cost Function">8.1 Cost Function</a></li>
                <li>
                    <a href="#82-simplified-cost-function-and-gradient-descent" aria-label="8.2 Simplified Cost Function and Gradient Descent">8.2 Simplified Cost Function and Gradient Descent</a></li>
                <li>
                    <a href="#83-advanced-optimization" aria-label="8.3 Advanced Optimization">8.3 Advanced Optimization</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#9-multiclass-classification" aria-label="9 Multiclass classification">9 Multiclass classification</a><ul>
                        <ul>
                        
                <li>
                    <a href="#91-multiclass-classification-one-vs-all" aria-label="9.1 Multiclass Classification: One-vs-all">9.1 Multiclass Classification: One-vs-all</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#10-solving-the-problem-of-overfitting" aria-label="10 Solving the Problem of Overfitting">10 Solving the Problem of Overfitting</a><ul>
                        <ul>
                        
                <li>
                    <a href="#101-the-problem-of-overfitting" aria-label="10.1 The Problem of Overfitting">10.1 The Problem of Overfitting</a></li>
                <li>
                    <a href="#102-cost-function" aria-label="10.2 Cost Function">10.2 Cost Function</a></li>
                <li>
                    <a href="#103-regularized-linear-regression" aria-label="10.3 Regularized Linear Regression">10.3 Regularized Linear Regression</a></li>
                <li>
                    <a href="#104-regularized-logistic-regression" aria-label="10.4 Regularized Logistic Regression">10.4 Regularized Logistic Regression</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#ex2-logistic-regression" aria-label="Ex2: Logistic Regressionüë®‚Äçüíª">Ex2: Logistic Regressionüë®‚Äçüíª</a><ul>
                        <ul>
                        
                <li>
                    <a href="#ex21-logistic-regression" aria-label="Ex2.1 Logistic Regression">Ex2.1 Logistic Regression</a></li>
                <li>
                    <a href="#ex22-regularized-logistic-regression" aria-label="Ex2.2 Regularized Logistic Regression">Ex2.2 Regularized Logistic Regression</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Course Link Ôºö<a href="https://www.coursera.org/learn/machine-learning/home/week/3">Week 3 - Logistic Regression</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="7-classification-and-representation">7 Classification and Representation<a hidden class="anchor" aria-hidden="true" href="#7-classification-and-representation">#</a></h2>
<h4 id="71-classification">7.1 Classification<a hidden class="anchor" aria-hidden="true" href="#71-classification">#</a></h4>
<ul>
<li>
<p>The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.</p>
</li>
<li>
<p>For now, we will focus on the <!-- raw HTML omitted -->binary classification problem<!-- raw HTML omitted -->.</p>
</li>
<li>
<p>$y \in {0,1}$ - the variable that we&rsquo;re trying to predict</p>
<ul>
<li>$0: \text{&ldquo;Negative Class&rdquo;}$ (e.g., benign tumor)</li>
<li>$1: \text{&ldquo;Positive Class&rdquo;}$ (e.g., malignant tumor)</li>
</ul>
</li>
<li>
<p>Applying linear regression to a classification problem often isn&rsquo;t a great idea.</p>
</li>
<li>
<p><!-- raw HTML omitted -->Logistic Regression<!-- raw HTML omitted -->: $0\leq h_\theta(x) \leq 1$ (BTW, this is actually a classification algorithm)</p>
</li>
</ul>
<h4 id="72-hypothesis-representation">7.2 Hypothesis Representation<a hidden class="anchor" aria-hidden="true" href="#72-hypothesis-representation">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Logistic Regression Model<!-- raw HTML omitted -->: want $0 \leq h_\theta(x) \leq 1$</li>
<li>$h_\theta(x) = g(\theta^Tx)$
<ul>
<li>Sigmoid function (or logistic function): $g(z) = \frac{1}{1 + e^{-z}}$. The following image shows us what the sigmoid function looks like:
<img loading="lazy" src="https://i.loli.net/2020/03/05/1UmdDKhyOuVRXgx.png" alt="image.png"  />
</li>
<li>$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$</li>
</ul>
</li>
<li>Interpretation of hypothesis output:
<ul>
<li>$h_\theta(x) = \text{estimated probability that } y=1 \text{ on input }x$</li>
<li>$h_\theta(x)= P(y=1 \vert   x;\theta)$ : probability that $y = 1$, given $x$, parameterized by $\theta$</li>
<li>$P(y=0 \vert x;\theta) + P(y=1 \vert x;\theta) = 1 \\ P(y=0 \vert x;\theta) = 1 - P(y=1 \vert x;\theta)$</li>
</ul>
</li>
</ul>
<h4 id="73-decision-boundary">7.3 Decision Boundary<a hidden class="anchor" aria-hidden="true" href="#73-decision-boundary">#</a></h4>
<ul>
<li>Logistic regression: $h_\theta(x) = g(\theta^Tx), \quad g(z)=\frac{1}{1+e^{-z}}$
<ul>
<li>Suppose predict $y=\begin{cases} 0, &amp; \text {if $h_\theta(x) \geq$ 0.5} \newline 1, &amp; \text{if $h_\theta(x) &lt; $ 0.5} \end{cases}$</li>
</ul>
</li>
<li>Decision boundary: the decision boundary is the line that separates the area where $y=0$ and where $y=1$.
<ul>
<li>It is created by our hypothesis function</li>
<li>The input to the sigmoid function of $g(z)$ doesn&rsquo;t need to be linear, and could be a function that describes a circle or any shape to fit the data</li>
</ul>
</li>
</ul>
<h2 id="8-logistic-regression-model">8 Logistic Regression Model<a hidden class="anchor" aria-hidden="true" href="#8-logistic-regression-model">#</a></h2>
<h4 id="81-cost-function">8.1 Cost Function<a hidden class="anchor" aria-hidden="true" href="#81-cost-function">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Logistic regression cost function<!-- raw HTML omitted -->:</li>
</ul>
<p>$$
J(\theta) = \frac{1}{m}\sum_{i =1}^{m}\text{Cost}(h_\theta(x^{(i)}, y^{(i)} )
$$</p>
<ul>
<li>$\text{Cost}(h_\theta(x), y)=\begin{cases} -\log(h_\theta(x)), &amp; \text {if } y =1 \newline -\log(1-h_\theta(x)), &amp; \text{if } y=0 \end{cases}$
<ul>
<li>$\text{Cost}(h_\theta(x), y) = 0, \text{ if } h_\theta(x) = y$</li>
<li>$\text{Cost}(h_\theta(x), y) \rightarrow \infty, \text{ if $y=0$ and $h_\theta(x) \rightarrow 1$} $</li>
<li>$\text{Cost}(h_\theta(x), y) \rightarrow \infty, \text{ if $y=1$ and $h_\theta(x) \rightarrow 0$} $</li>
</ul>
</li>
<li>When $y=1$, we get the following plot for $J(\theta) \text{ vs } h_\theta(x)$
<img loading="lazy" src="https://i.loli.net/2020/03/05/gJH7oC9dVLhsljz.png" alt="image.png"  />
</li>
<li>Similarly, when $y=0$, we get the following plot for $J(\theta) \text{ vs } h_\theta(x)$
<img loading="lazy" src="https://i.loli.net/2020/03/05/ZTPtRHfNSLVJvWI.png" alt="image.png"  />
</li>
</ul>
<h4 id="82-simplified-cost-function-and-gradient-descent">8.2 Simplified Cost Function and Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#82-simplified-cost-function-and-gradient-descent">#</a></h4>
<ul>
<li>
<p>Logistic regression cost function</p>
<ul>
<li>$J(\theta) = \frac{1}{m}\sum_{i =1}^{m}\text{Cost}(h_\theta(x^{(i)}, y^{(i)} )$</li>
<li>$\text{Cost}(h_\theta(x), y)=\begin{cases} -\log(h_\theta(x)), &amp; \text {if } y =1 \newline -\log(1-h_\theta(x)), &amp; \text{if } y=0 \end{cases}$</li>
<li>$\text{Note: $y=0$ or $ 1$ always}$</li>
</ul>
</li>
<li>
<p>Then, $\text{Cost}(h_\theta(x), y)=-y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))$</p>
</li>
</ul>
<p>$$
\begin{align} J(\theta) &amp;= \frac{1}{m}\sum_{i =1}^{m}\text{Cost}\left(h_\theta(x^{(i)}, y^{(i)}\right) \
&amp;= -\frac{1}{m}\sum_{i =1}^{m}\left[ y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log \left(1-h_\theta(x^{(i)})\right) \right]
\end{align}
$$</p>
<ul>
<li>
<p>A vectorized implementation is:</p>
<ul>
<li>$h = g(X\theta)$</li>
<li>$J(\theta) = \frac{1}{m}\cdot \left(-y^T\log(h)-(1-y)^T\log(1-h)\right)$</li>
</ul>
</li>
<li>
<p>To fit parameters $\theta : \mathop{\text{min}}\limits_\theta J(\theta)$</p>
</li>
<li>
<p>To make a prediction given new $x$ : output $h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$</p>
</li>
<li>
<p>Gradient descent:</p>
<p>$$
\left.
\begin{array}{l}
\text{repeat } {\
\qquad \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \newline
\qquad (\text{simultaneously update all $\theta_j$) } \newline
}
\end{array}
\right.
$$</p>
<p>(Algorithm <em>looks</em> identical to linear regression!)</p>
</li>
<li>
<p>A vectorized implementation is: $\theta := \theta - \frac{\alpha}{m}X^T\left( g(X\theta)- \vec{y} \right)$</p>
</li>
</ul>
<h4 id="83-advanced-optimization">8.3 Advanced Optimization<a hidden class="anchor" aria-hidden="true" href="#83-advanced-optimization">#</a></h4>
<ul>
<li>
<p>Cost function $J(\theta)$. Want $\mathop{\text{min}}\limits_\theta J(\theta)$.</p>
</li>
<li>
<p>Given $\theta$, we have code that can compute</p>
<ul>
<li>$J(\theta)$</li>
<li>$\frac{\partial}{\partial \theta_j}J(\theta), \ \text{(for $j = 0, 1, \dots,n$)} $</li>
</ul>
</li>
<li>
<p>Optimization algorithms:</p>
<ul>
<li>(In this class) Gradient descent</li>
<li>Conjugate gradient, BFGS, L-BFGS
<ul>
<li>Advantages:
<ul>
<li>No need to manually pick $\alpha$</li>
<li>Often faster that gradient descent</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>More sophisticated</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="9-multiclass-classification">9 Multiclass classification<a hidden class="anchor" aria-hidden="true" href="#9-multiclass-classification">#</a></h2>
<h4 id="91-multiclass-classification-one-vs-all">9.1 Multiclass Classification: One-vs-all<a hidden class="anchor" aria-hidden="true" href="#91-multiclass-classification-one-vs-all">#</a></h4>
<ul>
<li>
<p>$y = \lbrace 0, 1, \dots, n \rbrace$</p>
<p>$$     h_\theta^{(0)}(x) = P(y=0 \vert x;\theta) \ h_\theta^{(1)}(x) = P(y=1 \vert x;\theta) \     \cdots \     h_\theta^{(n)}(x) = P(y=n \vert x;\theta) \     \text{prediction} = \mathop{\text{max}}\limits_i h_\theta^{(i)}(x) $$</p>
</li>
<li>
<p>The following image shows how one could classify 3 classes:
<img loading="lazy" src="https://i.loli.net/2020/03/06/wnGJg5QLbKmoN6V.png" alt="image.png"  />
</p>
</li>
<li>
<p>One-vs-all (one-vs-rest):</p>
<ul>
<li>$h_\theta^{(i)} = P(y=i \vert x;\theta)\quad (i=1,2,\dots,n)$</li>
<li>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y = i$.</li>
<li>On a new input $x$, to make a prediction, pick the class i that maximizes $h_\theta(x)$ : $\mathop{\text{max}}\limits_i h_\theta^{(i)}(x)$</li>
</ul>
</li>
</ul>
<h2 id="10-solving-the-problem-of-overfitting">10 Solving the Problem of Overfitting<a hidden class="anchor" aria-hidden="true" href="#10-solving-the-problem-of-overfitting">#</a></h2>
<h4 id="101-the-problem-of-overfitting">10.1 The Problem of Overfitting<a hidden class="anchor" aria-hidden="true" href="#101-the-problem-of-overfitting">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Overfitting<!-- raw HTML omitted -->: If we have too many features, the learned hypothesis may fit the training set well ($J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}\left( h_\theta(x^{(i)}-y^{(i)}) \right)^2\approx 0$), but fail to generalize to new new examples (predict prices on new examples).
<img loading="lazy" src="https://i.loli.net/2020/03/07/kHOjCN48IvSs9Ec.png" alt="image.png"  />
 <img loading="lazy" src="https://i.loli.net/2020/03/07/O5buMDzp1KPeNik.png" alt="image.png"  />
</li>
<li>Options of addressing overfitting
<ul>
<li>Reduce number of features
<ul>
<li>Manually select which features to keep</li>
<li>Model selection algorithm</li>
</ul>
</li>
<li>Regularization
<ul>
<li>Keep all the features, but reduce magnitude/values of parameters $\theta_j$</li>
<li>Regularization works well when we have a lot of slightly useful features, each of which contributes a bit to predicting $y$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="102-cost-function">10.2 Cost Function<a hidden class="anchor" aria-hidden="true" href="#102-cost-function">#</a></h4>
<ul>
<li>
<p><!-- raw HTML omitted -->Regularization<!-- raw HTML omitted -->: small values for parameters $\theta_0, \theta_1, \dots, \theta_n$</p>
<ul>
<li>&ldquo;Simpler&rdquo; hypothesis</li>
<li>Less prone to overfitting</li>
</ul>
</li>
<li>
<p>Cost function after regularization:</p>
<p>$$
J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^m\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n \theta_j^2 \right]
$$</p>
<ul>
<li>$\lambda: \text{regularization parameter}$</li>
</ul>
</li>
</ul>
<h4 id="103-regularized-linear-regression">10.3 Regularized Linear Regression<a hidden class="anchor" aria-hidden="true" href="#103-regularized-linear-regression">#</a></h4>
<ul>
<li>
<p>Gradient descent:</p>
<p>$$
\left.
\begin{array}{l}
\text{repeat } {\
\qquad \theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)} \newline
\qquad  \theta_j := \theta_j - \alpha \frac{1}{m} \left[ \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} + \lambda\theta_j \right] \quad j\in\lbrace 1,2,\dots,n \rbrace \newline
}
\end{array}
\right.
$$</p>
<ul>
<li>When $j=1, 2, \dots, n$, you can also write $\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}$</li>
<li>$1-\alpha\frac{\lambda}{m} &lt; 1$</li>
</ul>
</li>
<li>
<p>Normal equaiton</p>
<p>$$
X=\begin{bmatrix} (x^{(1)})^T \newline \vdots \newline (x^{(m)})^T \end{bmatrix}<em>{m\times(n+1)}
\quad
y=\begin{bmatrix} y^{(1)} \newline \vdots \newline x^{(m)} \end{bmatrix}\in\mathbb{R^{m}}
\\
\text{To } \mathop{\text{min}}\limits</em>\theta J(\theta), \ \theta=\left(X^TX+\lambda
\begin{bmatrix}
0 &amp;  &amp;  &amp; \
&amp; 1 &amp; &amp; \
&amp;   &amp; \ddots \
&amp;   &amp;  &amp; 1
\end{bmatrix} \right)^{-1}X^Ty
$$</p>
</li>
<li>
<p>Non-invertibility</p>
<ul>
<li>Suppose $\mathop{m}\limits_{\text{#examples}} \leq \mathop{n}\limits_{\text{#features}} $, then $X^TX$ will be <em>non-invertible/singluar</em>.</li>
<li>$\text{If } \lambda &gt; 0, \  \theta=\left(\underbrace{X^TX+\lambda \begin{bmatrix} 0 &amp;  \newline  &amp; I_n \end{bmatrix} }_{\text{invertible}}  \right)^{-1}X^Ty$</li>
</ul>
</li>
</ul>
<h4 id="104-regularized-logistic-regression">10.4 Regularized Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#104-regularized-logistic-regression">#</a></h4>
<ul>
<li>
<p>We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pick line, is less likely to overfit than the non-regularized function represented by the blue line:
<img loading="lazy" src="https://i.loli.net/2020/03/07/TWQj2DiGIBwxOnz.png" alt="image.png"  />
</p>
</li>
<li>
<p>Regularized cost function:</p>
<p>$$
J(\theta)= -\frac{1}{m}\sum_{i =1}^{m}\left[ y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log \left(1-h_\theta(x^{(i)})\right) \right] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$</p>
<ul>
<li>$\sum_{j=1}^{n}\theta_j^2$ means to explicitly exclude the bias term, $\theta_0$.</li>
</ul>
</li>
<li>
<p>Gradient descent:</p>
<p>$$
\left.  \begin{array}{l}  \text{repeat } {\  \qquad \theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)} \newline  \qquad  \theta_j := \theta_j - \alpha \frac{1}{m} \left[ \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} + \lambda\theta_j \right] \quad j\in\lbrace 1,2,\dots,n \rbrace \newline  }  \end{array}  \right.
$$</p>
</li>
</ul>
<h2 id="ex2-logistic-regression">Ex2: Logistic Regressionüë®‚Äçüíª<a hidden class="anchor" aria-hidden="true" href="#ex2-logistic-regression">#</a></h2>
<blockquote>
<p>See this exercise on <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python/blob/master/ex2/ex2.ipynb">Coursera-MachineLearning-Python/ex2/ex2.ipynb</a></p>
</blockquote>
<h4 id="ex21-logistic-regression">Ex2.1 Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#ex21-logistic-regression">#</a></h4>
<p><strong>Instruction:</strong>
In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">scipy.optimize</span> <span class="k">as</span> <span class="nn">op</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 0: Loading Data and Defining Functions &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Hypothesis function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Logistic regression cost function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">costFunction</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)))</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The objective function to be minimized</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">costFunc</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">args</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)))</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">cost</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Method for computing the gradient vector</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gradFunc</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">args</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">grad</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The first two columns contains the exam scores and the third column contains the label</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading Data ... &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;ex2data1.txt&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 1: Plotting &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">fig0</span><span class="p">,</span> <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">label1</span><span class="p">,</span> <span class="n">label0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">label1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">label1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Admitted&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">label0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">label0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Not admitted&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 Score&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 Score&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 2: Compute Cost and Gradient &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>           <span class="c1"># Add intercept term to x and X_test</span>
</span></span><span class="line"><span class="cl"><span class="n">initial_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>    <span class="c1"># Initialize fitting parameters</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute and display initial cost and gradient</span>
</span></span><span class="line"><span class="cl"><span class="n">cost</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">costFunction</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Cost at initial theta : &#39;</span><span class="p">,</span> <span class="n">cost</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost (approx): 0.693&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient at initial theta :&#39;</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected gradients (approx): </span><span class="se">\t</span><span class="s1">-0.1000</span><span class="se">\t</span><span class="s1"> -12.0092</span><span class="se">\t</span><span class="s1"> -11.2628&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute and display cost and gradient with non-zero theta</span>
</span></span><span class="line"><span class="cl"><span class="n">test_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="n">cost</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">costFunction</span><span class="p">(</span><span class="n">test_theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Cost at test theta : &#39;</span><span class="p">,</span> <span class="n">cost</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost (approx): 0.218&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient at test theta :&#39;</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected gradients (approx): </span><span class="se">\t</span><span class="s1">0.043</span><span class="se">\t</span><span class="s1"> 2.5662</span><span class="se">\t</span><span class="s1"> 2.647&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 3: Optimizing using fminunc &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># uUse Newton Conjugate Gradient algorithm to obtain the optimal theta</span>
</span></span><span class="line"><span class="cl"><span class="n">res</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">costFunc</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;TNC&#39;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">gradFunc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">cost</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Cost at theta found by fminunc: &#39;</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost (approx): 0.203&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;theta: &#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected theta (approx): </span><span class="se">\t</span><span class="s1">-25.161</span><span class="se">\t</span><span class="s1"> 0.206</span><span class="se">\t</span><span class="s1"> 0.201&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot boundary</span>
</span></span><span class="line"><span class="cl"><span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x2</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x1</span><span class="p">)</span> <span class="o">/</span> <span class="n">theta</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 4: Predict and Accuracies &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">prob</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">85</span><span class="p">])),</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">For a student with scores 45 and 85, we predict an adimission probability of &#39;</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected value: 0.775 +/- 0.002&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train accuracy: &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected accuracy (approx): 89.0 %</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console
<img loading="lazy" src="https://i.loli.net/2020/03/09/kEjqygun1US25Xc.png" alt="image.png"  />
</li>
<li>Training data with decision boundary
<img loading="lazy" src="https://i.loli.net/2020/03/09/8PeaQu5iAqz3kFm.png" alt="Figure_1.png"  />
</li>
</ul>
<h4 id="ex22-regularized-logistic-regression">Ex2.2 Regularized Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#ex22-regularized-logistic-regression">#</a></h4>
<p><strong>Instruction:</strong>
In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 0: Loading Data and Defining Functions &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Feature mapping function to polynomial features</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mapFeature</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">degree</span> <span class="o">=</span> <span class="mi">6</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">X1</span><span class="o">**</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">j</span><span class="p">)</span> <span class="o">*</span> <span class="n">X2</span><span class="o">**</span><span class="p">(</span><span class="n">j</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Hypothesis function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute cost and gradient for logistic regression with regularization</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">costFunctionReg</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost1</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)))</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost2</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg_param</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="n">m</span>    <span class="c1"># Don&#39;t penalize theta_0</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost1</span> <span class="o">+</span> <span class="n">cost2</span>
</span></span><span class="line"><span class="cl">    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reg_param</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">m</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use Batch Gradient Descent algorithm to minimize cost</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">batchGradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">iters</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">J_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">costFunctionReg</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad</span>
</span></span><span class="line"><span class="cl">        <span class="n">J_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cost</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The first two columns contains the exam scores and the third column contains the label</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading Data ... &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;ex2data2.txt&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot data</span>
</span></span><span class="line"><span class="cl"><span class="n">fig0</span><span class="p">,</span> <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">label1</span><span class="p">,</span> <span class="n">label0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">label1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">label1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;y = 1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">label0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">label0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;y = 0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 1: Regularized Logistic Regression &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">mapFeature</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize fitting parameters</span>
</span></span><span class="line"><span class="cl"><span class="n">initial_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set regularization parameter lambda to 1</span>
</span></span><span class="line"><span class="cl"><span class="n">reg_param</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute and display inital cost gradient for regularized logistic regression</span>
</span></span><span class="line"><span class="cl"><span class="n">cost0</span><span class="p">,</span> <span class="n">grad0</span> <span class="o">=</span> <span class="n">costFunctionReg</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg_param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Cost at initial theta (zeros): &#39;</span><span class="p">,</span> <span class="n">cost0</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost (approx): 0.693&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient at initial theta (zeros) - first five values only: &#39;</span>
</span></span><span class="line"><span class="cl">      <span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">grad0</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected gradients (approx) - first five values only: &#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="s1">&#39;0.0085 0.0188 0.0001 0.0503, 0.0115&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute and display cost and gradient with all-ones theta and lambda = 10</span>
</span></span><span class="line"><span class="cl"><span class="n">test_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">cost1</span><span class="p">,</span> <span class="n">grad1</span> <span class="o">=</span> <span class="n">costFunctionReg</span><span class="p">(</span><span class="n">test_theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg_param</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Cost at test theta (with lambda = 10): &#39;</span><span class="p">,</span> <span class="n">cost1</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost (approx): 3.16&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient at test theta - first five values only: &#39;</span>
</span></span><span class="line"><span class="cl">      <span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">grad1</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected gradients (approx) - first five values only: &#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="s1">&#39;0.3460 0.1614 0.1948 0.2269, 0.0922&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; Part 2: Regularization and Accuracies &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Optimize</span>
</span></span><span class="line"><span class="cl"><span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span> <span class="o">=</span> <span class="n">batchGradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="n">reg_param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># fig1, ax1 = plt.subplots()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ax1.plot(np.arange(2000), J_history, &#39;c&#39;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot boundary</span>
</span></span><span class="line"><span class="cl"><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x1min</span><span class="p">,</span> <span class="n">x1max</span><span class="p">,</span> <span class="n">x2min</span><span class="p">,</span> <span class="n">x2max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x1min</span><span class="p">,</span> <span class="n">x1max</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x2min</span><span class="p">,</span> <span class="n">x2max</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">bd</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="n">bd</span> <span class="o">=</span> <span class="n">bd</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">CS</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">bd</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">CS</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;Decision</span><span class="se">\n</span><span class="s1">Boundary&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\lambda$ = &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">reg_param</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 2&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute accuracy on our training set</span>
</span></span><span class="line"><span class="cl"><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Train Accuracy: &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected accuracy (with lambda = 1): 83.1 % (approx)&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console ((Œª = 1)
<img loading="lazy" src="https://i.loli.net/2020/03/10/mL5Uq16nKgifRuB.png" alt="image.png"  />
</li>
<li>Training data with decision boundary (Œª = 1)
<img loading="lazy" src="https://i.loli.net/2020/03/10/K29fFZimhoAuxrb.png" alt="image.png"  />
</li>
<li>Too much regularization (Underfitting) (Œª = 100)
<img loading="lazy" src="https://i.loli.net/2020/03/10/eG36QAIqsiwbZ45.png" alt="image.png"  />
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://fang-lansheng.github.io/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/python/">Python</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://fang-lansheng.github.io/posts/2020-03-10-ml-ng-4/">
    <span class="title">¬´ Prev Page</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà4Ôºâ</span>
  </a>
  <a class="next" href="https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-2/">
    <span class="title">Next Page ¬ª</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ on twitter"
        href="https://twitter.com/intent/tweet/?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%883%ef%bc%89&amp;url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f&amp;hashtags=ML%2fDL%2cCourseLearning%2cPython">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f&amp;title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%883%ef%bc%89&amp;summary=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%883%ef%bc%89&amp;source=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%883%ef%bc%89">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ on whatsapp"
        href="https://api.whatsapp.com/send?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%883%ef%bc%89%20-%20https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ on telegram"
        href="https://telegram.me/share/url?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%883%ef%bc%89&amp;url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-03-04-ml-ng-3%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://fang-lansheng.github.io">Jifan&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
