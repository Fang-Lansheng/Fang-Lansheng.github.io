<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ | Jifan&#39;s Blog</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$).">
<meta name="author" content="Thistledown">
<link rel="canonical" href="https://jifan.tech/posts/2020-02-26-ml-ng-2/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://jifan.tech/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jifan.tech/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jifan.tech/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jifan.tech/apple-touch-icon.png">
<link rel="mask-icon" href="https://jifan.tech/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ" />
<meta property="og:description" content="Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jifan.tech/posts/2020-02-26-ml-ng-2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-02-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ"/>
<meta name="twitter:description" content="Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://jifan.tech/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ",
      "item": "https://jifan.tech/posts/2020-02-26-ml-ng-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ",
  "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ",
  "description": "Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \\cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \\cdots, n$).",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \\cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \\cdots, n$).   Hypothesis: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$  Previously: $h_\\theta(x) = \\theta_0 + \\theta_1x$ Now: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4$ For convenience of notation, define $x_0 = 1 \\ (x_0^{(i)}=1,\\ i=1, 2, \\cdots, m)$. $x = [ \\begin{matrix}x_0 \u0026 x_1 \u0026 x_2 \u0026 \\cdots \u0026 x_n \\end{matrix}]^T \\in \\mathbb{R^{n+1}}$ $\\theta = [ \\begin{matrix}\\theta_0 \u0026 \\theta_1 \u0026 \\theta_2 \u0026 \\cdots \u0026 \\theta_n \\end{matrix}]^T \\in \\mathbb{R^{n+1}}$   Multivariate Linear Regression  $$\\begin{align} h_\\theta(x)\u0026=\\theta^Tx= \\begin{bmatrix}\\theta_0 \\ \\theta_1 \\ \\theta_2 \\ \\cdots \\ \\theta_n \\end{bmatrix} \\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ \\cdots \\ x_n \\end{bmatrix} = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\end{align}$$\n5.2 Gradient Descent for Multiple Variables   Hypothesis: $\\begin{align} h_\\theta(x)\u0026=\\theta^Tx = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\end{align}$\n  Parameters: $\\theta_0, \\theta_1, \\dots, \\theta_n$ ($\\theta = \\begin{bmatrix} \\theta_0 \u0026 \\theta_1 \u0026 \\cdots \u0026 \\theta_n \\end{bmatrix}^T$)\n  Cost function: $J(\\theta_0, \\theta_1, \\dots, \\theta_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 $\n  Gradient descent:\n$$\\left.\\begin{array}{l} \\text{repeat until convergence}\\ { \\ \\qquad \\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} \\ \\qquad (\\text{simultaneously update for every } \\theta_j \\text{ for } j := 0\\dots n) \\ } \\end{array}\\right.$$\n  The following image compares gradient descent with one variable to gradient descent with multiple variables:   5.3 Gradient Descent in Practice I - Feature Scaling  Feature scaling Idea: make sure features are on similar scale. Get every feature into approximately a $-1 \\leq x_i \\leq 1$ range.   Mean normalization: replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean.  $x_i:= \\frac{x_i-u_i}{s_i}$: $u_i$ is the average of all the values for the feature $(i)$ and $s_i$ is the range of the values $(\\text{max - min})$, or $s_i$ is the standard deviation.    5.4 Gradient Descent in Practice II - Learning Rate  Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(\\theta)$, over the number of iterations of gradient descent. if $J(\\theta)$ ever increases, then you probably need to decrease $\\alpha$. Automatic convergence test. Declare convergence if $J(\\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it‚Äôs difficult to choose this threshold value. Make sure gradient descent is working correctly.  For sufficient small $\\alpha$, $J(\\theta)$ should decrease on every iteration. If $\\alpha$ is too large: may not decrease on every iteration and thus may not converge. if $\\alpha$ is too small: slow convergence.    5.5 Features and Polynomial Regression  [Example] Housing prices prediction: $h_\\theta(x)=\\theta_0 + \\theta_1 \\times frontage + \\theta_2 \\times depth$ Polynomial regression: We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).  6. Computing Parameters Analytically 6.1 Normal Equation   Normal equation: Method to solve for $\\theta$ analytically.\n $\\theta \\in \\mathbb{R^{n+1}}\\qquad J(\\theta_0,\\theta_1,\\dots,\\theta_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$ $\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\cdots=0 \\qquad (\\text{for every }j)$ Solve for $\\theta_0, \\theta_1, \\dots,\\theta_n$    $m$ examples $(x^{(1)},y^{(1)}), \\dots,(x^{(m)}, y^{(m)})$; $n$ features.\n$$x^{(i)}=\\begin{bmatrix} x_0^{(i)} \\ x_1^{(i)} \\ x_2^{(i)} \\ \\vdots \\ x_n^{(i)} \\end{bmatrix} \\in \\mathbb{R^{n+1}}, \\qquad \\mathop{x}\\limits_{\\text{(design matrix)}} = \\begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \\vdots \\ (x^{(m)})^T \\end{bmatrix}_{m\\times(n+1)}$$\n  $\\theta=(X^TX)^{-1}X^Ty$\n  The following is a comparison of gradient descent and the normal equation:\n     Gradient Descent Normal Equation     Need to choose $\\alpha$. No need to choose $\\alpha$.   Needs many iterations. Don‚Äôt need to iterate.   $\\text{O}(kn^2)$ $\\text{O}(n^3)$, Need to compute $(X^TX)^{-1}$   Works well even when $n$ is large. Slow if $n$ is very large.    6.2 Normal Equation and Noninvertibility  Normal equation: $\\theta=(X^TX)^{-1}X^Ty$. But what if $X^TX$ is non-invertible (singular/degenerate)? If $X^TX$ is noninvertible, the common causes might be having:  Redundant features, where two features are very closely related (i.e. they are linearly dependent). Too many features (e.g. $m \\leq n$). In this case, delete some features, or use regularization.    Ex1: Linear Regressionüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex1/ex1.ipynb\n Ex1.1 Linear regression with one variable Instruction: In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\nCode:\nimport numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D \"\"\" Part 1: Basic Function \"\"\" def computeCost(X, y, theta): ''' Compute cost for linear regression :param X: input variables :param y: output variables :param theta: parameters :return: Cost function ''' m = len(y) J = 0 for xi, yi in zip(X, y): J = J + np.square(float(np.dot(xi, theta)) - yi) return J / (2 * m) def gradientDescent(X, y, theta, alpha, num_iters): ''' updates theta by taking num_iters gradient steps with learning rate alpha :param X: input variables :param y: output variables :param theta: parameters :param alpha: learning rate :param num_iters: times of iteration :return: [theta, J_history] ''' # Initialize some useful values m = len(y) J_history = np.zeros(num_iters) for i in range(num_iters): sum0, sum1 = 0, 0 for j in range(m): sum0 = sum0 + (np.dot(X[j], theta) - y[j]) * X[j][0] sum1 = sum1 + (np.dot(X[j], theta) - y[j]) * X[j][1] theta[0] = theta[0] - alpha * sum0 / m theta[1] = theta[1] - alpha * sum1 / m # Save the cost J in every iteration J_history[i] = computeCost(X, y, theta) return theta, J_history \"\"\" Part 2: Plotting \"\"\" print('Plotting Data ...') data = np.loadtxt(\"ex1data1.txt\", dtype=float, delimiter=',') X = data[:, 0] y = data[:, 1] m = len(y) # number of training examples # Plot Data fig0, ax0 = plt.subplots() ax0.scatter(X, y, marker='x', label='Training data') ax0.set_xlabel('Population of City in 10,000s') ax0.set_ylabel('Profit in $10,000s') \"\"\" Part 3: Cost and Gradient Descent \"\"\" X = np.c_[np.ones((m, 1)), data[:,0]] # Add a column of ones to X theta = np.zeros((2, 1)) # initialize fitting parameters # Some gradient descent settings iterations = 1500 alpha = 0.01 print('\\nTesting the cost function ... \\n') # compute and display initial cost J = computeCost(X, y, theta) print('With theta = [0 ; 0], cost computed = ', J) print('Expected cost value (approx) 32.07\\n') # further testing of the cost function J = computeCost(X, y, np.array([[-1], [2]])) print('With theta = [-1 ; 2], cost computed = ', J) print('Expected cost value (approx) 54.24\\n') print('Running Gradient Descent ...\\n') # run gradient descent theta, J_history = gradientDescent(X, y, theta, alpha, iterations) # print theta to screen print('Theta found by gradient descent: Œ∏0 =', theta[0], ', Œ∏1 =', theta[1]) print('Expected theta values (approx): -3.6303 1.6664\\n') # Plot the linear fit ax0.plot(X[:, 1], np.dot(X, theta), color='r', label='Linear regression') ax0.legend(loc='lower right') ax0.set_title('Linear regression with one variable') # Predict values for population sizes of 35,000 and 70,000 predict1 = np.dot([1, 3.5], theta) predict2 = np.dot([1, 7], theta) print('For population = 35,000, we predict a profit of ', predict1 * 10000) print('For population = 70,000, we predict a profit of ', predict2 * 10000) \"\"\" Part 4: Visualizing J(theta_0, theta_1) \"\"\" print('\\nVisualizing J(theta_0, theta_1) ... \\n') # Plot the reducing of cost function during iteration fig1, ax1 = plt.subplots() ax1.plot(np.arange(iterations), J_history, 'b') ax1.set_xlabel('Iterations') ax1.set_ylabel(r'$J(\\theta_0, \\theta_1)$') ax1.set_title(r'Reducing of $J(\\theta_0, \\theta_1)$ during iteration') # Grid over which we will calculate J theta0_vals = np.linspace(-10, 10, 100) theta1_vals = np.linspace(-1, 4, 100) # Initialize J_vals to a matrix of 0's J_vals = np.zeros([len(theta0_vals), len(theta1_vals)]) # Fill out J_vals for i in range(len(theta0_vals)): for j in range(len(theta1_vals)): t = [theta0_vals[i], theta1_vals[j]] J_vals[i][j] = computeCost(X, y, t) x_contour, y_contour = theta0_vals, theta1_vals theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals) # Produce surface and contour plots of J(Œ∏) fig2 = plt.figure() ax2 = Axes3D(fig2) ax2.plot_surface(theta0_vals, theta1_vals, J_vals.T, rstride=1, cstride=1, cmap=cm.rainbow) ax2.set_xlabel(r'$\\theta_0$') ax2.set_ylabel(r'$\\theta_1$') ax2.set_title('Surface') fig3, ax3 = plt.subplots() CS = ax3.contour(theta0_vals, theta1_vals, J_vals.T, levels=50) ax3.plot(theta[0], theta[1], 'bx') ax3.set_xlabel(r'$\\theta_0$') ax3.set_ylabel(r'$\\theta_1$') ax3.set_title('Contour') plt.show() Output:\n Console   Training data with linear regression fit.   Reducing of $J(\\theta_0, \\theta_1)$ during iteration   Produce surface and contour plots of $J(Œ∏)$  Ex1.2 Linear regression with multiple variables Instruction: In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\nCode:\nimport numpy as np import matplotlib.pyplot as plt \"\"\" Part 0: Basic Function \"\"\" def computeCost(X, y, theta): ''' Compute cost for linear regression :param X: input variables :param y: output variables :param theta: parameters :return: Cost function ''' m = len(y) J = np.sum(np.square(X.dot(theta) - y)) / (2 * m) return J def gradientDescentMulti(X, y, theta, alpha, num_iters): ''' updates theta by taking num_iters gradient steps with learning rate alpha :param X: input variables :param y: output variables :param theta: parameters :param alpha: learning rate :param num_iters: times of iteration :return: [theta, J_history] ''' # Initialize some useful values m, n = len(y), len(theta) J_history = np.zeros(num_iters) for i in range(num_iters): temp = np.dot((np.dot(X, theta) - y.reshape(m, 1)).T, X) theta = theta - alpha * temp.T / m J_history[i] = computeCost(X, y, theta) return theta, J_history def featureNormalize(X): ''' Normalize the features in X :param X: features :return: X_norm: normalized X; mean: mean value; sigma: standard deviation ''' mean = np.mean(X, 0) sigma = np.std(X, 0) X_norm = (X - mean) / sigma return X_norm, mean, sigma def normalEqn(X, y): ''' Computes the closed-form solution to linear regression :param X: input variables :param y: output variables :return: parameters ''' theta = np.linalg.inv(X.T@X)@X.T@y return theta \"\"\" Part 1: Feature Normalization \"\"\" # Load data data = np.loadtxt('ex1data2.txt', dtype=float, delimiter=',') X = data[:, 0:2] y = data[:, 2:3] m = len(y) # Scale features and set them to zero mean X_norm, mean, sigma = featureNormalize(X) y_norm, _, _ = featureNormalize(y) # Add intercept term to X X_norm = np.c_[np.ones([m, 1]), X_norm] \"\"\" Part 2: Gradient Descent \"\"\" print('Running gradient descent ... ') # Choose some alpha value alpha = 0.01 num_iters = 1000 # Init theta and run gradient descent theta = np.zeros([3, 1]) theta, J_history = gradientDescentMulti(X_norm, y, theta, alpha, num_iters) # Plot the convergence graph fig1, ax1 = plt.subplots() ax1.plot(np.arange(num_iters), J_history, 'b') ax1.set_xlabel('Iterations') ax1.set_ylabel(r'$J(\\theta)$') ax1.set_title(r'Convergence of $J(\\theta)$') # Display gradient descent's result print('Theta computed from gradient descent: \\n', theta) \"\"\" Part 3: Normal Equations \"\"\" print('\\nSolving with normal equations ... ') # Add intercept term to X X = np.c_[np.ones([m, 1]), X] # Calculate the parameters from the normal equation theta1 = normalEqn(X, y) print('Theta computed from normal equations: \\n', theta1) plt.show() Output:\n Console (There is some differences between the two results)   The convergence graph  ",
  "wordCount" : "1828",
  "inLanguage": "en",
  "datePublished": "2020-02-28T00:00:00Z",
  "dateModified": "2020-02-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thistledown"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jifan.tech/posts/2020-02-26-ml-ng-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jifan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jifan.tech/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jifan.tech" accesskey="h" title="Jifan&#39;s Blog (Alt + H)">Jifan&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jifan.tech/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jifan.tech/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jifan.tech/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ
    </h1>
    <div class="post-meta"><span title='2020-02-28 00:00:00 +0000 UTC'>February 28, 2020</span>&nbsp;¬∑&nbsp;Thistledown

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#5-multivariate-linear-regression" aria-label="5. Multivariate Linear Regression">5. Multivariate Linear Regression</a><ul>
                        <ul>
                        
                <li>
                    <a href="#51-multiple-features" aria-label="5.1 Multiple Features">5.1 Multiple Features</a></li>
                <li>
                    <a href="#52-gradient-descent-for-multiple-variables" aria-label="5.2 Gradient Descent for Multiple Variables">5.2 Gradient Descent for Multiple Variables</a></li>
                <li>
                    <a href="#53-gradient-descent-in-practice-i---feature-scaling" aria-label="5.3 Gradient Descent in Practice I - Feature Scaling">5.3 Gradient Descent in Practice I - Feature Scaling</a></li>
                <li>
                    <a href="#54-gradient-descent-in-practice-ii---learning-rate" aria-label="5.4 Gradient Descent in Practice II - Learning Rate">5.4 Gradient Descent in Practice II - Learning Rate</a></li>
                <li>
                    <a href="#55-features-and-polynomial-regression" aria-label="5.5 Features and Polynomial Regression">5.5 Features and Polynomial Regression</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#6-computing-parameters-analytically" aria-label="6. Computing Parameters Analytically">6. Computing Parameters Analytically</a><ul>
                        <ul>
                        
                <li>
                    <a href="#61-normal-equation" aria-label="6.1 Normal Equation">6.1 Normal Equation</a></li>
                <li>
                    <a href="#62-normal-equation-and-noninvertibility" aria-label="6.2 Normal Equation and Noninvertibility">6.2 Normal Equation and Noninvertibility</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#ex1-linear-regression" aria-label="Ex1: Linear Regressionüë®‚Äçüíª">Ex1: Linear Regressionüë®‚Äçüíª</a><ul>
                        <ul>
                        
                <li>
                    <a href="#ex11-linear-regression-with-one-variable" aria-label="Ex1.1 Linear regression with one variable">Ex1.1 Linear regression with one variable</a></li>
                <li>
                    <a href="#ex12-linear-regression-with-multiple-variables" aria-label="Ex1.2 Linear regression with multiple variables">Ex1.2 Linear regression with multiple variables</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Course Link Ôºö<a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2 - Linear Regression with Multiple Variables</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="5-multivariate-linear-regression">5. Multivariate Linear Regression<a hidden class="anchor" aria-hidden="true" href="#5-multivariate-linear-regression">#</a></h2>
<h4 id="51-multiple-features">5.1 Multiple Features<a hidden class="anchor" aria-hidden="true" href="#51-multiple-features">#</a></h4>
<ul>
<li>Training set:
<!-- raw HTML omitted --></li>
<li>Notation:
<ul>
<li>$m$ = the number of training examples</li>
<li>$n$ = number of features</li>
<li>$x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$).</li>
<li>$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$).</li>
</ul>
</li>
<li>Hypothesis: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$
<ul>
<li>Previously: $h_\theta(x) = \theta_0 + \theta_1x$</li>
<li>Now: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4$</li>
<li>For convenience of notation, define $x_0 = 1 \ (x_0^{(i)}=1,\ i=1, 2, \cdots, m)$.</li>
<li>$x = [ \begin{matrix}x_0 &amp; x_1 &amp; x_2 &amp; \cdots &amp; x_n \end{matrix}]^T \in \mathbb{R^{n+1}}$</li>
<li>$\theta = [ \begin{matrix}\theta_0 &amp; \theta_1 &amp; \theta_2 &amp; \cdots &amp; \theta_n \end{matrix}]^T \in \mathbb{R^{n+1}}$</li>
</ul>
</li>
<li><!-- raw HTML omitted -->Multivariate Linear Regression<!-- raw HTML omitted --></li>
</ul>
<p>$$\begin{align}
h_\theta(x)&amp;=\theta^Tx= \begin{bmatrix}\theta_0 \ \theta_1 \ \theta_2 \ \cdots \ \theta_n \end{bmatrix} \begin{bmatrix}x_0 \ x_1 \ x_2 \ \cdots \ x_n \end{bmatrix}
= \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
\end{align}$$</p>
<h4 id="52-gradient-descent-for-multiple-variables">5.2 Gradient Descent for Multiple Variables<a hidden class="anchor" aria-hidden="true" href="#52-gradient-descent-for-multiple-variables">#</a></h4>
<ul>
<li>
<p>Hypothesis: $\begin{align}
h_\theta(x)&amp;=\theta^Tx
= \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
\end{align}$</p>
</li>
<li>
<p>Parameters: $\theta_0, \theta_1, \dots, \theta_n$  ($\theta = \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n \end{bmatrix}^T$)</p>
</li>
<li>
<p>Cost function: $J(\theta_0, \theta_1, \dots, \theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 $</p>
</li>
<li>
<p>Gradient descent:</p>
<p>$$\left.\begin{array}{l} \text{repeat until convergence}\ { \ \qquad \theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \ \qquad (\text{simultaneously update for every } \theta_j \text{ for } j := 0\dots n) \ } \end{array}\right.$$</p>
</li>
<li>
<p>The following image compares gradient descent with one variable to gradient descent with multiple variables:
<img loading="lazy" src="https://i.loli.net/2020/02/29/tJXVdIn4EsCbDkf.png" alt="image.png"  />
</p>
</li>
</ul>
<h4 id="53-gradient-descent-in-practice-i---feature-scaling">5.3 Gradient Descent in Practice I - Feature Scaling<a hidden class="anchor" aria-hidden="true" href="#53-gradient-descent-in-practice-i---feature-scaling">#</a></h4>
<ul>
<li><!-- raw HTML omitted -->Feature scaling<!-- raw HTML omitted -->
<ul>
<li>Idea: make sure features are on similar scale.</li>
<li>Get every feature into approximately a $-1 \leq x_i \leq 1$ range.</li>
</ul>
</li>
<li><!-- raw HTML omitted -->Mean normalization<!-- raw HTML omitted -->: replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean.
<ul>
<li>$x_i:= \frac{x_i-u_i}{s_i}$: $u_i$ is the <em>average</em> of all the values for the feature $(i)$ and $s_i$ is the range of the values $(\text{max - min})$, or $s_i$ is the standard deviation.</li>
</ul>
</li>
</ul>
<h4 id="54-gradient-descent-in-practice-ii---learning-rate">5.4 Gradient Descent in Practice II - Learning Rate<a hidden class="anchor" aria-hidden="true" href="#54-gradient-descent-in-practice-ii---learning-rate">#</a></h4>
<ul>
<li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$, over the number of iterations of gradient descent. if $J(\theta)$ ever increases, then you probably need to decrease $\alpha$.</li>
<li><strong>Automatic convergence test.</strong> Declare convergence if $J(\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it&rsquo;s difficult to choose this threshold value.</li>
<li><strong>Make sure gradient descent is working correctly.</strong>
<ul>
<li>For sufficient small $\alpha$, $J(\theta)$ should decrease on every iteration.</li>
<li>If $\alpha$ is too large: may not decrease on every iteration and thus may not converge.</li>
<li>if $\alpha$ is too small: slow convergence.</li>
</ul>
</li>
</ul>
<h4 id="55-features-and-polynomial-regression">5.5 Features and Polynomial Regression<a hidden class="anchor" aria-hidden="true" href="#55-features-and-polynomial-regression">#</a></h4>
<ul>
<li>[<em>Example</em>] <strong>Housing prices prediction</strong>: $h_\theta(x)=\theta_0 + \theta_1 \times frontage + \theta_2 \times depth$</li>
<li><strong>Polynomial regression</strong>: We can <em>change the behavior or curve</em> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</li>
</ul>
<h2 id="6-computing-parameters-analytically">6. Computing Parameters Analytically<a hidden class="anchor" aria-hidden="true" href="#6-computing-parameters-analytically">#</a></h2>
<h4 id="61-normal-equation">6.1 Normal Equation<a hidden class="anchor" aria-hidden="true" href="#61-normal-equation">#</a></h4>
<ul>
<li>
<p><!-- raw HTML omitted -->Normal equation<!-- raw HTML omitted -->: Method to solve for $\theta$ analytically.</p>
<ul>
<li>$\theta \in \mathbb{R^{n+1}}\qquad J(\theta_0,\theta_1,\dots,\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$</li>
<li>$\frac{\partial}{\partial\theta_j}J(\theta)=\cdots=0 \qquad (\text{for every }j)$</li>
<li>Solve for $\theta_0, \theta_1, \dots,\theta_n$</li>
</ul>
</li>
<li>
<p>$m$ examples $(x^{(1)},y^{(1)}), \dots,(x^{(m)}, y^{(m)})$; $n$ features.</p>
<p>$$x^{(i)}=\begin{bmatrix} x_0^{(i)} \ x_1^{(i)} \ x_2^{(i)} \ \vdots \ x_n^{(i)}  \end{bmatrix} \in \mathbb{R^{n+1}}, \qquad  \mathop{x}\limits_{\text{(design matrix)}} = \begin{bmatrix} (x^{(1)})^T \ (x^{(2)})^T \ \vdots \  (x^{(m)})^T \end{bmatrix}_{m\times(n+1)}$$</p>
</li>
<li>
<p>$\theta=(X^TX)^{-1}X^Ty$</p>
</li>
<li>
<p>The following is a comparison of gradient descent and the normal equation:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><!-- raw HTML omitted -->Gradient Descent<!-- raw HTML omitted --></th>
<th><!-- raw HTML omitted -->Normal Equation<!-- raw HTML omitted --></th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose $\alpha$.</td>
<td>No need to choose $\alpha$.</td>
</tr>
<tr>
<td>Needs many iterations.</td>
<td>Don&rsquo;t need to iterate.</td>
</tr>
<tr>
<td>$\text{O}(kn^2)$</td>
<td>$\text{O}(n^3)$, Need to compute $(X^TX)^{-1}$</td>
</tr>
<tr>
<td>Works well even when $n$ is large.</td>
<td>Slow if $n$ is very large.</td>
</tr>
</tbody>
</table>
<h4 id="62-normal-equation-and-noninvertibility">6.2 Normal Equation and Noninvertibility<a hidden class="anchor" aria-hidden="true" href="#62-normal-equation-and-noninvertibility">#</a></h4>
<ul>
<li>Normal equation: $\theta=(X^TX)^{-1}X^Ty$. But what if $X^TX$ is non-invertible (singular/degenerate)?</li>
<li>If $X^TX$ is <em>noninvertible</em>, the common causes might be having:
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent).</li>
<li>Too many features (e.g. $m \leq n$). In this case, delete some features, or use regularization.</li>
</ul>
</li>
</ul>
<h2 id="ex1-linear-regression">Ex1: Linear Regressionüë®‚Äçüíª<a hidden class="anchor" aria-hidden="true" href="#ex1-linear-regression">#</a></h2>
<blockquote>
<p>See this exercise on <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python/blob/master/ex1/ex1.ipynb">Coursera-MachineLearning-Python/ex1/ex1.ipynb</a></p>
</blockquote>
<h4 id="ex11-linear-regression-with-one-variable">Ex1.1 Linear regression with one variable<a hidden class="anchor" aria-hidden="true" href="#ex11-linear-regression-with-one-variable">#</a></h4>
<p><strong>Instruction:</strong>
In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 1: Basic Function &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Compute cost for linear regression
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param X: input variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param y: output variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param theta: parameters
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: Cost function
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">J</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">J</span> <span class="o">=</span> <span class="n">J</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span> <span class="o">-</span> <span class="n">yi</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">J</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    updates theta by taking num_iters gradient steps with learning rate alpha
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param X: input variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param y: output variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param theta: parameters
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param alpha: learning rate
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param num_iters: times of iteration
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: [theta, J_history]
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize some useful values</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">J_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">sum0</span><span class="p">,</span> <span class="n">sum1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum0</span> <span class="o">=</span> <span class="n">sum0</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">sum1</span> <span class="o">=</span> <span class="n">sum1</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">sum0</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">sum1</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Save the cost J in every iteration</span>
</span></span><span class="line"><span class="cl">        <span class="n">J_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 2: Plotting &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Plotting Data ...&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&#34;ex1data1.txt&#34;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>      <span class="c1"># number of training examples</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot Data</span>
</span></span><span class="line"><span class="cl"><span class="n">fig0</span><span class="p">,</span> <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Population of City in 10,000s&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Profit in $10,000s&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 3: Cost and Gradient Descent &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span>   <span class="c1"># Add a column of ones to X</span>
</span></span><span class="line"><span class="cl"><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>                <span class="c1"># initialize fitting parameters</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Some gradient descent settings</span>
</span></span><span class="line"><span class="cl"><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1500</span>
</span></span><span class="line"><span class="cl"><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Testing the cost function ... </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># compute and display initial cost</span>
</span></span><span class="line"><span class="cl"><span class="n">J</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;With theta = [0 ; 0], cost computed = &#39;</span><span class="p">,</span> <span class="n">J</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost value (approx) 32.07</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># further testing of the cost function</span>
</span></span><span class="line"><span class="cl"><span class="n">J</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]]))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;With theta = [-1 ; 2], cost computed = &#39;</span><span class="p">,</span> <span class="n">J</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected cost value (approx) 54.24</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running Gradient Descent ...</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># run gradient descent</span>
</span></span><span class="line"><span class="cl"><span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># print theta to screen</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Theta found by gradient descent: Œ∏0 =&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;, Œ∏1 =&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Expected theta values (approx): -3.6303  1.6664</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the linear fit</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear regression&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Linear regression with one variable&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Predict values for population sizes of 35,000 and 70,000</span>
</span></span><span class="line"><span class="cl"><span class="n">predict1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">],</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">predict2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For population = 35,000, we predict a profit of &#39;</span><span class="p">,</span> <span class="n">predict1</span> <span class="o">*</span> <span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For population = 70,000, we predict a profit of &#39;</span><span class="p">,</span> <span class="n">predict2</span> <span class="o">*</span> <span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 4: Visualizing J(theta_0, theta_1) &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Visualizing J(theta_0, theta_1) ... </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the reducing of cost function during iteration</span>
</span></span><span class="line"><span class="cl"><span class="n">fig1</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">iterations</span><span class="p">),</span> <span class="n">J_history</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta_0, \theta_1)$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Reducing of $J(\theta_0, \theta_1)$ during iteration&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Grid over which we will calculate J</span>
</span></span><span class="line"><span class="cl"><span class="n">theta0_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">theta1_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize J_vals to a matrix of 0&#39;s</span>
</span></span><span class="line"><span class="cl"><span class="n">J_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">theta0_vals</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta1_vals</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Fill out J_vals</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta0_vals</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta1_vals</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta0_vals</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">theta1_vals</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">J_vals</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x_contour</span><span class="p">,</span> <span class="n">y_contour</span> <span class="o">=</span> <span class="n">theta0_vals</span><span class="p">,</span> <span class="n">theta1_vals</span>
</span></span><span class="line"><span class="cl"><span class="n">theta0_vals</span><span class="p">,</span> <span class="n">theta1_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">theta0_vals</span><span class="p">,</span> <span class="n">theta1_vals</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Produce surface and contour plots of J(Œ∏)</span>
</span></span><span class="line"><span class="cl"><span class="n">fig2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">fig2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">theta0_vals</span><span class="p">,</span> <span class="n">theta1_vals</span><span class="p">,</span> <span class="n">J_vals</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Surface&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fig3</span><span class="p">,</span> <span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">CS</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">theta0_vals</span><span class="p">,</span> <span class="n">theta1_vals</span><span class="p">,</span> <span class="n">J_vals</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;bx&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Contour&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/uUr8wWJka7mXcHd.png" alt="image.png"  />
</p>
<ul>
<li>Training data with linear regression fit.</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/3SVAbhH5y78zXkc.png" alt="image.png"  />
</p>
<ul>
<li>Reducing of $J(\theta_0, \theta_1)$ during iteration</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/VWOMoKai5QcCAsn.png" alt="Figure_2.png"  />
</p>
<ul>
<li>Produce surface and contour plots of $J(Œ∏)$</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/3HfaXsZnPyuelWr.png" alt="Figure_3.png"  />
</p>
<p><img loading="lazy" src="https://i.loli.net/2020/03/03/wXyKdAUvBaHTWRY.png" alt="Figure_4.png"  />
</p>
<h4 id="ex12-linear-regression-with-multiple-variables">Ex1.2 Linear regression with multiple variables<a hidden class="anchor" aria-hidden="true" href="#ex12-linear-regression-with-multiple-variables">#</a></h4>
<p><strong>Instruction:</strong>
In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.</p>
<p><strong>Code:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 0: Basic Function &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Compute cost for linear regression
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param X: input variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param y: output variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param theta: parameters
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: Cost function
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">J</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">J</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gradientDescentMulti</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    updates theta by taking num_iters gradient steps with learning rate alpha
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param X: input variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param y: output variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param theta: parameters
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param alpha: learning rate
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param num_iters: times of iteration
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: [theta, J_history]
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize some useful values</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">J_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">temp</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">        <span class="n">J_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">featureNormalize</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Normalize the features in X
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param X: features
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: X_norm: normalized X; mean: mean value; sigma: standard deviation
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">X_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sigma</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">normalEqn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Computes the closed-form solution to linear regression
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param X: input variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param y: output variables
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: parameters
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">)</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 1: Feature Normalization &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Load data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;ex1data2.txt&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Scale features and set them to zero mean</span>
</span></span><span class="line"><span class="cl"><span class="n">X_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">featureNormalize</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_norm</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">featureNormalize</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Add intercept term to X</span>
</span></span><span class="line"><span class="cl"><span class="n">X_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">X_norm</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 2: Gradient Descent &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running gradient descent ... &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Choose some alpha value</span>
</span></span><span class="line"><span class="cl"><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl"><span class="n">num_iters</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Init theta and run gradient descent</span>
</span></span><span class="line"><span class="cl"><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span> <span class="o">=</span> <span class="n">gradientDescentMulti</span><span class="p">(</span><span class="n">X_norm</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the convergence graph</span>
</span></span><span class="line"><span class="cl"><span class="n">fig1</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_iters</span><span class="p">),</span> <span class="n">J_history</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Convergence of $J(\theta)$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display gradient descent&#39;s result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Theta computed from gradient descent: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34; Part 3: Normal Equations &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Solving with normal equations ... &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Add intercept term to X</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the parameters from the normal equation</span>
</span></span><span class="line"><span class="cl"><span class="n">theta1</span> <span class="o">=</span> <span class="n">normalEqn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Theta computed from normal equations: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">theta1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><strong>Output:</strong></p>
<ul>
<li>Console (There is some differences between the two results)</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/04/HTaInqFUc7e9hPu.png" alt="image.png"  />
</p>
<ul>
<li>The convergence graph</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/03/04/EDNHosUvmA8kw13.png" alt="Figure_5.png"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jifan.tech/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://jifan.tech/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://jifan.tech/tags/python/">Python</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jifan.tech/posts/2020-03-04-ml-ng-3/">
    <span class="title">¬´ Prev Page</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ</span>
  </a>
  <a class="next" href="https://jifan.tech/posts/2020-02-26-ml-ng-1/">
    <span class="title">Next Page ¬ª</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà1Ôºâ</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://jifan.tech">Jifan&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
