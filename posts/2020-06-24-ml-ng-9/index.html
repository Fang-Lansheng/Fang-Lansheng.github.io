<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ | Jifan&#39;s Blog</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ÔºöWeek9 - Anomaly Detection &amp; Recommender Systems
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application
 Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?
 Imagine you&rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you&rsquo;re doing QA  Measure some features from engines (e.">
<meta name="author" content="Thistledown">
<link rel="canonical" href="https://fang-lansheng.github.io/posts/2020-06-24-ml-ng-9/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://fang-lansheng.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://fang-lansheng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://fang-lansheng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://fang-lansheng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://fang-lansheng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ" />
<meta property="og:description" content="Course Link ÔºöWeek9 - Anomaly Detection &amp; Recommender Systems
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application
 Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?
 Imagine you&rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you&rsquo;re doing QA  Measure some features from engines (e." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fang-lansheng.github.io/posts/2020-06-24-ml-ng-9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-24T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-06-24T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ"/>
<meta name="twitter:description" content="Course Link ÔºöWeek9 - Anomaly Detection &amp; Recommender Systems
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application
 Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?
 Imagine you&rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you&rsquo;re doing QA  Measure some features from engines (e."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://fang-lansheng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ",
      "item": "https://fang-lansheng.github.io/posts/2020-06-24-ml-ng-9/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ",
  "name": "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ",
  "description": "Course Link ÔºöWeek9 - Anomaly Detection \u0026amp; Recommender Systems\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application\n Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?\n Imagine you\u0026rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you\u0026rsquo;re doing QA  Measure some features from engines (e.",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ÔºöWeek9 - Anomaly Detection \u0026 Recommender Systems\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application\n Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?\n Imagine you‚Äôre an aircraft engine manufacturer As engines roll off your assembly line you‚Äôre doing QA  Measure some features from engines (e.g. heat generated and vibration)   You now have a dataset of $x^1$ to $x^m$ (i.e. $m$ engines were tested)    More formally\n We have a dataset which contains normal (data)  How we ensure they‚Äôre normal is up to us In reality it‚Äôs OK if there are a few which aren‚Äôt actually normal   Using that dataset as a reference point we can see if other examples are anomalous    How do we do this?\n First, using our training dataset we build a model  We can access this model using $p(x)$  This asks, ‚ÄúWhat is the probability that example $x$ is normal‚Äù     Having built a model  if $p(x_{text}) if $p(x_{text}) \\ge \\varepsilon$, this is OK $\\epsilon$ is some threshold probability value which we define, depending on how sure we need/want to be      Applications\n Fraud detection  $x^{(i)}$ = features of user $i$‚Äôs activities Model $p(x)$ from data Identify unusual user by checking which have $p(x)   Manufacturing Monitoring computers in a data center  $x^{(i)}$ = features of machine $i$      28.2 Gaussian Distribution  Also called the normal distribution $X \\sim N(\\mu, \\sigma^2)$  $p(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left( - \\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$ $\\mu$: mean $\\sigma^2$: variance  $\\sigma$: the standard deviation   examples:    Parameter estimation  Dataset: $\\lbrace x^{(1)}, x^{(2)}, \\dots, x^{(m)} \\rbrace \\quad x^{(i)}\\in\\Bbb{R}$ $\\mu = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}$ $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m \\left( x^{(i)}-\\mu \\right)^2$  or $\\sigma^2 = \\frac{1}{m-1} \\sum_{i=1}^m \\left( x^{(i)}-\\mu \\right)^2$      28.3 Algorithm   Density estimation\n  Training set: $\\lbrace x^{(1)}, \\dots, x^{(m)}$, each example is $x \\in \\Bbb{R}^n$\n  $x_1 \\sim N(\\mu_1, \\sigma_1^2), \\dots, x_n \\sim N(\\mu_n, \\sigma_n^2)$\n  So\n$$ p(x) = p(x_1;\\mu_1, \\sigma_1^2)\\cdots p(x_n; \\mu_n, \\sigma^2_n) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) $$\n    Anomaly detection algorithm\n  Choose features $x_i$ that you think might be indicative of anomalous examples\n  Fit parameters $\\mu_1, \\dots, \\mu_n, \\sigma_1^2, \\dots, \\sigma_n^2$\n $$\\mu_j = \\frac{1}{m}\\sum_{i=1}^m x_j^{(i)}$$ $$ \\sigma^{2}{j} = \\frac{1}{m} \\sum{i=1}^m \\left( x^{(i)}_j-\\mu_j \\right)^2$$    Given new example $x$, compute $p(x)$:\n$$ p(x) = \\prod_{j=1}^n p(x_j;\\mu_j, \\sigma_j^2) = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma_j}\\exp \\left( -\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2} \\right) $$\n  Anomaly if $p(x)    Anomaly detection example   29 Building an Anomaly Detection System 29.1 Developing and Evaluating an Anomaly Detection System   The importance of real-number evaluation\n When developing a learning algorithm (choosing features etc.), making decisions is much easier if we have a way of evaluating our learning algorithm Assume we have some labeled data, of anomalous and non-anomalous examples. ($y=0$ if normal, $y=1$ if anomalous) Training set: $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$ (assume normal examples/not anomalous) Cross validation set: $(x_{cv}^{(1)}, y_{cv}^{(1)}), \\dots, (x_{cv}^{(m_{cv})}, y_{cv}^{(m_{cv})})$ Test set: $(x_{test}^{(1)}, y_{test}^{(1)}), \\dots, (x_{test}^{(m_{test})}, y_{test}^{(m_{test})})$    Algorithm evaluation\n  Fit model $p(x)$ on training set $\\lbrace x^{(1)}, \\dots, x^{(m)} \\rbrace$\n  On a cross validation/test example $x$, predict\n$$ y = \\begin{cases} 1 \u0026 \\text{if $p(x)    Possible evaluation metrics:\n True positive, false positive, false negative, true negative  Precision/Recall $F_1$-score      Can also use cross validation set to choose parameter $\\varepsilon$\n  29.2 Anomaly Detection vs. Supervised Learning 29.3 Choosing What Features to Use   Non-Gaussian features\n  Non-Gaussian data might look like this:\n  Can play with different transformations of the data to make it look more Gaussian:\n $x \\leftarrow \\log(x + c)$ $x \\leftarrow x^{\\text{small number}}$      Error analysis for anomaly detection\n Want:  $p(x)$ large for normal examples $x$ $p(x)$ small for anomalous examples $x$   Most common problems:  $p(x)$ is comparable (say, both large) for normal and anomalous examples   Solve (like supervised learning error analysis procedure):  Run algorithm on cross validation set See which one it got wrong Try coming up with more features to distinguish between the normal and the anomalous examples      30 Multivariate Gaussian Distribution 30.1 Multivariate Gaussian Distribution   Multivariate Gaussian (Normal) distribution\n  $x \\in \\Bbb{R}^n$. Don‚Äôt model $p(x_1), p(x_2), \\dots$, etc. separately.\n  Model $p(x)$ all in one go\n  Parameters: $\\mu \\in \\Bbb{R}^n$, $\\Sigma \\in \\Bbb{R}^{n \\times n}$ (covariance matrix)\n  Formula:\n$$ p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n    Multivariate Gaussian (Normal) examples:\n  30.2 Anomaly Detection using the Multivariate Gaussian Distribution   Multivariate Gaussian (Normal) distribution\n  Formula:\n$$ p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n Parameters $\\mu, \\Sigma$    Given training set $\\lbrace x^{(1)}, x^{(2)}, \\dots, x^{(m)} \\rbrace$\n  Parameter fitting\n  $$ \\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} $$\n  $$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} \\left( x^{(i)} - \\mu \\right)\\left( x^{(i)} - \\mu \\right)^T $$\n      Anomaly detection with the multivariate Gaussian\n  Fit model $p(x)$ by setting\n  $$ \\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} $$\n  $$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} \\left( x^{(i)} - \\mu \\right)\\left( x^{(i)} - \\mu \\right)^T $$\n    Given a new example $x$, compute\n$$ p(x )=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n    Flag an anomaly of $p(x)  Relationship to original model\n  Original model:\n$$ p(x) = p(x_1; \\mu_1, \\sigma_1^2) \\times p(x_2; \\mu_2, \\sigma_2^2) \\times \\cdots \\times p(x_n; \\mu_n, \\sigma_n^2) $$\n  Corresponds to multivariate Gaussian:\n$$ p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n Where $$ \\Sigma = \\begin{bmatrix}\n\\sigma_{1}^2 \u0026 \u0026 \u0026 \\\n\u0026 \\sigma _2^2 \u0026 \u0026 \\ \u0026 \u0026 \\ddots \u0026 \\\n\u0026 \u0026 \u0026 \\sigma_n^2 \\\n\\end{bmatrix}\n$$      Comparison   31 Predicting Movie Ratings 31.1 Problem Formulation   Example: Predicting movie ratings\n  User rates movies using zero to five stars\n  And we have\n  Let\n $n_u$ = no. users $n_m$ = no. movies $r(i, j) = 1$ if user $j$ has rated movie $i$ $y(i, j)$ = rating given by user $j$ to movie $i$ (defined only if $r(i,j)=1$)    The problem is as follows\n Given $ r(i,j)$ and $y(i,j)$, go through and try and predict missing values Come up with a learning algorithm that can fill in these missing values      31.2 Content Based Recommendations   Using our example above, how do we predict?\n For each movie we have a feature which measure degree to which each film is a  Romance ($x_1$) Action ($x_2$)   If we have features like these, each film can be recommended by a feature vector  Add an extra feature which is $x_0 = 1$ for each film So for each film we have a $[3 \\times 1]$ vector, which for film number 1 (‚ÄúLove at Last‚Äù) would be $x^{(1)} = \\begin{bmatrix} 1 \u0026 0.9 \u0026 0 \\end{bmatrix}^T$ i.e. for our dataset we have $\\lbrace x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}, x^{(5)} \\rbrace$  Where each of these is a $[3\\times 1]$ vector with an $x_0 = 1$ and then a romance and an action score     We could treat each rating for each user as a separate linear regression problem  For each user $j$ we could learn a parameter vector $\\theta^{(j)} \\in \\Bbb{R}^{n+1}$ Then predict that user $j$ will rate movie $i $ with $(Œ∏^{(j)})^T x^{(i)} = \\text{stars}$   All we‚Äôre doing here is applying a linear regression method for each user  So we determine a future rating based on their interest in romance and action based on previous films   We should also add one final piece of notation  $m^{(j)}$, number of movies rated by the user $j$      Problem formulation\n  Notations:\n $r(i, j) = 1$ if user $j$ has rated movie $i$ $y(i, j)$ = rating given by user $j$ to movie $i$ (if defined) $\\theta^{(j)} \\in \\Bbb{R}^{n+1}$ = parameters vector for user $j$ $x^{(i)}\\in \\Bbb{R}^{n+1}$ = feature vector for movie $i$ $m^{(j)}$ = no. of movies rated by user $j$    For user $j$, movie $i$, predict rating: $\\left( \\theta^{(j)} \\right)^T x^{(i)}$\n  To learn $\\theta^{(j)}$:\n$$ \\min_{\\theta^{(j)}} \\frac{1}{2m^{(j)}} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2 m^{(j)}} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n $m^{(j)}$ is a constant value - you can remove it      Optimization objective\n  To learn $\\theta^{(j)}$ (parameter for user $j$):\n$$ \\min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2 } \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n    To learn $\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(n_u)}$\n$$ \\min_{\\theta^{(1)}, \\dots, \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n  Optimization algorithm\n$$ \\min_{\\theta^{(1)}, \\dots, \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n  Gradient descent update\n  $$ \\theta_k^{(j)}:= \\theta_k^{(j)} - \\alpha \\sum_{i:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) x_k^{(i)} \\qquad \\text{for $k=0$} $$\n  $$ \\theta_k^{(j)}:= \\theta_k^{(j)} - \\alpha \\left( \\sum_{i:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) x_k^{(i)} + \\lambda \\theta^{(j)}_k \\right) \\qquad \\text{for $k \\ne 0$} $$\n      32 Collaborative Filtering 32.1 Collaborative Filtering   The collaborative filtering algorithm has a very interesting property - does feature learning\n i.e. it can learn for itself what features it needs to learn    Recall our original data set above for our five films and four raters\n Here we assume someone had calculated the ‚Äúromance‚Äù and ‚Äúaction‚Äù amounts of the films  This can be very hard to do in reality Often want more features than just two      So - let‚Äôs change the problem and pretend we have a data set where we don‚Äôt know any of the features associated with the films\n Now let‚Äôs make a different assumption  We‚Äôve polled each user and found out how much each user likes  Romantic films Action films   Which has generated the following parameter set Alice and Bob like romance but hate action Carol and Dave like action but hate romance      If we can get these parameters from the users we can infer the missing values from our table\n Lets look at ‚ÄúLove at Last‚Äù  Alice and Bob loved it Carol and Dave hated it   We know from the feature vectors Alice and Bob love romantic films, while Carol and Dave hate them  Based on the factor Alice and Bob liked ‚ÄúLove at Last‚Äù and Carol and Dave hated it we may be able to (correctly) conclude that ‚ÄúLove at Last‚Äù is a romantic film      This is a bit of a simplification in terms of the math, but what we‚Äôre really asking is\n What feature vector $x^{(1)}$ should be - so that  $(\\theta^{(1)})^T x_1$ is about 5 $(\\theta^{(2)})^T x_1$ is about 5 $(\\theta^{(3)})^T x_1$ is about 0 $(\\theta^{(4)})^T x_1$ is about 0   From this we can guess that $x^{(1)}$ may be $\\begin{bmatrix} 1 \u0026 1.0 \u0026 0.0 \\end{bmatrix}^T$ Using that same approach we should then be able to determine the remaining feature vectors for the other films    Formalizing the collaborative filtering problem\n  Given $\\theta^{(1)}, \\dots, \\theta^{(n_u)}$ (i.e. given the parameter vectors for each users‚Äô preferences), to learn $x^{(i)}$\n  We must minimize an optimization function which tries to identify the best parameter vector associated with a film\n$$ \\min_{x^{(i)}} \\frac{1}{2} \\sum_{j:r(i, j)=1)} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} \\left( x_k^{(i)} \\right)^2 $$\n So we‚Äôre summing over all the indices $j$ for where we have data for movie $i$ We‚Äôre minimizing this squared error    Like before, the above equation gives us a way to learn the features for one film\n We want to learn all the features for all the films - so we need an additional summation term      How does this work with the previous recommendation system\n Content based recommendation systems  Saw that if we have a set of features for movie rating you can learn a user‚Äôs preferences   Now  If you have your users preferences you can therefore determine a film‚Äôs features   This is a bit of a chicken \u0026 egg problem What you can do is  Randomly guess values for $\\theta$ Then use collaborative filtering to generate $x$ Then use content based recommendation to improve $\\theta$ Use that to improve $x$ And so on   This actually works  Causes your algorithm to converge on a reasonable set of parameters This is collaborative filtering   We call it collaborative filtering because in this example the users are collaborating together to help the algorithm learn better features and help the system and the other users    32.2 Collaborative Filtering Algorithm   Here we combine the ideas from before to build a collaborative filtering algorithm\n  Collaborative filtering optimization objective\n  If we‚Äôre given the film‚Äôs features we can use that to work out the users‚Äô preference:\n$$ \\text{Given $x^{(1)}, \\dots, x^{(n_m)} $, estimate $ \\theta^{(1)}, \\dots, \\theta^{(n_u)} $ :} \\ \\min_{\\theta^{(1)}, \\dots, \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n    If we‚Äôre given the users‚Äô preferences we can use them to work out the film‚Äôs features\n$$ \\text{Given $ \\theta^{(1)}, \\dots, \\theta^{(n_u)} $, estimate $x^{(1)}, \\dots, x^{(n_m)} $ :} \\ \\min_{x^{(1)}, \\dots, x^{(n_m)}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^n \\left(x_k^{(i)} \\right)^2 $$\n  One thing you could do is\n Randomly initialize parameter    Go back and forward\n    But there‚Äôs a more efficient algorithm which can solve $\\theta$ and $x$ simultaneously\n Define a new optimization objective which is a function of $x$ and $\\theta$  $$ \\text{Minimizing $ x^{(1)}, \\dots, x^{(n_m)}$ and $\\theta^{(1)}, \\dots, \\theta^{(n_u)} $ simultaneously :} \\\n\\begin{aligned} J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)}) \u0026= \\frac{1}{2} \\sum_{(i, j):r(i, j)=1} \\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\ \u0026= \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^n \\left(x_k^{(i)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 \\end{aligned} $$\n  Goal:\n$$ \\min_{\\mathop{x^{(1)}, \\dots, x^{(n_m)}\\\\theta^{(1)}, \\dots, \\theta^{(n_u)}} }J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)}) $$\n    Understanding this optimization objective\n  The squared error term is the same as the squared error term in the two individual objectives above $\\sum_{(i, j):r(i, j)=1} \\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2$\n So it‚Äôs summing over every movie rated by every users Note the ‚Äú$:$‚Äù means, ‚Äúfor which‚Äù  Sum over all pairs $(i,j)$ for which $r(i,j)$ is equal to 1      The regularization terms\n Are simply added to the end from the original two optimization functions    This newly defined function has the property that\n If you held $x$ constant and only solved $\\theta$ then you solve the, ‚ÄúGiven $x$, solve $\\theta$‚Äù objective above Similarly, if you held $\\theta$ constant you could solve $ x $    In order to come up with just one optimization function we treat this function as a function of both film features $x$ and user parameters $\\theta$\n Only difference between this in the back-and-forward approach is that we minimize with respect to both $x$ and $\\theta$ simultaneously    When we‚Äôre learning the features this way\n  Previously had a convention that we have an $x_0 = 1$ term\n  When we‚Äôre using this kind of approach we have no\n$x_0$,\n So now our vectors (both $x$ and $\\theta$) are $n$-dimensional (NOT $n+1$) i.e. $x \\in \\Bbb{R}^n, \\theta \\in \\Bbb{R}^n$    We do this because we are now learning all the features so if the system needs a feature always = 1 then the algorithm can learn one\n      Collaborative filtering algorithm\n   Initialize $\\theta^{(1)}, \\dots, \\theta^{(n_u)}$ and $x^{(1)}, \\dots, x^{(n_m)}$ to small random values   A bit like neural networks - initialize all parameters to small random numbers    Minimize cost function $J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)})$ using gradient descent  $$ \\text{for every $j=1, \\dots, n_u, i = 1, \\dots, n_m $ :} \\ x_k^{(i)}:= x_k^{(i)} - \\alpha \\left( \\sum_{j:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) \\theta^{(j)}k + \\lambda x_k^{(i)} \\right) \\ \\theta_k^{(j)}:= \\theta_k^{(j)} - \\alpha \\left( \\sum{i:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) x_k^{(i)} + \\lambda \\theta^{(j)}_k \\right) $$\n  Having minimized the values, given a user (user $j$) with parameters $\\theta$ and movie (movie $i$) with learned features $x$, we predict a star rating of $\\theta^Tx$      33 Low Rank Matrix Factorization 33.1 Vectorization: Low Rank Matrix Factorization  low rank matrix factorization  $Y = \\begin{bmatrix} \\left(\\theta^{(1)}\\right)^{T}\\left(x^{(1)}\\right) \u0026 \\left(\\theta^{(2)}\\right)^{T}\\left(x^{(1)}\\right) \u0026 \\dots \u0026 \\left(\\theta^{\\left(n_{u}\\right)}\\right)^{T}\\left(x^{(1)}\\right) \\newline \\left(\\theta^{(1)}\\right)^{T}\\left(x^{(2)}\\right) \u0026 \\left(\\theta^{(2)}\\right)^{T}\\left(x^{(2)}\\right) \u0026 \\dots \u0026 \\left(\\theta^{\\left(n_{u}\\right)}\\right)^{T}\\left(x^{(2)}\\right) \\newline \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\newline \\left(\\theta^{(1)}\\right)^{T}\\left(x^{\\left(n_{m}\\right)}\\right) \u0026 \\left(\\theta^{(2)}\\right)^{T}\\left(x^{\\left(n_{m}\\right)}\\right) \u0026 \\dots \u0026 \\left(\\theta^{\\left(n_{u}\\right)}\\right)^{T}\\left(x^{\\left(n_{m}\\right)}\\right) \\end{bmatrix}$ $X = \\begin{bmatrix}-\\left( x^{(1)} \\right)^T- \\\\ \\cdots \\\\ -\\left( x^{(n_m)} \\right)^T- \\end{bmatrix}, \\Theta = \\begin{bmatrix}-\\left( \\theta^{(1)} \\right)^T- \\\\ \\cdots \\\\ -\\left( \\theta^{(n_u)} \\right)^T- \\end{bmatrix}$ $Y = X\\Theta^T$   Find related movies  For each product $i$, we learn a feature vector $x^{(i)} \\in \\Bbb{R}^n$ How to find movie $j$ related to movie $i$ ?  $\\text{small } |x^{(i)} - x^{(j)}| \\rightarrow \\text{movie $j$ and $i$ are ‚Äúsimilar‚Äù}$      33.2 Implementational Detail: Mean Normalization Ex8: Anomaly Detection and Recommender Systemüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex8/ex8.ipynb\n ",
  "wordCount" : "2688",
  "inLanguage": "en",
  "datePublished": "2020-06-24T00:00:00Z",
  "dateModified": "2020-06-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thistledown"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fang-lansheng.github.io/posts/2020-06-24-ml-ng-9/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jifan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fang-lansheng.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://fang-lansheng.github.io" accesskey="h" title="Jifan&#39;s Blog (Alt + H)">Jifan&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://fang-lansheng.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://fang-lansheng.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://fang-lansheng.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ
    </h1>
    <div class="post-meta"><span title='2020-06-24 00:00:00 +0000 UTC'>June 24, 2020</span>&nbsp;¬∑&nbsp;Thistledown

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#28-density-estimation" aria-label="28 Density Estimation">28 Density Estimation</a><ul>
                        <ul>
                        
                <li>
                    <a href="#281-problem-motivation" aria-label="28.1 Problem Motivation">28.1 Problem Motivation</a></li>
                <li>
                    <a href="#282-gaussian-distribution" aria-label="28.2 Gaussian Distribution">28.2 Gaussian Distribution</a></li>
                <li>
                    <a href="#283-algorithm" aria-label="28.3 Algorithm">28.3 Algorithm</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#29-building-an-anomaly-detection-system" aria-label="29 Building an Anomaly Detection System">29 Building an Anomaly Detection System</a><ul>
                        <ul>
                        
                <li>
                    <a href="#291-developing-and-evaluating-an-anomaly-detection-system" aria-label="29.1 Developing and Evaluating an Anomaly Detection System">29.1 Developing and Evaluating an Anomaly Detection System</a></li>
                <li>
                    <a href="#292-anomaly-detection-vs-supervised-learning" aria-label="29.2 Anomaly Detection vs. Supervised Learning">29.2 Anomaly Detection vs. Supervised Learning</a></li>
                <li>
                    <a href="#293-choosing-what-features-to-use" aria-label="29.3 Choosing What Features to Use">29.3 Choosing What Features to Use</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#30-multivariate-gaussian-distribution" aria-label="30 Multivariate Gaussian Distribution">30 Multivariate Gaussian Distribution</a><ul>
                        <ul>
                        
                <li>
                    <a href="#301-multivariate-gaussian-distribution" aria-label="30.1 Multivariate Gaussian Distribution">30.1 Multivariate Gaussian Distribution</a></li>
                <li>
                    <a href="#302-anomaly-detection-using-the-multivariate-gaussian-distribution" aria-label="30.2 Anomaly Detection using the Multivariate Gaussian Distribution">30.2 Anomaly Detection using the Multivariate Gaussian Distribution</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#31-predicting-movie-ratings" aria-label="31 Predicting Movie Ratings">31 Predicting Movie Ratings</a><ul>
                        <ul>
                        
                <li>
                    <a href="#311-problem-formulation" aria-label="31.1 Problem Formulation">31.1 Problem Formulation</a></li>
                <li>
                    <a href="#312-content-based-recommendations" aria-label="31.2 Content Based Recommendations">31.2 Content Based Recommendations</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#32-collaborative-filtering" aria-label="32 Collaborative Filtering">32 Collaborative Filtering</a><ul>
                        <ul>
                        
                <li>
                    <a href="#321-collaborative-filtering" aria-label="32.1 Collaborative Filtering">32.1 Collaborative Filtering</a></li>
                <li>
                    <a href="#322-collaborative-filtering-algorithm" aria-label="32.2 Collaborative Filtering Algorithm">32.2 Collaborative Filtering Algorithm</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#33-low-rank-matrix-factorization" aria-label="33 Low Rank Matrix Factorization">33 Low Rank Matrix Factorization</a><ul>
                        <ul>
                        
                <li>
                    <a href="#331-vectorization-low-rank-matrix-factorization" aria-label="33.1 Vectorization: Low Rank Matrix Factorization">33.1 Vectorization: Low Rank Matrix Factorization</a></li>
                <li>
                    <a href="#332-implementational-detail-mean-normalization" aria-label="33.2 Implementational Detail: Mean Normalization">33.2 Implementational Detail: Mean Normalization</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#ex8-anomaly-detection-and-recommender-system" aria-label="Ex8: Anomaly Detection and Recommender Systemüë®‚Äçüíª">Ex8: Anomaly Detection and Recommender Systemüë®‚Äçüíª</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Course Link Ôºö<a href="https://www.coursera.org/learn/machine-learning/home/week/9">Week9 - Anomaly Detection &amp; Recommender Systems</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="28-density-estimation">28 Density Estimation<a hidden class="anchor" aria-hidden="true" href="#28-density-estimation">#</a></h2>
<h4 id="281-problem-motivation">28.1 Problem Motivation<a hidden class="anchor" aria-hidden="true" href="#281-problem-motivation">#</a></h4>
<ul>
<li>
<p>Anomaly detection is a reasonably commonly used type of machine learning application</p>
<ul>
<li>Can be thought of as a solution to an unsupervised learning problem</li>
<li>But, has aspects of supervised learning</li>
</ul>
</li>
<li>
<p>What is anomaly detection?</p>
<ul>
<li>Imagine you&rsquo;re an aircraft engine manufacturer</li>
<li>As engines roll off your assembly line you&rsquo;re doing QA
<ul>
<li>Measure some features from engines (e.g. heat generated and vibration)</li>
</ul>
</li>
<li>You now have a dataset of $x^1$ to $x^m$ (i.e. $m$ engines were tested)</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/MUBhamjxZrAOeqE.png" alt="image-20200624091718538"  />
</p>
</li>
<li>
<p>More formally</p>
<ul>
<li>We have a dataset which contains <strong>normal</strong> (data)
<ul>
<li>How we ensure they&rsquo;re normal is up to us</li>
<li>In reality it&rsquo;s OK if there are a few which aren&rsquo;t actually normal</li>
</ul>
</li>
<li>Using that dataset as a reference point we can see if other examples are <strong>anomalous</strong></li>
</ul>
</li>
<li>
<p>How do we do this?</p>
<ul>
<li>First, using our training dataset we build a model
<ul>
<li>We can access this model using $p(x)$
<ul>
<li>This asks, &ldquo;What is the probability that example $x$ is normal&rdquo;</li>
</ul>
</li>
</ul>
</li>
<li>Having built a model
<ul>
<li>if $p(x_{text}) &lt; \varepsilon$, flag this as an anomaly</li>
<li>if $p(x_{text}) \ge \varepsilon$, this is OK</li>
<li>$\epsilon$ is some threshold probability value which we define, depending on how sure we need/want to be</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Applications</p>
<ul>
<li>Fraud detection
<ul>
<li>$x^{(i)}$ = features of user $i$&rsquo;s activities</li>
<li>Model $p(x)$ from data</li>
<li>Identify unusual user by checking which have $p(x) &lt; \varepsilon$</li>
</ul>
</li>
<li>Manufacturing</li>
<li>Monitoring computers in a data center
<ul>
<li>$x^{(i)}$ = features of machine $i$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="282-gaussian-distribution">28.2 Gaussian Distribution<a hidden class="anchor" aria-hidden="true" href="#282-gaussian-distribution">#</a></h4>
<ul>
<li>Also called the <strong>normal distribution</strong></li>
<li>$X \sim N(\mu, \sigma^2)$
<ul>
<li>$p(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi} \sigma} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)$</li>
<li>$\mu$: mean</li>
<li>$\sigma^2$: variance
<ul>
<li>$\sigma$: the standard deviation</li>
</ul>
</li>
<li>examples:
<img loading="lazy" src="https://i.loli.net/2020/06/24/BNZoxt3LTUiE2kS.png" alt="image-20200624093327972"  />
</li>
</ul>
</li>
<li>Parameter estimation
<ul>
<li>Dataset: $\lbrace x^{(1)}, x^{(2)}, \dots, x^{(m)} \rbrace \quad x^{(i)}\in\Bbb{R}$</li>
<li>$\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)}$</li>
<li>$\sigma^2 = \frac{1}{m} \sum_{i=1}^m \left( x^{(i)}-\mu \right)^2$
<ul>
<li>or $\sigma^2 = \frac{1}{m-1} \sum_{i=1}^m \left( x^{(i)}-\mu \right)^2$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="283-algorithm">28.3 Algorithm<a hidden class="anchor" aria-hidden="true" href="#283-algorithm">#</a></h4>
<ul>
<li>
<p>Density estimation</p>
<ul>
<li>
<p>Training set: $\lbrace x^{(1)}, \dots, x^{(m)}$, each example is $x \in \Bbb{R}^n$</p>
</li>
<li>
<p>$x_1 \sim N(\mu_1, \sigma_1^2), \dots, x_n \sim N(\mu_n, \sigma_n^2)$</p>
</li>
<li>
<p>So</p>
<p>$$
p(x) = p(x_1;\mu_1, \sigma_1^2)\cdots p(x_n; \mu_n, \sigma^2_n) = \prod_{j=1}^n p(x_j; \mu_j, \sigma_j^2)
$$</p>
</li>
</ul>
</li>
<li>
<p>Anomaly detection algorithm</p>
<ul>
<li>
<p>Choose features $x_i$ that you think might be indicative of anomalous examples</p>
</li>
<li>
<p>Fit parameters $\mu_1, \dots, \mu_n, \sigma_1^2, \dots, \sigma_n^2$</p>
<ul>
<li>$$\mu_j = \frac{1}{m}\sum_{i=1}^m x_j^{(i)}$$</li>
<li>$$ \sigma^{2}<em>{j} = \frac{1}{m} \sum</em>{i=1}^m \left( x^{(i)}_j-\mu_j \right)^2$$</li>
</ul>
</li>
<li>
<p>Given new example $x$, compute $p(x)$:</p>
<p>$$
p(x) = \prod_{j=1}^n p(x_j;\mu_j, \sigma_j^2) = \prod_{j=1}^n \frac{1}{\sqrt{2\pi}\sigma_j}\exp \left( -\frac{(x_j-\mu_j)^2}{2\sigma_j^2} \right)
$$</p>
</li>
<li>
<p>Anomaly if $p(x) &lt; \varepsilon$</p>
</li>
</ul>
</li>
<li>
<p>Anomaly detection example
<img loading="lazy" src="https://i.loli.net/2020/06/24/KnyOqhVcrGbdXp5.png" alt="image-20200624095703162"  />
</p>
</li>
</ul>
<h2 id="29-building-an-anomaly-detection-system">29 Building an Anomaly Detection System<a hidden class="anchor" aria-hidden="true" href="#29-building-an-anomaly-detection-system">#</a></h2>
<h4 id="291-developing-and-evaluating-an-anomaly-detection-system">29.1 Developing and Evaluating an Anomaly Detection System<a hidden class="anchor" aria-hidden="true" href="#291-developing-and-evaluating-an-anomaly-detection-system">#</a></h4>
<ul>
<li>
<p>The importance of real-number evaluation</p>
<ul>
<li>When developing a learning algorithm (choosing features etc.), making decisions is much easier if we have a way of evaluating our learning algorithm</li>
<li>Assume we have some labeled data, of anomalous and non-anomalous examples. ($y=0$ if normal, $y=1$ if anomalous)</li>
<li>Training set: $x^{(1)}, x^{(2)}, \dots, x^{(m)}$ (assume normal examples/not anomalous)</li>
<li>Cross validation set: $(x_{cv}^{(1)}, y_{cv}^{(1)}), \dots, (x_{cv}^{(m_{cv})}, y_{cv}^{(m_{cv})})$</li>
<li>Test set: $(x_{test}^{(1)}, y_{test}^{(1)}), \dots, (x_{test}^{(m_{test})}, y_{test}^{(m_{test})})$</li>
</ul>
</li>
<li>
<p>Algorithm evaluation</p>
<ul>
<li>
<p>Fit model $p(x)$ on training set $\lbrace x^{(1)}, \dots, x^{(m)} \rbrace$</p>
</li>
<li>
<p>On a cross validation/test example $x$, predict</p>
<p>$$
y = \begin{cases}
1 &amp; \text{if $p(x) &lt; \varepsilon$ (anomaly)} \
0 &amp; \text{if $p(x) \ge \varepsilon$ (normal)}
\end{cases}
$$</p>
</li>
</ul>
</li>
<li>
<p>Possible evaluation metrics:</p>
<ul>
<li>True positive, false positive, false negative, true negative
<ul>
<li>Precision/Recall</li>
<li>$F_1$-score</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Can also use cross validation set to choose parameter $\varepsilon$</p>
</li>
</ul>
<h4 id="292-anomaly-detection-vs-supervised-learning">29.2 Anomaly Detection vs. Supervised Learning<a hidden class="anchor" aria-hidden="true" href="#292-anomaly-detection-vs-supervised-learning">#</a></h4>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/cJlo8L5w2PYKgH1.png" alt="image-20200624102257992"  />
</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/lQyhN6nR84SitOv.png" alt="image-20200624102528559"  />
</p>
<h4 id="293-choosing-what-features-to-use">29.3 Choosing What Features to Use<a hidden class="anchor" aria-hidden="true" href="#293-choosing-what-features-to-use">#</a></h4>
<ul>
<li>
<p>Non-Gaussian features</p>
<ul>
<li>
<p>Non-Gaussian data might look like this:</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/1WgPCEAYIHezn6X.png" alt="image-20200624104011269"  />
</p>
</li>
<li>
<p>Can play with different transformations of the data to make it look more Gaussian:</p>
<ul>
<li>$x \leftarrow \log(x + c)$</li>
<li>$x \leftarrow x^{\text{small number}}$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Error analysis for anomaly detection</p>
<ul>
<li>Want:
<ul>
<li>$p(x)$ large for normal examples $x$</li>
<li>$p(x)$ small for anomalous examples $x$</li>
</ul>
</li>
<li>Most common problems:
<ul>
<li>$p(x)$ is comparable (say, both large) for normal and anomalous examples</li>
</ul>
</li>
<li>Solve (like supervised learning error analysis procedure):
<ul>
<li>Run algorithm on cross validation set</li>
<li>See which one it got wrong</li>
<li>Try coming up with more features to distinguish between the normal and the anomalous examples</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="30-multivariate-gaussian-distribution">30 Multivariate Gaussian Distribution<a hidden class="anchor" aria-hidden="true" href="#30-multivariate-gaussian-distribution">#</a></h2>
<h4 id="301-multivariate-gaussian-distribution">30.1 Multivariate Gaussian Distribution<a hidden class="anchor" aria-hidden="true" href="#301-multivariate-gaussian-distribution">#</a></h4>
<ul>
<li>
<p>Multivariate Gaussian (Normal) distribution</p>
<ul>
<li>
<p>$x \in \Bbb{R}^n$. Don&rsquo;t model $p(x_1), p(x_2), \dots$, etc. separately.</p>
</li>
<li>
<p>Model $p(x)$ all in one go</p>
</li>
<li>
<p>Parameters: $\mu \in \Bbb{R}^n$, $\Sigma \in \Bbb{R}^{n \times n}$ (covariance matrix)</p>
</li>
<li>
<p>Formula:</p>
<p>$$
p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
$$</p>
</li>
</ul>
</li>
<li>
<p>Multivariate Gaussian (Normal) examples:</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/CPGhnb96cmdoxY5.png" alt="image-20200624105313756"  />

<img loading="lazy" src="https://i.loli.net/2020/06/24/sQMUzoIZ9evbnYO.png" alt="image-20200624105403097"  />

<img loading="lazy" src="https://i.loli.net/2020/06/24/uFGsl5iI9xhCRjW.png" alt="image-20200624105427252"  />

<img loading="lazy" src="https://i.loli.net/2020/06/24/qkg1OKVa4A9RFJ5.png" alt="image-20200624105527526"  />
</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/rETdy2BipKUawgZ.png" alt="image-20200624105632945"  />

<img loading="lazy" src="https://i.loli.net/2020/06/24/Je19rYbDpyNwZgG.png" alt="image-20200624105802039"  />
</p>
</li>
</ul>
<h4 id="302-anomaly-detection-using-the-multivariate-gaussian-distribution">30.2 Anomaly Detection using the Multivariate Gaussian Distribution<a hidden class="anchor" aria-hidden="true" href="#302-anomaly-detection-using-the-multivariate-gaussian-distribution">#</a></h4>
<ul>
<li>
<p>Multivariate Gaussian (Normal) distribution</p>
<ul>
<li>
<p>Formula:</p>
<p>$$
p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
$$</p>
<ul>
<li>Parameters $\mu, \Sigma$</li>
</ul>
</li>
<li>
<p>Given training set $\lbrace x^{(1)}, x^{(2)}, \dots, x^{(m)} \rbrace$</p>
</li>
<li>
<p>Parameter fitting</p>
<ul>
<li>
<p>$$
\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}
$$</p>
</li>
<li>
<p>$$
\Sigma = \frac{1}{m} \sum_{i=1}^{m} \left( x^{(i)} - \mu \right)\left( x^{(i)} - \mu \right)^T
$$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Anomaly detection with the multivariate Gaussian</p>
<ul>
<li>
<p>Fit model $p(x)$ by setting</p>
<ul>
<li>
<p>$$
\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}
$$</p>
</li>
<li>
<p>$$
\Sigma = \frac{1}{m} \sum_{i=1}^{m} \left( x^{(i)} - \mu \right)\left( x^{(i)} - \mu \right)^T
$$</p>
</li>
</ul>
</li>
<li>
<p>Given a new example $x$, compute</p>
<p>$$
p(x )=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
$$</p>
</li>
</ul>
</li>
<li>
<p>Flag an anomaly of $p(x) &lt; \varepsilon$</p>
</li>
<li>
<p>Relationship to original model</p>
<ul>
<li>
<p>Original model:</p>
<p>$$
p(x) = p(x_1; \mu_1, \sigma_1^2) \times p(x_2; \mu_2, \sigma_2^2) \times \cdots
\times p(x_n; \mu_n, \sigma_n^2)
$$</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/urA1kx47Sym6aIZ.png" alt="image-20200624111749404"  />
</p>
</li>
<li>
<p>Corresponds to multivariate Gaussian:</p>
<p>$$
p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
$$</p>
<ul>
<li>Where
$$
\Sigma = \begin{bmatrix}<br>
\sigma_{1}^2 &amp;             &amp;          &amp;  \<br>
&amp; \sigma _2^2 &amp;          &amp;  \
&amp;             &amp; \ddots   &amp;  \<br>
&amp;             &amp;          &amp; \sigma_n^2  \<br>
\end{bmatrix}<br>
$$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Comparison
<img loading="lazy" src="https://i.loli.net/2020/06/24/95O8WqJUIaBPn4x.png" alt="image-20200624112253898"  />
</p>
</li>
</ul>
<h2 id="31-predicting-movie-ratings">31 Predicting Movie Ratings<a hidden class="anchor" aria-hidden="true" href="#31-predicting-movie-ratings">#</a></h2>
<h4 id="311-problem-formulation">31.1 Problem Formulation<a hidden class="anchor" aria-hidden="true" href="#311-problem-formulation">#</a></h4>
<ul>
<li>
<p>Example: Predicting movie ratings</p>
<ul>
<li>
<p>User rates movies using zero to five stars</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/NjdeQVfDHq4Mvam.png" alt="image-20200624114048801"  />
</p>
</li>
<li>
<p>And we have</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/CVNvFUj8rsyO3fJ.png" alt="image-20200624114421357"  />
</p>
</li>
<li>
<p>Let</p>
<ul>
<li>$n_u$ = no. users</li>
<li>$n_m$ = no. movies</li>
<li>$r(i, j) = 1$ if user $j$ has rated movie $i$</li>
<li>$y(i, j)$ = rating given by user $j$ to movie $i$ (defined only if $r(i,j)=1$)</li>
</ul>
</li>
<li>
<p>The problem is as follows</p>
<ul>
<li>Given $ r(i,j)$ and $y(i,j)$, go through and try and predict missing values</li>
<li>Come up with a learning algorithm that can fill in these missing values</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="312-content-based-recommendations">31.2 Content Based Recommendations<a hidden class="anchor" aria-hidden="true" href="#312-content-based-recommendations">#</a></h4>
<ul>
<li>
<p>Using our example above, how do we predict?</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/94yx3MbrU5JeNqz.png" alt="image-20200624115154254"  />
</p>
<ul>
<li>For each movie we have a feature which measure degree to which each film is a
<ul>
<li>Romance ($x_1$)</li>
<li>Action ($x_2$)</li>
</ul>
</li>
<li>If we have features like these, each film can be recommended by a feature vector
<ul>
<li>Add an extra feature which is $x_0 = 1$ for each film</li>
<li>So for each film we have a $[3 \times 1]$ vector, which for film number 1 (&ldquo;Love at Last&rdquo;) would be $x^{(1)} = \begin{bmatrix} 1 &amp; 0.9 &amp; 0 \end{bmatrix}^T$</li>
<li>i.e. for our dataset we have $\lbrace x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}, x^{(5)} \rbrace$
<ul>
<li>Where each of these is a $[3\times 1]$ vector with an $x_0 = 1$ and then a romance and an action score</li>
</ul>
</li>
</ul>
</li>
<li>We could treat each rating for each user as a separate linear regression problem
<ul>
<li>For each user $j$ we could learn a parameter vector $\theta^{(j)} \in \Bbb{R}^{n+1}$</li>
<li>Then predict that user $j$ will rate movie $i $ with $(Œ∏^{(j)})^T x^{(i)} = \text{stars}$</li>
</ul>
</li>
<li>All we&rsquo;re doing here is applying a linear regression method for each user
<ul>
<li>So we determine a future rating based on their interest in romance and action based on previous films</li>
</ul>
</li>
<li>We should also add one final piece of notation
<ul>
<li>$m^{(j)}$, number of movies rated by the user $j$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Problem formulation</p>
<ul>
<li>
<p>Notations:</p>
<ul>
<li>$r(i, j) = 1$ if user $j$ has rated movie $i$</li>
<li>$y(i, j)$ = rating given by user $j$ to movie $i$ (if defined)</li>
<li>$\theta^{(j)} \in \Bbb{R}^{n+1}$ = parameters vector for user $j$</li>
<li>$x^{(i)}\in \Bbb{R}^{n+1}$ = feature vector for movie $i$</li>
<li>$m^{(j)}$ = no. of movies rated by user $j$</li>
</ul>
</li>
<li>
<p>For user $j$, movie $i$, predict rating: $\left( \theta^{(j)} \right)^T x^{(i)}$</p>
</li>
<li>
<p>To learn $\theta^{(j)}$:<br>
$$
\min_{\theta^{(j)}} \frac{1}{2m^{(j)}} \sum_{i:r(i, j)=1}\left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \frac{\lambda}{2 m^{(j)}} \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
$$</p>
<ul>
<li>$m^{(j)}$ is a constant value - you can remove it</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Optimization objective</p>
<ul>
<li>
<p>To learn $\theta^{(j)}$ (parameter for user $j$):</p>
<p>$$
\min_{\theta^{(j)}} \frac{1}{2} \sum_{i:r(i, j)=1}\left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \frac{\lambda}{2 } \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
$$</p>
</li>
</ul>
</li>
<li>
<p>To learn $\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(n_u)}$</p>
<p>$$
\min_{\theta^{(1)},  \dots, \theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i, j)=1}\left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
$$</p>
</li>
<li>
<p>Optimization algorithm</p>
<p>$$
\min_{\theta^{(1)},  \dots, \theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i, j)=1}\left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
$$</p>
<ul>
<li>
<p>Gradient descent update</p>
<ul>
<li>
<p>$$
\theta_k^{(j)}:= \theta_k^{(j)} - \alpha \sum_{i:r(i,j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right) x_k^{(i)} \qquad \text{for $k=0$}
$$</p>
</li>
<li>
<p>$$
\theta_k^{(j)}:= \theta_k^{(j)} - \alpha
\left(
\sum_{i:r(i,j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right) x_k^{(i)} + \lambda \theta^{(j)}_k \right) \qquad \text{for $k \ne 0$}
$$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="32-collaborative-filtering">32 Collaborative Filtering<a hidden class="anchor" aria-hidden="true" href="#32-collaborative-filtering">#</a></h2>
<h4 id="321-collaborative-filtering">32.1 Collaborative Filtering<a hidden class="anchor" aria-hidden="true" href="#321-collaborative-filtering">#</a></h4>
<ul>
<li>
<p>The collaborative filtering algorithm has a very interesting property - does feature learning</p>
<ul>
<li>i.e. it can learn for itself what features it needs to learn</li>
</ul>
</li>
<li>
<p>Recall our original data set above for our five films and four raters</p>
<ul>
<li>Here we assume someone had calculated the &ldquo;romance&rdquo; and &ldquo;action&rdquo; amounts of the films
<ul>
<li>This can be very hard to do in reality</li>
<li>Often want more features than just two</li>
</ul>
</li>
</ul>
</li>
<li>
<p>So - let&rsquo;s change the problem and pretend we have a data set where we don&rsquo;t know any of the features associated with the films</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/oOjtVmChpAEandF.png" alt="image-20200624134457256"  />
</p>
<ul>
<li>Now let&rsquo;s make a different assumption
<ul>
<li>We&rsquo;ve polled each user and found out how much each user likes
<ul>
<li>Romantic films</li>
<li>Action films</li>
</ul>
</li>
<li>Which has generated the following parameter set</li>
<li>Alice and Bob like romance but hate action</li>
<li>Carol and Dave like action but hate romance</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If we can get these parameters from the users we can infer the missing values from our table</p>
<ul>
<li>Lets look at &ldquo;Love at Last&rdquo;
<ul>
<li>Alice and Bob loved it</li>
<li>Carol and Dave hated it</li>
</ul>
</li>
<li>We know from the feature vectors Alice and Bob love romantic films, while Carol and Dave hate them
<ul>
<li>Based on the factor Alice and Bob liked &ldquo;Love at Last&rdquo; and Carol and Dave hated it we may be able to (correctly) conclude that &ldquo;Love at Last&rdquo; is a romantic film</li>
</ul>
</li>
</ul>
</li>
<li>
<p>This is a bit of a simplification in terms of the math, but what we&rsquo;re really asking is</p>
<ul>
<li>What feature vector $x^{(1)}$ should be  - so that
<ul>
<li>$(\theta^{(1)})^T x_1$ is about 5</li>
<li>$(\theta^{(2)})^T x_1$ is about 5</li>
<li>$(\theta^{(3)})^T x_1$ is about 0</li>
<li>$(\theta^{(4)})^T x_1$ is about 0</li>
</ul>
</li>
<li>From this we can guess that $x^{(1)}$ may be $\begin{bmatrix} 1 &amp; 1.0 &amp; 0.0 \end{bmatrix}^T$</li>
<li>Using that same approach we should then be able to determine the remaining feature vectors for the other films</li>
</ul>
</li>
<li>
<p><strong>Formalizing the collaborative filtering problem</strong></p>
<ul>
<li>
<p>Given $\theta^{(1)}, \dots, \theta^{(n_u)}$ (i.e. given the parameter vectors for each users&rsquo; preferences), to learn $x^{(i)}$</p>
</li>
<li>
<p>We must minimize an optimization function which tries to identify the best parameter vector associated with a film</p>
<p>$$
\min_{x^{(i)}} \frac{1}{2} \sum_{j:r(i, j)=1)} \left(
(\theta^{(j)})^Tx^{(i)} - y^{(i,j)}
\right)^2 + \frac{\lambda}{2} \sum_{k=1}^{n} \left( x_k^{(i)} \right)^2
$$</p>
<ul>
<li>So we&rsquo;re summing over all the indices $j$ for where we have data for movie $i$</li>
<li>We&rsquo;re minimizing this squared error</li>
</ul>
</li>
<li>
<p>Like before, the above equation gives us a way to learn the features for one film</p>
<ul>
<li>We want to learn all the features for <em>all</em> the films - so we need an additional summation term</li>
</ul>
</li>
</ul>
</li>
<li>
<p>How does this work with the previous recommendation system</p>
<ul>
<li>Content based recommendation systems
<ul>
<li>Saw that if we have a set of features for movie rating you can learn a user&rsquo;s preferences</li>
</ul>
</li>
<li>Now
<ul>
<li>If you have your users preferences you can therefore determine a film&rsquo;s features</li>
</ul>
</li>
<li>This is a bit of a chicken &amp; egg problem</li>
<li>What you can do is
<ul>
<li>Randomly guess values for $\theta$</li>
<li>Then use collaborative filtering to generate $x$</li>
<li>Then use content based recommendation to improve $\theta$</li>
<li>Use that to improve $x$</li>
<li>And so on</li>
</ul>
</li>
<li>This actually works
<ul>
<li>Causes your algorithm to converge on a reasonable set of parameters</li>
<li>This is collaborative filtering</li>
</ul>
</li>
<li>We call it collaborative filtering because in this example the users are collaborating together to help the algorithm learn better features and help the system and the other users</li>
</ul>
</li>
</ul>
<h4 id="322-collaborative-filtering-algorithm">32.2 Collaborative Filtering Algorithm<a hidden class="anchor" aria-hidden="true" href="#322-collaborative-filtering-algorithm">#</a></h4>
<ul>
<li>
<p>Here we combine the ideas from before to build a collaborative filtering algorithm</p>
</li>
<li>
<p>Collaborative filtering optimization objective</p>
<ul>
<li>
<p>If we&rsquo;re given the film&rsquo;s features we can use that to work out the users&rsquo; preference:</p>
<p>$$
\text{Given $x^{(1)}, \dots, x^{(n_m)} $, estimate $ \theta^{(1)}, \dots, \theta^{(n_u)} $ :} \
\min_{\theta^{(1)},  \dots, \theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i, j)=1}\left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
$$</p>
</li>
</ul>
</li>
<li>
<p>If we&rsquo;re given the users&rsquo; preferences we can use them to work out the film&rsquo;s features</p>
<p>$$
\text{Given $ \theta^{(1)}, \dots, \theta^{(n_u)}  $, estimate $x^{(1)}, \dots, x^{(n_m)} $ :} \
\min_{x^{(1)}, \dots, x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i, j)=1}\left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n \left(x_k^{(i)} \right)^2
$$</p>
<ul>
<li>
<p>One thing you could do is</p>
<ul>
<li>Randomly initialize parameter</li>
</ul>
</li>
<li>
<p>Go back and forward</p>
</li>
</ul>
</li>
<li>
<p>But there&rsquo;s a more efficient algorithm which can solve $\theta$ and $x$ simultaneously</p>
<ul>
<li>Define a new optimization objective which is a function of $x$ and $\theta$</li>
</ul>
<p>$$
\text{Minimizing $ x^{(1)}, \dots, x^{(n_m)}$ and $\theta^{(1)}, \dots, \theta^{(n_u)} $ simultaneously :} \</p>
<p>\begin{aligned}
J(x^{(1)}, \dots, x^{(n_m)},\theta^{(1)}, \dots, \theta^{(n_u)}) &amp;= \frac{1}{2} \sum_{(i, j):r(i, j)=1} \left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2 + \
&amp;= \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n \left(x_k^{(i)} \right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
\end{aligned}
$$</p>
<ul>
<li>
<p>Goal:</p>
<p>$$
\min_{\mathop{x^{(1)}, \dots, x^{(n_m)}\\theta^{(1)}, \dots, \theta^{(n_u)}} }J(x^{(1)}, \dots, x^{(n_m)},\theta^{(1)}, \dots, \theta^{(n_u)})
$$</p>
</li>
</ul>
</li>
<li>
<p>Understanding this optimization objective</p>
<ul>
<li>
<p>The squared error term is the same as the squared error term in the two individual objectives above $\sum_{(i, j):r(i, j)=1} \left[\left( \theta^{(j)} \right)^T x^{(i)} - y^{(i, j)}  \right]^2$</p>
<ul>
<li>So it&rsquo;s summing over every movie rated by every users</li>
<li>Note the &ldquo;$:$&rdquo; means, &ldquo;for which&rdquo;
<ul>
<li>Sum over all pairs $(i,j)$ for which $r(i,j)$ is equal to 1</li>
</ul>
</li>
</ul>
</li>
<li>
<p>The regularization terms</p>
<ul>
<li>Are simply added to the end from the original two optimization functions</li>
</ul>
</li>
<li>
<p>This newly defined function has the property that</p>
<ul>
<li>If you held $x$ constant and only solved $\theta$ then you solve the, &ldquo;Given $x$, solve $\theta$&rdquo; objective above</li>
<li>Similarly, if you held $\theta$ constant you could solve $ x $</li>
</ul>
</li>
<li>
<p>In order to come up with just one optimization function we treat this function as a function of both film features $x$ and user parameters $\theta$</p>
<ul>
<li>Only difference between this in the back-and-forward approach is that we minimize with respect to both $x$ and $\theta$ simultaneously</li>
</ul>
</li>
<li>
<p>When we&rsquo;re learning the features this way</p>
<ul>
<li>
<p>Previously had a convention that we have an $x_0 = 1$ term</p>
</li>
<li>
<p>When we&rsquo;re using this kind of approach we have no</p>
<p>$x_0$,</p>
<ul>
<li>So now our vectors (both $x$ and $\theta$) are $n$-dimensional (NOT $n+1$)</li>
<li>i.e. $x \in \Bbb{R}^n, \theta \in \Bbb{R}^n$</li>
</ul>
</li>
<li>
<p>We do this because we are now learning all the features so if the system needs a feature always = 1 then the algorithm can learn one</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Collaborative filtering algorithm</strong></p>
<ul>
<li>
<ol>
<li>Initialize $\theta^{(1)}, \dots, \theta^{(n_u)}$ and $x^{(1)}, \dots, x^{(n_m)}$ to small random values</li>
</ol>
<ul>
<li>A bit like neural networks - initialize all parameters to small random numbers</li>
</ul>
</li>
<li>
<ol start="2">
<li>Minimize cost function $J(x^{(1)}, \dots, x^{(n_m)},\theta^{(1)}, \dots, \theta^{(n_u)})$ using gradient descent</li>
</ol>
<p>$$
\text{for every $j=1, \dots, n_u, i = 1, \dots, n_m $ :} \
x_k^{(i)}:= x_k^{(i)} - \alpha
\left(
\sum_{j:r(i,j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right) \theta^{(j)}<em>k + \lambda x_k^{(i)} \right)  \
\theta_k^{(j)}:= \theta_k^{(j)} - \alpha
\left(
\sum</em>{i:r(i,j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right) x_k^{(i)} + \lambda \theta^{(j)}_k \right)
$$</p>
</li>
<li>
<ol start="3">
<li>Having minimized the values, given a user (user $j$) with parameters $\theta$ and movie (movie $i$) with learned features $x$, we predict a star rating of $\theta^Tx$</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="33-low-rank-matrix-factorization">33 Low Rank Matrix Factorization<a hidden class="anchor" aria-hidden="true" href="#33-low-rank-matrix-factorization">#</a></h2>
<h4 id="331-vectorization-low-rank-matrix-factorization">33.1 Vectorization: Low Rank Matrix Factorization<a hidden class="anchor" aria-hidden="true" href="#331-vectorization-low-rank-matrix-factorization">#</a></h4>
<ul>
<li><strong>low rank matrix factorization</strong>
<ul>
<li>$Y = \begin{bmatrix}
\left(\theta^{(1)}\right)^{T}\left(x^{(1)}\right) &amp; \left(\theta^{(2)}\right)^{T}\left(x^{(1)}\right) &amp; \dots &amp; \left(\theta^{\left(n_{u}\right)}\right)^{T}\left(x^{(1)}\right) \newline
\left(\theta^{(1)}\right)^{T}\left(x^{(2)}\right) &amp; \left(\theta^{(2)}\right)^{T}\left(x^{(2)}\right) &amp; \dots &amp; \left(\theta^{\left(n_{u}\right)}\right)^{T}\left(x^{(2)}\right) \newline
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline
\left(\theta^{(1)}\right)^{T}\left(x^{\left(n_{m}\right)}\right) &amp; \left(\theta^{(2)}\right)^{T}\left(x^{\left(n_{m}\right)}\right) &amp; \dots &amp; \left(\theta^{\left(n_{u}\right)}\right)^{T}\left(x^{\left(n_{m}\right)}\right)
\end{bmatrix}$</li>
<li>$X = \begin{bmatrix}-\left( x^{(1)} \right)^T- \\ \cdots \\ -\left( x^{(n_m)} \right)^T- \end{bmatrix}, \Theta = \begin{bmatrix}-\left( \theta^{(1)} \right)^T- \\ \cdots \\ -\left( \theta^{(n_u)} \right)^T- \end{bmatrix}$</li>
<li>$Y = X\Theta^T$</li>
</ul>
</li>
<li>Find related movies
<ul>
<li>For each product $i$, we learn a feature vector $x^{(i)} \in \Bbb{R}^n$</li>
<li>How to find movie $j$ related to movie $i$ ?
<ul>
<li>$\text{small } |x^{(i)} - x^{(j)}| \rightarrow \text{movie $j$ and $i$ are &ldquo;similar&rdquo;}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="332-implementational-detail-mean-normalization">33.2 Implementational Detail: Mean Normalization<a hidden class="anchor" aria-hidden="true" href="#332-implementational-detail-mean-normalization">#</a></h4>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/1d7aykgZ5FmSvQK.png" alt="image-20200624150219443"  />
</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/24/hkVAcO4C3WeNJsT.png" alt="image-20200624151551949"  />
</p>
<h2 id="ex8-anomaly-detection-and-recommender-system">Ex8: Anomaly Detection and Recommender Systemüë®‚Äçüíª<a hidden class="anchor" aria-hidden="true" href="#ex8-anomaly-detection-and-recommender-system">#</a></h2>
<blockquote>
<p>See this exercise on <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python/blob/master/ex8/ex8.ipynb">Coursera-MachineLearning-Python/ex8/ex8.ipynb</a></p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://fang-lansheng.github.io/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://fang-lansheng.github.io/tags/python/">Python</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://fang-lansheng.github.io/posts/2020-06-25-ml-ng-10/">
    <span class="title">¬´ Prev Page</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà10Ôºâ</span>
  </a>
  <a class="next" href="https://fang-lansheng.github.io/posts/2020-06-22-ml-ng-8/">
    <span class="title">Next Page ¬ª</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ on twitter"
        href="https://twitter.com/intent/tweet/?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%889%ef%bc%89&amp;url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f&amp;hashtags=ML%2fDL%2cCourseLearning%2cPython">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f&amp;title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%889%ef%bc%89&amp;summary=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%889%ef%bc%89&amp;source=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%889%ef%bc%89">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ on whatsapp"
        href="https://api.whatsapp.com/send?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%889%ef%bc%89%20-%20https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ on telegram"
        href="https://telegram.me/share/url?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e5%90%b4%e6%81%a9%e8%be%be%ef%bc%9a%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%8f%8a%e6%80%bb%e7%bb%93%ef%bc%889%ef%bc%89&amp;url=https%3a%2f%2ffang-lansheng.github.io%2fposts%2f2020-06-24-ml-ng-9%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://fang-lansheng.github.io">Jifan&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
