<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>机器学习-吴恩达：学习笔记及总结（10） | Jifan&#39;s Blog</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link ：Week10 - Large Scale Machine Learning &amp; Week11 - Application Example: Photo OCR
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data
 However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data">
<meta name="author" content="jifan">
<link rel="canonical" href="https://jifan.tech/posts/2020-06-25-ml-ng-10/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://jifan.tech/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jifan.tech/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jifan.tech/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jifan.tech/apple-touch-icon.png">
<link rel="mask-icon" href="https://jifan.tech/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="机器学习-吴恩达：学习笔记及总结（10）" />
<meta property="og:description" content="Course Link ：Week10 - Large Scale Machine Learning &amp; Week11 - Application Example: Photo OCR
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data
 However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jifan.tech/posts/2020-06-25-ml-ng-10/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-25T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-06-25T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="机器学习-吴恩达：学习笔记及总结（10）"/>
<meta name="twitter:description" content="Course Link ：Week10 - Large Scale Machine Learning &amp; Week11 - Application Example: Photo OCR
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data
 However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://jifan.tech/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "机器学习-吴恩达：学习笔记及总结（10）",
      "item": "https://jifan.tech/posts/2020-06-25-ml-ng-10/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习-吴恩达：学习笔记及总结（10）",
  "name": "机器学习-吴恩达：学习笔记及总结（10）",
  "description": "Course Link ：Week10 - Large Scale Machine Learning \u0026amp; Week11 - Application Example: Photo OCR\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data\n However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link ：Week10 - Large Scale Machine Learning \u0026 Week11 - Application Example: Photo OCR\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data\n However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data\n But learning with large datasets comes with its own computational problems    Because of the computational cost of this massive summation, we’ll look at more efficient ways around this\n Either using a different approach Optimizing to avoid the summation    First thing to do is ask if we can train on 1000 examples instead of 100,000,000\n Randomly pick a small selection Can you develop a system which performs as well?  Sometimes yes - if this is the case you can avoid a lot of the headaches associated with big data      To see if taking a smaller sample works, you can sanity check by plotting error vs. training set size\n  If our plot looked like this\n  Looks like a high variance problem\n More examples should improve performance    If plot looked like this\n  This looks like a high bias problem\n More examples may not actually help - save a lot of time and effort if we know this before hand One natural thing to do here might be to:  Add extra features Add extra hidden units (if using neural networks)        34.2 Stochastic Gradient Descent   For many learning algorithms, we derived them by coming up with an optimization objective (cost function) and using an algorithm to minimize that cost function\n When you have a large dataset, gradient descent becomes very expensive So here we’ll define a different way to optimize for large data sets which will allow us to scale the algorithms    Suppose you’re training a linear regression model with gradient descent\n  We will use linear regression for our algorithmic example here when talking about stochastic gradient descent, although the ideas apply to other algorithms too, such as\n Logistic regression Neural networks    Below we have a contour plot for gradient descent showing iteration to a global minimum\n  As mentioned, if $m$ is large gradient descent can be very expensive\n  Although so far we just referred to it as gradient descent, this kind of gradient descent is called batch gradient descent\n This just means we look at all the examples at the same time    Batch gradient descent is not great for huge datasets\n If you have 300,000,000 records you need to read in all the records into memory from disk because you can’t store them all in memory  By reading all the records, you can move one step (iteration) through the algorithm   Then repeat for EVERY step  This means it take a LONG time to converge Especially because disk I/O is typically a system bottleneck anyway, and this will inevitably require a huge number of reads      What we’re going to do here is come up with a different algorithm which only needs to look at single example at a time\n    Stochastic gradient descent\n Define our cost function slightly differently, as $\\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)=\\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$  So the function represents the cost of $\\theta$ with respect to a specific example $(x^{(i)}, y^{(i)})$  And we calculate this value as one half times the squared error on that example   Measures how well the hypothesis works on a single example   The overall cost function can now be re-written in the following form: $J_{t r a i n}(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)$  This is equivalent to the batch gradient descent cost function   With this slightly modified (but equivalent) view of linear regression we can write out how stochastic gradient descent works So what’s going on here?  The term $(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ Is the same as that found in the summation for batch gradient descent It’s possible to show that this term is equal to the partial derivative with respect to the parameter $θ_j$ of the $\\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)$   What stochastic gradient descent algorithm is doing is scanning through each example  The inner for loop does something like this…  Looking at example 1, take a step with respect to the cost of just the 1st training example  Having done this, we go on to the second training example   Now take a second step in parameter space to try and fit the second training example better  Now move onto the third training example   And so on… Until it gets to the end of the data   We may now repeat this who procedure and take multiple passes over the data   The randomly shuffling at the start means we ensure the data is in a random order so we don’t bias the movement  Randomization should speed up convergence a little bit   Although stochastic gradient descent is a lot like batch gradient descent, rather than waiting to sum up the gradient terms over all $m$ examples, we take just one example and make progress in improving the parameters already  Means we update the parameters on EVERY step through data, instead of at the end of each loop through all the data      What does the algorithm do to the parameters?\n  As we saw, batch gradient descent does something like this to get to a global minimum\n  With stochastic gradient descent every iteration is much faster, but every iteration is flitting a single example\n What you find is that you “generally” move in the direction of the global minimum, but not always Never actually converges like batch gradient descent does, but ends up wandering around some region close to the global minimum  In practice, this isn’t a problem - as long as you’re close to the minimum that’s probably OK        One final implementation note\n May need to loop over the entire dataset 1-10 times If you have a truly massive dataset it’s possible that by the time you’ve taken a single pass through the dataset you may already have a perfectly good hypothesis  In which case the inner loop might only need to happen 1 if $m$ is very very large      If we contrast this to batch gradient descent\n We have to make $k$ passes through the entire dataset, where $k$ is the number of steps needed to move through the data    34.3 Mini-Batch Gradient Descent   Mini-batch gradient descent is an additional approach which can work even faster than stochastic gradient descent\n  To summarize our approaches so far\n Batch gradient descent: Use all $m$ examples in each iteration Stochastic gradient descent: Use 1 example in each iteration Mini-batch gradient descent: Use $b$ examples in each iteration  $b$ = mini-batch size (typical range for $b$ = 2-100 (10 maybe))      For example\n $b$ = 10 Get 10 examples from training set Perform gradient descent update using the ten examples    Mini-batch algorithm\n​\n We for-loop through $b$-size batches of $m$ Compared to batch gradient descent this allows us to get through data in a much more efficient way  After just $b$ examples we begin to improve our parameters Don’t have to update parameters after every example, and don’t have to wait until you cycled through all the data      Mini-batch gradient descent vs. stochastic gradient descent\n Why should we use mini-batch?  Allows you to have a vectorized implementation Means implementation is much more efficient Can partially parallelize your computation (i.e. do 10 at once)   A disadvantage of mini-batch gradient descent is the optimization of the parameter $b$  But this is often worth it!   To be honest, stochastic gradient descent and batch gradient descent are just specific forms of batch-gradient descent  For mini-batch gradient descent, $b$ is somewhere in between 1 and $m$ and you can try to optimize for it      34.4 Stochastic Gradient Descent Convergence  With batch gradient descent, we could plot cost function vs number of iterations  Should decrease on every iteration This works when the training set size was small because we could sum over all examples  Doesn’t work when you have a massive dataset     With stochastic gradient descent  We don’t want to have to pause the algorithm periodically to do a summation over all data Moreover, the whole point of stochastic gradient descent is to avoid those whole-data summations   For stochastic gradient descent, we have to do something different  Take cost function definition $\\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)=\\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$  One half the squared error on a single example   While the algorithm is looking at the example $\\left(x^{(i)}, y^{(i)}\\right)$, but before it has updated $\\theta$ we can compute the cost of the example $cost\\left(x^{(i)}, y^{(i)}\\right)$  i.e. we compute how well the hypothesis is working on the training example  Need to do this before we update $\\theta$ because if we did it after $\\theta$ was updated the algorithm would be performing a bit better (because we’d have just used $\\left(x^{(i)}, y^{(i)}\\right)$ to improve $\\theta$)     To check for the convergence, every 1000 iterations we can plot the costs averaged over the last 1000 examples  Gives a running estimate of how well we’ve done on the last 1000 estimates By looking at the plots we should be able to check convergence is happening        In general\n Might be a bit noisy (1000 examples isn’t that much)    If you get a figure like this\n That’s a pretty decent run Algorithm may have convergence    If you use a smaller learning rate you may get an even better final solution\n This is because the parameter oscillate around the global minimum A smaller learning rate means smaller oscillations    If you average over 1000 examples and 5000 examples you may get a smoother curve\n The disadvantage of a larger average means you get less frequent feedback    Sometimes you may get a plot that looks like this\n Looks like cost is not decreasing at all But if you then increase to averaging over a larger number of examples you do see this general trend  Means the blue line was too noisy, and that noise is ironed out by taking a greater number of entries per average   Of course, it may not decrease, even with a large number    If you see a curve the looks like its increasing then the algorithm may be displaying divergence\n Should use a smaller learning rate    Learning rate $\\alpha$ is typically held constant. Can slowly decrease over time if we want $\\theta$ to converge. (E.g. $\\alpha = \\frac{\\text{const1}}{\\text{iterationNumber}+\\text{const2}}$)\n  35 Advanced Topics 35.1 Online Learning   New setting\n Allows us to model problems where you have a continuous stream of data you want an algorithm to learn from Similar idea of stochastic gradient descent, in that you do slow updates Web companies use various types of online learning algorithms to learn from traffic  Can (for example) learn about user preferences and hence optimize your website      Example - Shipping service\n Users come and tell you origin and destination You offer to ship the package for some amount of money ($10 - $50) Based on the price you offer, sometimes the user uses your service ($y = 1$), sometimes they don’t ($y = 0$) Build an algorithm to optimize what price we offer to the users  Capture  Information about user Origin and destination   Work out  What the probability of a user selecting the service is   We want to optimize the price   To model this probability we have something like  $p(y=1|x;\\theta)$  Probability that $y =1$, given $x$, parameterized by $\\theta$   Build this model with something like  Logistic regression Neural network     If you have a website that runs continuously an online learning algorithm would do something like this  User comes - is represented as an $(x,y)$ pair where  $x$ - feature vector including price we offer, origin, destination $y$ - if they chose to use our service or not   The algorithm updates $\\theta$ using just the $(x,y)$ pair: $\\theta_j := \\theta_j - \\alpha \\left(h_{\\theta}(x) - y \\right)x_j \\quad (j=0, \\dots, n)$ So we basically update all the $\\theta$ parameters every time we get some new data   While in previous examples we might have described the data example as $(x^{(i)}, y^{(i)})$ for an online learning problem we discard this idea of a data “set” - instead we have a continuous stream of data so indexing is largely irrelevant as you’re not storing the data (although presumably you could store it)    If you have a major website where you have a massive stream of data then this kind of algorithm is pretty reasonable\n You’re free of the need to deal with all your training data    If you had a small number of users you could save their data and then run a normal algorithm on a dataset\n  An online algorithm can adapt to changing user preferences\n So over time users may become more price sensitive The algorithm adapts and learns to this So your system is dynamic    Another example - product search\n  Other things you can do\n Special offers to show the user Show news articles - learn what users like Product recommendation    These problems could have been formulated using standard techniques, but they are the kinds of problems where you have so much data that this is a better way to do things\n  35.2 Map Reduce and Data Parallelism   More generally map reduce uses the following scheme (e.g. where you split into 4)\n  The bulk of the work in gradient descent is the summation\n Now, because each of the computers does a quarter of the work at the same time, you get a $4\\times$ speedup Of course, in practice, because of network latency, combining results, it’s slightly less than $4\\times$, but still good!    Important thing to ask is: “Can algorithm be expressed as computing sums of functions of the training set?”\n Many algorithms can    More broadly, by taking algorithms which compute sums you can scale them to very large datasets through parallelization\n Parallelization can come from  Multiple machines Multiple CPUs Multiple cores in each CPU   So even on a single compute can implement parallelization    The advantage of thinking about Map Reduce here is because you don’t need to worry about network issues\n It’s all internal to the same machine    Finally caveat/thought\n Depending on implementation detail, certain numerical linear algebra libraries can automatically parallelize your calculations across multiple cores So, if this is the case and you have a good vectorization implementation you can not worry about local Parallelization and the local libraries sort optimization out for you    36 Photo OCR 36.1 Problem Description and Pipeline   Case study focused around Photo OCR\n  Three reasons to do this\n   Look at how a complex system can be put together    The idea of a machine learning pipeline   What to do next How to do it    Some more interesting ideas   Applying machine learning to tangible problems Artificial data synthesis      What is the photo OCR problem?\n Photo OCR = Photo Optical Character Recognition  With growth of digital photography, lots of digital pictures One idea which has interested many people is getting computers to understand those photos The photo OCR problem is getting computers to read text in an image  Possible applications for this would include  Make searching easier (e.g. searching for photos based on words in them) Car navigation       OCR of documents is a comparatively easy problem  From photos it’s really hard      OCR pipeline\n   Look through image and find text    Do character segmentation    Do character classification    some may do spell check after this too (Optional)   We’re not focusing on such systems though      Pipelines are common in machine learning\n Separate modules which may each be a machine learning component or data processing component    If you’re designing a machine learning system, pipeline design is one of the most important questions\n Performance of pipeline and each module often has a big impact on the overall performance a problem You would often have different engineers working on each module  Offers a natural way to divide up the workload      36.2 Sliding Windows   As mentioned, stage 1 is text detection\n Unusual problem in computer vision - different rectangles (which surround text) may have different aspect ratios (aspect ratio being height : width)  Text may be short (few words) or long (many words) Tall or short font Text might be straight on Slanted      Pedestrian detection\n  Want to take an image and find pedestrians in the image\n  This is a slightly simpler problem because the aspect ration remains pretty constant\n  Building our detection system\n  Have $82 \\times 36$ aspect ratio\n This is a typical aspect ratio for a standing human    Collect training set of positive and negative examples\n  Could have 1000 - 10 000 training examples\n  Train a neural network to take an image and classify that image as pedestrian or not\n Gives you a way to train your system      Now we have a new image - how do we find pedestrians in it?\n  Start by taking a rectangular $82 \\times 36$ patch in the image\n Run patch through classifier - hopefully in this example it will return $y = 0$    Next slide the rectangle over to the right a little bit and re-run\n Then slide again The amount you slide each rectangle over is a parameter called the step-size or stride  Could use 1 pixel  Best, but computationally expensive   More commonly 5-8 pixels used   So, keep stepping rectangle along all the way to the right  Eventually get to the end   Then move back to the left hand side but step down a bit too Repeat until you’ve covered the whole image    Now, we initially started with quite a small rectangle\n So now we can take a larger image patch (of the same aspect ratio) Each time we process the image patch, we’re resizing the larger patch to a smaller image, then running that smaller image through the classifier    Hopefully, by changing the patch size and rastering repeatedly across the image, you eventually recognize all the pedestrians in the picture\n      Text detection example\n  Like pedestrian detection, we generate a labeled training set with\n Positive examples (some kind of text) Negative examples (not text)    Having trained the classifier we apply it to an image\n So, run a sliding window classifier at a fixed rectangle size If you do that end up with something like this    White region show where text detection system thinks text is\n Different shades of gray correspond to probability associated with how sure the classifier is the section contains text  Black - no text White - text   For text detection, we want to draw rectangles around all the regions where there is text in the image    Take classifier output and apply an expansion algorithm\n  Takes each of white regions and expands it\n  How do we implement this\n Say, for every pixel, is it within some distance of a white pixel? If yes then color it white      Look at connected white regions in the image above\n  Draw rectangles around those which make sense as text (i.e. tall thin boxes don’t make sense)\n    This example misses a piece of text on the door because the aspect ratio is wrong\n Very hard to read        Stage 2: character segmentation\n  Use supervised learning algorithm\n  Look in a defined image patch and decide, is there a split between two characters?\n So, for example, our first training data item below looks like there is such a split Similarly, the negative examples are either empty or hold a full characters    We train a classifier to try and classify between positive and negative examples\n Run that classifier on the regions detected as containing text in the previous section    Use a 1-dimensional sliding window to move along text regions\n Does each window snapshot look like the split between two characters?  If yes insert a split If not move on   So we have something that looks like this      Character classification\n Standard OCR, where you apply standard supervised learning which takes an input and identify which character we decide it is  Multi-class characterization problem      36.3 Getting Lots of Data and Artificial Data   We’ve seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias algorithm and train on a massive data set\n Where do we get so much data from In ML artifice data synthesis  Doesn’t apply to every problem If it applies to your problem can be a great way to generate loads of data      Two main principles\n   Creating data from scratch    If we already have a small labeled training set can we amplify it into a larger training set      Character recognition as an example of data synthesis\n  If we go and collect a large labeled data set will look like this\n Goal is to take an image patch and have the system recognize the character Treat the images as gray-scale (makes it a bit easer)    How can we amplify this\n Modern computers often have a big font library If you go to websites, huge free font libraries For more training data, take characters from different fonts, paste these characters again random backgrounds    After some work, can build a synthetic training set\n Random background Maybe some blurring/distortion filters Takes thought and work to make it look realistic  If you do a sloppy job this won’t help! So unlimited supply of training examples   This is an example of creating new data from scratch    Other way is to introduce distortion into existing data\n  e.g. take a character and warp it\n 16 new examples Allows you amplify existing training set    This, again, takes though and insight in terms of deciding how to amplify\n      Another example: speech recognition\n Learn from audio clip - what were the words  Have a labeled training example Introduce audio distortions into the examples   So only took one example  Created lots of new ones!   When introducing distortion, they should be reasonable relative to the issues your classifier may encounter    Getting more data\n Before creating new data, make sure you have a low bias classifier  Plot learning curve   If not a low bias classifier increase number of features  Then create large artificial training set   Very important question: How much work would it be to get $10\\times$ data as we currently have?  Often the answer is, “Not that hard” This is often a huge way to improve an algorithm Good question to ask yourself or ask the team   How many minutes/hours does it take to get a certain number of examples  Say we have 1000 examples 10 seconds to label an example So we need another 9000 - 90000 seconds Comes to a few days (25 hours!)   Crowd sourcing is also a good way to get data  Risk or reliability issues Cost Example: Amazon Mechanical Turk (MTurk)      36.4 Celling Analysis: What Part of the Pipeline to Work on Next   Through the course repeatedly said one of the most valuable resources is developer time\n Pick the right thing for you and your team to work on Avoid spending a lot of time to realize the work was pointless in terms of enhancing performance    Photo OCR pipeline\n Three modules  Each one could have a small team on it Where should you allocate resources?   Good to have a single real number as an evaluation metric  So, character accuracy for this example Find that our test set has 72% accuracy      Ceiling analysis on our pipeline\n We go to the first module  Mess around with the test set - manually tell the algorithm where the text is Simulate if your text detection system was 100% accurate  So we’re feeding the character segmentation module with 100% accurate data now   How does this change the accuracy of the overall system Accuracy goes up to 89%   Next do the same for the character segmentation  Accuracy goes up to 90% now   Finally doe the same for character recognition  Goes up to 100%   Having done this we can qualitatively show what the upside to improving each module would be  Perfect text detection improves accuracy by 17%!  Would bring the biggest gain if we could improve   Perfect character segmentation would improve it by 1%  Not worth working on   Perfect character recognition would improve it by 10%  Might be worth working on, depends if it looks easy or not     The “ceiling” is that each module has a ceiling by which making it perfect would improve the system overall    Another example - face recognition\n  This is not how it’s done in practice\n  How would you do ceiling analysis for this\n     Conclusion  Supervised Learning  Linear regression, logistic regression, neural networks, SVMs   Unsupervised Learning  K-means, PCA, Anomaly detection   Special applications/special topics  Recommender systems, large scale machine learning   Advice on building a machine learning system  Bias/variance, regularization; deciding what to work on next: evaluation of learning algorithms, learning curves, error analysis, ceiling analysis   AND A BIG THANK YOU!  ",
  "wordCount" : "4108",
  "inLanguage": "en",
  "datePublished": "2020-06-25T00:00:00Z",
  "dateModified": "2020-06-25T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "jifan"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jifan.tech/posts/2020-06-25-ml-ng-10/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jifan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jifan.tech/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jifan.tech" accesskey="h" title="Jifan&#39;s Blog (Alt + H)">Jifan&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jifan.tech/archives/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://jifan.tech/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://jifan.tech/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      机器学习-吴恩达：学习笔记及总结（10）
    </h1>
    <div class="post-meta"><span title='2020-06-25 00:00:00 +0000 UTC'>June 25, 2020</span>&nbsp;·&nbsp;jifan

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#34-gradient-descent-with-large-datasets" aria-label="34 Gradient Descent with Large Datasets">34 Gradient Descent with Large Datasets</a><ul>
                        <ul>
                        
                <li>
                    <a href="#341-learning-with-large-datasets" aria-label="34.1 Learning With Large Datasets">34.1 Learning With Large Datasets</a></li>
                <li>
                    <a href="#342-stochastic-gradient-descent" aria-label="34.2 Stochastic Gradient Descent">34.2 Stochastic Gradient Descent</a></li>
                <li>
                    <a href="#343-mini-batch-gradient-descent" aria-label="34.3 Mini-Batch Gradient Descent">34.3 Mini-Batch Gradient Descent</a></li>
                <li>
                    <a href="#344-stochastic-gradient-descent-convergence" aria-label="34.4 Stochastic Gradient Descent Convergence">34.4 Stochastic Gradient Descent Convergence</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#35-advanced-topics" aria-label="35 Advanced Topics">35 Advanced Topics</a><ul>
                        <ul>
                        
                <li>
                    <a href="#351-online-learning" aria-label="35.1 Online Learning">35.1 Online Learning</a></li>
                <li>
                    <a href="#352-map-reduce-and-data-parallelism" aria-label="35.2 Map Reduce and Data Parallelism">35.2 Map Reduce and Data Parallelism</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#36-photo-ocr" aria-label="36 Photo OCR">36 Photo OCR</a><ul>
                        <ul>
                        
                <li>
                    <a href="#361-problem-description-and-pipeline" aria-label="36.1 Problem Description and Pipeline">36.1 Problem Description and Pipeline</a></li>
                <li>
                    <a href="#362-sliding-windows" aria-label="36.2 Sliding Windows">36.2 Sliding Windows</a></li>
                <li>
                    <a href="#363-getting-lots-of-data-and-artificial-data" aria-label="36.3 Getting Lots of Data and Artificial Data">36.3 Getting Lots of Data and Artificial Data</a></li>
                <li>
                    <a href="#364-celling-analysis-what-part-of-the-pipeline-to-work-on-next" aria-label="36.4 Celling Analysis: What Part of the Pipeline to Work on Next">36.4 Celling Analysis: What Part of the Pipeline to Work on Next</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Course Link ：<a href="https://www.coursera.org/learn/machine-learning/home/week/10">Week10 - Large Scale Machine Learning</a> &amp; <a href="https://www.coursera.org/learn/machine-learning/home/week/11">Week11 - Application Example: Photo OCR</a></p>
<p>Code : <a href="https://github.com/Fang-Lansheng/Coursera-MachineLearning-Python">Fang-Lansheng/Coursera-MachineLearning-Python</a></p>
</blockquote>
<h2 id="34-gradient-descent-with-large-datasets">34 Gradient Descent with Large Datasets<a hidden class="anchor" aria-hidden="true" href="#34-gradient-descent-with-large-datasets">#</a></h2>
<h4 id="341-learning-with-large-datasets">34.1 Learning With Large Datasets<a hidden class="anchor" aria-hidden="true" href="#341-learning-with-large-datasets">#</a></h4>
<ul>
<li>
<p>If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data</p>
<ul>
<li>However, with this increase in data comes great responsibility? No, comes a much more significant computational cost</li>
<li>New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level</li>
</ul>
</li>
<li>
<p>One of best ways to get high performance is take a low bias algorithm and train it on a lot of data</p>
<ul>
<li>But learning with large datasets comes with its own computational problems</li>
</ul>
</li>
<li>
<p>Because of the computational cost of this massive summation, we&rsquo;ll look at more efficient ways around this</p>
<ul>
<li>Either using a different approach</li>
<li>Optimizing to avoid the summation</li>
</ul>
</li>
<li>
<p>First thing to do is ask if we can train on 1000 examples instead of 100,000,000</p>
<ul>
<li>Randomly pick a small selection</li>
<li>Can you develop a system which performs as well?
<ul>
<li>Sometimes yes - if this is the case you can avoid a lot of the headaches associated with big data</li>
</ul>
</li>
</ul>
</li>
<li>
<p>To see if taking a smaller sample works, you can sanity check by plotting error vs. training set size</p>
<ul>
<li>
<p>If our plot looked like this</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/8iPEDhsVRdlIKT9.png" alt="image-20200625092247264"  />
</p>
</li>
<li>
<p>Looks like a <strong>high variance problem</strong></p>
<ul>
<li>More examples should improve performance</li>
</ul>
</li>
<li>
<p>If plot looked like this</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/a3Y2c6CnQmZMFwr.png" alt="image-20200625092329037"  />
</p>
</li>
<li>
<p>This looks like a <strong>high bias problem</strong></p>
<ul>
<li>More examples may not actually help - save a lot of time and effort if we know this <em>before hand</em></li>
<li>One natural thing to do here might be to:
<ul>
<li>Add extra features</li>
<li>Add extra hidden units (if using neural networks)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="342-stochastic-gradient-descent">34.2 Stochastic Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#342-stochastic-gradient-descent">#</a></h4>
<ul>
<li>
<p>For many learning algorithms, we derived them by coming up with an optimization objective (cost function) and using an algorithm to minimize that cost function</p>
<ul>
<li>When you have a large dataset, gradient descent becomes very expensive</li>
<li>So here we&rsquo;ll define a different way to optimize for large data sets which will allow us to scale the algorithms</li>
</ul>
</li>
<li>
<p>Suppose you&rsquo;re training a linear regression model with gradient descent</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/yhEfJP1q8gbir75.png" alt="image-20200625094716711"  />
</p>
<ul>
<li>
<p>We will use linear regression for our algorithmic example here when talking about <strong>stochastic gradient descent</strong>, although the ideas apply to other algorithms too, such as</p>
<ul>
<li>Logistic regression</li>
<li>Neural networks</li>
</ul>
</li>
<li>
<p>Below we have a contour plot for gradient descent showing iteration to a global minimum</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/vSInYrAR2p7ElPc.png" alt="image-20200625094849106"  />
</p>
</li>
<li>
<p>As mentioned, if $m$ is large gradient descent can be very expensive</p>
</li>
<li>
<p>Although so far we just referred to it as gradient descent, this kind of gradient descent is called <strong>batch gradient descent</strong></p>
<ul>
<li>This just means we look at all the examples at the same time</li>
</ul>
</li>
<li>
<p>Batch gradient descent is not great for huge datasets</p>
<ul>
<li>If you have 300,000,000 records you need to read in all the records into memory from disk because you can&rsquo;t store them all in memory
<ul>
<li>By reading all the records, you can move one step (iteration) through the algorithm</li>
</ul>
</li>
<li>Then repeat for EVERY step
<ul>
<li>This means it take a LONG time to converge</li>
<li>Especially because disk I/O is typically a system bottleneck anyway, and this will inevitably require a <em>huge</em> number of reads</li>
</ul>
</li>
</ul>
</li>
<li>
<p>What we&rsquo;re going to do here is come up with a different algorithm which only needs to look at single example at a time</p>
</li>
</ul>
</li>
<li>
<p><strong>Stochastic gradient descent</strong></p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/ZYbuRNvXlA7n4ch.png" alt="image-20200625095354418"  />
</p>
<ul>
<li>Define our cost function slightly differently, as $\operatorname{cost}\left(\theta,\left(x^{(i)}, y^{(i)}\right)\right)=\frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$
<ul>
<li>So the function represents the cost of $\theta$ with respect to a specific example $(x^{(i)}, y^{(i)})$
<ul>
<li>And we calculate this value as one half times the squared error on that example</li>
</ul>
</li>
<li>Measures how well the hypothesis works on a single example</li>
</ul>
</li>
<li>The overall cost function can now be re-written in the following form: $J_{t r a i n}(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(\theta,\left(x^{(i)}, y^{(i)}\right)\right)$
<ul>
<li>This is equivalent to the batch gradient descent cost function</li>
</ul>
</li>
<li>With this slightly modified (but equivalent) view of linear regression we can write out how stochastic gradient descent works</li>
<li><em>So what&rsquo;s going on here?</em>
<ul>
<li>The term $(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$</li>
<li>Is the same as that found in the summation for batch gradient descent</li>
<li>It&rsquo;s possible to show that this term is equal to the partial derivative with respect to the parameter $θ_j$ of the $\operatorname{cost}\left(\theta,\left(x^{(i)}, y^{(i)}\right)\right)$</li>
</ul>
</li>
<li>What stochastic gradient descent algorithm is doing is scanning through each example
<ul>
<li>The inner for loop does something like this&hellip;
<ul>
<li>Looking at example 1, take a step with respect to the cost of just the 1st training example
<ul>
<li>Having done this, we go on to the second training example</li>
</ul>
</li>
<li>Now take a second step in parameter space to try and fit the second training example better
<ul>
<li>Now move onto the third training example</li>
</ul>
</li>
<li>And so on&hellip;</li>
<li>Until it gets to the end of the data</li>
</ul>
</li>
<li>We may now repeat this who procedure and take multiple passes over the data</li>
</ul>
</li>
<li>The <strong>randomly shuffling</strong> at the start means we ensure the data is in a random order so we don&rsquo;t bias the movement
<ul>
<li>Randomization should speed up convergence a little bit</li>
</ul>
</li>
<li>Although stochastic gradient descent is a lot like batch gradient descent, rather than waiting to sum up the gradient terms over all $m$ examples, we take just one example and make progress in improving the parameters already
<ul>
<li>Means we update the parameters on EVERY step through data, instead of at the end of each loop through all the data</li>
</ul>
</li>
</ul>
</li>
<li>
<p>What does the algorithm do to the parameters?</p>
<ul>
<li>
<p>As we saw, batch gradient descent does something like this to get to a global minimum</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/h5kmVXFu6Uf1RcS.png" alt="image-20200625095902737"  />
</p>
</li>
<li>
<p>With stochastic gradient descent every iteration is much faster, but every iteration is flitting a single example</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/jAGPDgMLuKJ7yZz.png" alt="image-20200625095918157"  />
</p>
<ul>
<li>What you find is that you &ldquo;generally&rdquo; move in the direction of the global minimum, but not always</li>
<li>Never actually converges like batch gradient descent does, but ends up wandering around some region close to the global minimum
<ul>
<li>In practice, this isn&rsquo;t a problem - as long as you&rsquo;re close to the minimum that&rsquo;s probably OK</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>One final implementation note</p>
<ul>
<li>May need to loop over the entire dataset 1-10 times</li>
<li>If you have a truly massive dataset it&rsquo;s possible that by the time you&rsquo;ve taken a single pass through the dataset you may already have a perfectly good hypothesis
<ul>
<li>In which case the inner loop might only need to happen 1 if $m$ is very very large</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If we contrast this to batch gradient descent</p>
<ul>
<li>We have to make $k$ passes through the entire dataset, where $k$ is the number of steps needed to move through the data</li>
</ul>
</li>
</ul>
<h4 id="343-mini-batch-gradient-descent">34.3 Mini-Batch Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#343-mini-batch-gradient-descent">#</a></h4>
<ul>
<li>
<p><strong>Mini-batch gradient descent</strong> is an additional approach which can work even faster than stochastic gradient descent</p>
</li>
<li>
<p>To summarize our approaches so far</p>
<ul>
<li>Batch gradient descent: Use all $m$ examples in each iteration</li>
<li>Stochastic gradient descent: Use 1 example in each iteration</li>
<li>Mini-batch gradient descent: Use $b$ examples in each iteration
<ul>
<li>$b$ = mini-batch size (typical range for $b$ = 2-100 (10 maybe))</li>
</ul>
</li>
</ul>
</li>
<li>
<p>For example</p>
<ul>
<li>$b$ = 10</li>
<li>Get 10 examples from training set</li>
<li>Perform gradient descent update using the ten examples</li>
</ul>
</li>
<li>
<p><strong>Mini-batch algorithm</strong></p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/glj6hU2MGXudKDt.png" alt="image-20200625100632298"  />
</p>
<p>​</p>
<ul>
<li>We for-loop through $b$-size batches of $m$</li>
<li>Compared to batch gradient descent this allows us to get through data in a much more efficient way
<ul>
<li>After just $b$ examples we begin to improve our parameters</li>
<li>Don&rsquo;t have to update parameters after <em>every</em> example, and don&rsquo;t have to wait until you cycled through all the data</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Mini-batch gradient descent vs. stochastic gradient descent</strong></p>
<ul>
<li>Why should we use mini-batch?
<ul>
<li>Allows you to have a vectorized implementation</li>
<li>Means implementation is much more efficient</li>
<li>Can partially parallelize your computation (i.e. do 10 at once)</li>
</ul>
</li>
<li>A disadvantage of mini-batch gradient descent is the optimization of the parameter $b$
<ul>
<li>But this is often worth it!</li>
</ul>
</li>
<li>To be honest, stochastic gradient descent and batch gradient descent are just specific forms of batch-gradient descent
<ul>
<li>For mini-batch gradient descent, $b$ is somewhere in between 1 and $m$ and you can try to optimize for it</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="344-stochastic-gradient-descent-convergence">34.4 Stochastic Gradient Descent Convergence<a hidden class="anchor" aria-hidden="true" href="#344-stochastic-gradient-descent-convergence">#</a></h4>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/7ThSJ4iftFs6Mby.png" alt="image-20200625101714542"  />
</p>
<ul>
<li>With batch gradient descent, we could plot cost function vs number of iterations
<ul>
<li>Should decrease on every iteration</li>
<li>This works when the training set size was small because we could sum over all examples
<ul>
<li>Doesn&rsquo;t work when you have a massive dataset</li>
</ul>
</li>
</ul>
</li>
<li>With stochastic gradient descent
<ul>
<li>We don&rsquo;t want to have to pause the algorithm periodically to do a summation over all data</li>
<li>Moreover, the whole point of stochastic gradient descent is to <em>avoid</em> those whole-data summations</li>
</ul>
</li>
<li>For stochastic gradient descent, we have to do something different
<ul>
<li>Take cost function definition $\operatorname{cost}\left(\theta,\left(x^{(i)}, y^{(i)}\right)\right)=\frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$
<ul>
<li>One half the squared error on a single example</li>
</ul>
</li>
<li>While the algorithm is looking at the example $\left(x^{(i)}, y^{(i)}\right)$, but <em>before</em> it has updated $\theta$ we can compute the cost of the example $cost\left(x^{(i)}, y^{(i)}\right)$
<ul>
<li>i.e. we compute how well the hypothesis is working on the training example
<ul>
<li>Need to do this before we update $\theta$ because if we did it after $\theta$ was updated the algorithm would be performing a bit better (because we&rsquo;d have just used $\left(x^{(i)}, y^{(i)}\right)$ to improve $\theta$)</li>
</ul>
</li>
</ul>
</li>
<li>To check for the convergence, every 1000 iterations we can plot the costs averaged over the last 1000 examples
<ul>
<li>Gives a running estimate of how well we&rsquo;ve done on the last 1000 estimates</li>
<li>By looking at the plots we should be able to check convergence is happening</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/4jl9yN26uvdQxRK.png" alt="image-20200625102127260"  />
</p>
<ul>
<li>
<p>In general</p>
<ul>
<li>Might be a bit noisy (1000 examples isn&rsquo;t that much)</li>
</ul>
</li>
<li>
<p>If you get a figure like this</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/klQ6GKXWNeD8yiV.png" alt="image-20200625102202969"  />
</p>
<ul>
<li>That&rsquo;s a pretty decent run</li>
<li>Algorithm may have convergence</li>
</ul>
</li>
<li>
<p>If you use a smaller learning rate you may get an even better final solution</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/06/25/N0tLh4.png" alt="N0tLh4.png"  />
</p>
<ul>
<li>This is because the parameter oscillate around the global minimum</li>
<li>A smaller learning rate means smaller oscillations</li>
</ul>
</li>
<li>
<p>If you average over 1000 examples and 5000 examples you may get a smoother curve</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/SoFE1qB8Hmn3gN4.png" alt="image-20200625102409053"  />
</p>
<ul>
<li>The disadvantage of a larger average means you get less frequent feedback</li>
</ul>
</li>
<li>
<p>Sometimes you may get a plot that looks like this</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/mU4TkPljF87cRKY.png" alt="image-20200625102500168"  />
</p>
<ul>
<li>Looks like cost is not decreasing at all</li>
<li>But if you then increase to averaging over a larger number of examples you do see this general trend
<ul>
<li>Means the blue line was too noisy, and that noise is ironed out by taking a greater number of entries per average</li>
</ul>
</li>
<li>Of course, it may not decrease, even with a large number</li>
</ul>
</li>
<li>
<p>If you see a curve the looks like its increasing then the algorithm may be displaying divergence</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/1wqzOb9spSc2Fti.png" alt="image-20200625102612621"  />
</p>
<ul>
<li>Should use a smaller learning rate</li>
</ul>
</li>
<li>
<p>Learning rate $\alpha$ is typically held constant. Can slowly decrease over time if we want $\theta$ to converge. (E.g. $\alpha = \frac{\text{const1}}{\text{iterationNumber}+\text{const2}}$)</p>
</li>
</ul>
<h2 id="35-advanced-topics">35 Advanced Topics<a hidden class="anchor" aria-hidden="true" href="#35-advanced-topics">#</a></h2>
<h4 id="351-online-learning">35.1 Online Learning<a hidden class="anchor" aria-hidden="true" href="#351-online-learning">#</a></h4>
<ul>
<li>
<p>New setting</p>
<ul>
<li>Allows us to model problems where you have a continuous stream of data you want an algorithm to learn from</li>
<li>Similar idea of stochastic gradient descent, in that you do slow updates</li>
<li>Web companies use various types of online learning algorithms to learn from traffic
<ul>
<li>Can (for example) learn about user preferences and hence optimize your website</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Example - Shipping service</p>
<ul>
<li>Users come and tell you origin and destination</li>
<li>You offer to ship the package for some amount of money ($10 - $50)</li>
<li>Based on the price you offer, sometimes the user uses your service ($y = 1$), sometimes they don&rsquo;t ($y = 0$)</li>
<li>Build an algorithm to optimize what price we offer to the users
<ul>
<li>Capture
<ul>
<li>Information about user</li>
<li>Origin and destination</li>
</ul>
</li>
<li>Work out
<ul>
<li>What the probability of a user selecting the service is</li>
</ul>
</li>
<li>We want to optimize the price</li>
</ul>
</li>
<li>To model this probability we have something like
<ul>
<li>$p(y=1|x;\theta)$
<ul>
<li>Probability that $y =1$, given $x$, parameterized by $\theta$</li>
</ul>
</li>
<li>Build this model with something like
<ul>
<li>Logistic regression</li>
<li>Neural network</li>
</ul>
</li>
</ul>
</li>
<li>If you have a website that runs continuously an online learning algorithm would do something like this
<ul>
<li>User comes - is represented as an $(x,y)$ pair where
<ul>
<li>$x$ - feature vector including price we offer, origin, destination</li>
<li>$y$ - if they chose to use our service or not</li>
</ul>
</li>
<li>The algorithm updates $\theta$ using just the $(x,y)$ pair: $\theta_j := \theta_j - \alpha \left(h_{\theta}(x) - y \right)x_j \quad (j=0, \dots, n)$</li>
<li>So we basically update all the $\theta$ parameters every time we get some new data</li>
</ul>
</li>
<li>While in previous examples we might have described the data example as $(x^{(i)}, y^{(i)})$ for an online learning problem we discard this idea of a data &ldquo;set&rdquo; - instead we have a continuous stream of data so indexing is largely irrelevant as you&rsquo;re not storing the data (although presumably you could store it)</li>
</ul>
</li>
<li>
<p>If you have a major website where you have a massive stream of data then this kind of algorithm is pretty reasonable</p>
<ul>
<li>You&rsquo;re free of the need to deal with all your training data</li>
</ul>
</li>
<li>
<p>If you had a small number of users you could save their data and then run a normal algorithm on a dataset</p>
</li>
<li>
<p>An online algorithm can adapt to changing user preferences</p>
<ul>
<li>So over time users may become more price sensitive</li>
<li>The algorithm adapts and learns to this</li>
<li>So your system is dynamic</li>
</ul>
</li>
<li>
<p>Another example - product search</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/PaYuTR1qAwfjU9d.png" alt="image-20200625104932154"  />
</p>
</li>
<li>
<p>Other things you can do</p>
<ul>
<li>Special offers to show the user</li>
<li>Show news articles - learn what users like</li>
<li>Product recommendation</li>
</ul>
</li>
<li>
<p>These problems could have been formulated using standard techniques, but they are the kinds of problems where you have so much data that this is a better way to do things</p>
</li>
</ul>
<h4 id="352-map-reduce-and-data-parallelism">35.2 Map Reduce and Data Parallelism<a hidden class="anchor" aria-hidden="true" href="#352-map-reduce-and-data-parallelism">#</a></h4>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/emng8K2LZfbGMEs.png" alt="image-20200625105404862"  />
</p>
<ul>
<li>
<p>More generally map reduce uses the following scheme (e.g. where you split into 4)</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/06/25/N0Byxe.png" alt="N0Byxe.png"  />
</p>
</li>
<li>
<p>The bulk of the work in gradient descent is the summation</p>
<ul>
<li>Now, because each of the computers does a quarter of the work at the same time, you get a $4\times$ speedup</li>
<li>Of course, in practice, because of network latency, combining results, it&rsquo;s slightly less than $4\times$, but still good!</li>
</ul>
</li>
<li>
<p>Important thing to ask is: &ldquo;Can algorithm be expressed as computing sums of functions of the training set?&rdquo;</p>
<ul>
<li>Many algorithms can</li>
</ul>
</li>
<li>
<p>More broadly, by taking algorithms which compute sums you can scale them to very large datasets through parallelization</p>
<ul>
<li>Parallelization can come from
<ul>
<li>Multiple machines</li>
<li>Multiple CPUs</li>
<li>Multiple cores in each CPU</li>
</ul>
</li>
<li>So even on a single compute can implement parallelization</li>
</ul>
</li>
<li>
<p>The advantage of thinking about Map Reduce here is because you don&rsquo;t need to worry about network issues</p>
<ul>
<li>It&rsquo;s all internal to the same machine</li>
</ul>
</li>
<li>
<p>Finally caveat/thought</p>
<ul>
<li>Depending on implementation detail, certain numerical linear algebra libraries can automatically parallelize your calculations across multiple cores</li>
<li>So, if this is the case and you have a good vectorization implementation you can not worry about local Parallelization and the local libraries sort optimization out for you</li>
</ul>
</li>
</ul>
<h2 id="36-photo-ocr">36 Photo OCR<a hidden class="anchor" aria-hidden="true" href="#36-photo-ocr">#</a></h2>
<h4 id="361-problem-description-and-pipeline">36.1 Problem Description and Pipeline<a hidden class="anchor" aria-hidden="true" href="#361-problem-description-and-pipeline">#</a></h4>
<ul>
<li>
<p>Case study focused around <strong>Photo OCR</strong></p>
</li>
<li>
<p>Three reasons to do this</p>
<ul>
<li>
<ol>
<li>Look at how a <strong>complex system</strong> can be put together</li>
</ol>
</li>
<li>
<ol start="2">
<li>The idea of a machine learning pipeline</li>
</ol>
<ul>
<li>What to do next</li>
<li>How to do it</li>
</ul>
</li>
<li>
<ol start="3">
<li>Some more interesting ideas</li>
</ol>
<ul>
<li>Applying machine learning to tangible problems</li>
<li><strong>Artificial data synthesis</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>What is the photo OCR problem?</strong></p>
<ul>
<li>Photo OCR = Photo Optical Character Recognition
<ul>
<li>With growth of digital photography, lots of digital pictures</li>
<li>One idea which has interested many people is getting computers to understand those photos</li>
<li>The photo OCR problem is getting computers to read text in an image
<ul>
<li>Possible applications for this would include
<ul>
<li>Make searching easier (e.g. searching for photos based on words in them)</li>
<li>Car navigation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>OCR of documents is a comparatively easy problem
<ul>
<li>From photos it&rsquo;s really hard</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>OCR pipeline</strong></p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/STH5PGmQq13eEkr.png" alt="image-20200625112755463"  />
</p>
<ul>
<li>
<ol>
<li>Look through image and find text</li>
</ol>
</li>
<li>
<ol start="2">
<li>Do character segmentation</li>
</ol>
</li>
<li>
<ol start="3">
<li>Do character classification</li>
</ol>
</li>
<li>
<ol start="4">
<li>some may do spell check after this too (<em>Optional</em>)</li>
</ol>
<ul>
<li>We&rsquo;re not focusing on such systems though</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Pipelines</strong> are common in machine learning</p>
<ul>
<li>Separate modules which may each be a machine learning component or data processing component</li>
</ul>
</li>
<li>
<p>If you&rsquo;re designing a machine learning system, pipeline design is one of the most important questions</p>
<ul>
<li>Performance of pipeline and each module often has a big impact on the overall performance a problem</li>
<li>You would often have different engineers working on each module
<ul>
<li>Offers a natural way to divide up the workload</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="362-sliding-windows">36.2 Sliding Windows<a hidden class="anchor" aria-hidden="true" href="#362-sliding-windows">#</a></h4>
<ul>
<li>
<p>As mentioned, stage 1 is <strong>text detection</strong></p>
<ul>
<li>Unusual problem in computer vision - different rectangles (which surround text) may have different aspect ratios (aspect ratio being height : width)
<ul>
<li>Text may be short (few words) or long (many words)</li>
<li>Tall or short font</li>
<li>Text might be straight on</li>
<li>Slanted</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Pedestrian detection</strong></p>
<ul>
<li>
<p>Want to take an image and find pedestrians in the image</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/sgywR8JheMUAF75.png" alt="image-20200625132715115"  />
</p>
</li>
<li>
<p>This is a slightly simpler problem because the aspect ration remains pretty constant</p>
</li>
<li>
<p>Building our detection system</p>
<ul>
<li>
<p>Have $82 \times 36$ aspect ratio</p>
<ul>
<li>This is a typical aspect ratio for a standing human</li>
</ul>
</li>
<li>
<p>Collect training set of positive and negative examples</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/Dx79EbZ2JUKcfps.png" alt="image-20200625132808996"  />
</p>
</li>
<li>
<p>Could have 1000 - 10 000 training examples</p>
</li>
<li>
<p>Train a neural network to take an image and classify that image as pedestrian or not</p>
<ul>
<li>Gives you a way to train your system</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Now we have a new image - how do we find pedestrians in it?</p>
<ul>
<li>
<p>Start by taking a rectangular $82 \times 36$ patch in the image</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/U5MHIFhxrndtZDz.png" alt="image-20200625132905578"  />
</p>
<ul>
<li>Run patch through classifier - hopefully in this example it will return $y = 0$</li>
</ul>
</li>
<li>
<p>Next slide the rectangle over to the right a little bit and re-run</p>
<ul>
<li>Then slide again</li>
<li>The amount you slide each rectangle over is a parameter called the step-size or stride
<ul>
<li>Could use 1 pixel
<ul>
<li>Best, but computationally expensive</li>
</ul>
</li>
<li>More commonly 5-8 pixels used</li>
</ul>
</li>
<li>So, keep stepping rectangle along all the way to the right
<ul>
<li>Eventually get to the end</li>
</ul>
</li>
<li>Then move back to the left hand side but step down a bit too</li>
<li>Repeat until you&rsquo;ve covered the whole image</li>
</ul>
</li>
<li>
<p>Now, we initially started with quite a small rectangle</p>
<ul>
<li>So now we can take a larger image patch (of the same aspect ratio)</li>
<li>Each time we process the image patch, we&rsquo;re resizing the larger patch to a smaller image, then running that smaller image through the classifier</li>
</ul>
</li>
<li>
<p>Hopefully, by changing the patch size and rastering repeatedly across the image, you eventually recognize all the pedestrians in the picture</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/oab82htpzmMP5KS.png" alt="image-20200625133037283"  />
</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Text detection example</strong></p>
<ul>
<li>
<p>Like pedestrian detection, we generate a labeled training set with</p>
<ul>
<li>Positive examples (some kind of text)</li>
<li>Negative examples (not text)</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/cNkDvG3XyKsmYTx.png" alt="image-20200625133118391"  />
</p>
</li>
<li>
<p>Having trained the classifier we apply it to an image</p>
<ul>
<li>So, run a sliding window classifier at a fixed rectangle size</li>
<li>If you do that end up with something like this</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/7n6Y3kcHVqrZpKw.png" alt="image-20200625133148066"  />
</p>
<ul>
<li>
<p>White region show where text detection system thinks text is</p>
<ul>
<li>Different shades of gray correspond to probability associated with how sure the classifier is the section contains text
<ul>
<li>Black - no text</li>
<li>White - text</li>
</ul>
</li>
<li>For text detection, we want to draw rectangles around all the regions where there is text in the image</li>
</ul>
</li>
<li>
<p>Take classifier output and apply an <strong>expansion algorithm</strong></p>
<ul>
<li>
<p>Takes each of white regions and expands it</p>
</li>
<li>
<p>How do we implement this</p>
<ul>
<li>Say, for every pixel, is it within some distance of a white pixel?</li>
<li>If yes then color it white</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/gt6wGzYip3XhWDV.png" alt="image-20200625133257541"  />
</p>
</li>
</ul>
</li>
<li>
<p>Look at connected white regions in the image above</p>
<ul>
<li>
<p>Draw rectangles around those which make sense as text (i.e. tall thin boxes don&rsquo;t make sense)</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/5VaKDzgQmEr1yUv.png" alt="image-20200625133329514"  />
</p>
</li>
</ul>
</li>
<li>
<p>This example misses a piece of text on the door because the aspect ratio is wrong</p>
<ul>
<li>Very hard to read</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Stage 2: character segmentation</strong></p>
<ul>
<li>
<p>Use supervised learning algorithm</p>
</li>
<li>
<p>Look in a defined image patch and decide, is there a split between two characters?</p>
<ul>
<li>So, for example, our first training data item below looks like there is such a split</li>
<li>Similarly, the negative examples are either empty or hold a full characters</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/I1yRLCfMZ9zNpgF.png" alt="image-20200625133426434"  />
</p>
</li>
<li>
<p>We train a classifier to try and classify between positive and negative examples</p>
<ul>
<li>Run that classifier on the regions detected as containing text in the previous section</li>
</ul>
</li>
<li>
<p>Use a 1-dimensional sliding window to move along text regions</p>
<ul>
<li>Does each window snapshot look like the split between two characters?
<ul>
<li>If yes insert a split</li>
<li>If not move on</li>
</ul>
</li>
<li>So we have something that looks like this</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/DT8NmRIEgt4WkYe.png" alt="image-20200625133519198"  />
</p>
</li>
</ul>
</li>
<li>
<p><strong>Character classification</strong></p>
<ul>
<li>Standard OCR, where you apply standard supervised learning which takes an input and identify which character we decide it is
<ul>
<li>Multi-class characterization problem</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="363-getting-lots-of-data-and-artificial-data">36.3 Getting Lots of Data and Artificial Data<a hidden class="anchor" aria-hidden="true" href="#363-getting-lots-of-data-and-artificial-data">#</a></h4>
<ul>
<li>
<p>We&rsquo;ve seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias algorithm and train on a massive data set</p>
<ul>
<li>Where do we get so much data from</li>
<li>In ML artifice data synthesis
<ul>
<li>Doesn&rsquo;t apply to every problem</li>
<li>If it applies to your problem can be a great way to generate loads of data</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Two main principles</p>
<ul>
<li>
<ol>
<li>Creating data from scratch</li>
</ol>
</li>
<li>
<ol start="2">
<li>If we already have a small labeled training set can we amplify it into a larger training set</li>
</ol>
</li>
</ul>
</li>
<li>
<p>Character recognition as an example of data synthesis</p>
<ul>
<li>
<p>If we go and collect a large labeled data set will look like this</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/06/25/N0XtQU.png" alt="N0XtQU.png"  />
</p>
<ul>
<li>Goal is to take an image patch and have the system recognize the character</li>
<li>Treat the images as gray-scale (makes it a bit easer)</li>
</ul>
</li>
<li>
<p>How can we amplify this</p>
<ul>
<li>Modern computers often have a big font library</li>
<li>If you go to websites, huge free font libraries</li>
<li>For more training data, take characters from different fonts, paste these characters again random backgrounds</li>
</ul>
</li>
<li>
<p>After some work, can build a synthetic training set</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/glQkv9hiwLyatfo.png" alt="image-20200625135309337"  />
</p>
<ul>
<li>Random background</li>
<li>Maybe some blurring/distortion filters</li>
<li>Takes thought and work to make it look realistic
<ul>
<li>If you do a sloppy job this won&rsquo;t help!</li>
<li>So unlimited supply of training examples</li>
</ul>
</li>
<li>This is an example of creating new data from scratch</li>
</ul>
</li>
<li>
<p>Other way is to introduce distortion into existing data</p>
<ul>
<li>
<p>e.g. take a character and warp it</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/JPBVzGvtj8WITwK.png" alt="image-20200625135401927"  />
</p>
<ul>
<li>16 new examples</li>
<li>Allows you amplify existing training set</li>
</ul>
</li>
<li>
<p>This, again, takes though and insight in terms of deciding how to amplify</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Another example: speech recognition</p>
<ul>
<li>Learn from audio clip - what were the words
<ul>
<li>Have a labeled training example</li>
<li>Introduce audio distortions into the examples</li>
</ul>
</li>
<li>So only took one example
<ul>
<li>Created lots of new ones!</li>
</ul>
</li>
<li>When introducing distortion, they should be reasonable relative to the issues your classifier may encounter</li>
</ul>
</li>
<li>
<p>Getting more data</p>
<ul>
<li>Before creating new data, make sure you have a low bias classifier
<ul>
<li>Plot learning curve</li>
</ul>
</li>
<li>If not a low bias classifier increase number of features
<ul>
<li>Then create large artificial training set</li>
</ul>
</li>
<li>Very important question: How much work would it be to get $10\times$ data as we currently have?
<ul>
<li>Often the answer is, &ldquo;Not that hard&rdquo;</li>
<li>This is often a huge way to improve an algorithm</li>
<li>Good question to ask yourself or ask the team</li>
</ul>
</li>
<li>How many minutes/hours does it take to get a certain number of examples
<ul>
<li>Say we have 1000 examples</li>
<li>10 seconds to label an example</li>
<li>So we need another 9000 - 90000 seconds</li>
<li>Comes to a few days (25 hours!)</li>
</ul>
</li>
<li>Crowd sourcing is also a good way to get data
<ul>
<li>Risk or reliability issues</li>
<li>Cost</li>
<li>Example: <em>Amazon Mechanical Turk</em> (<em>MTurk</em>)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="364-celling-analysis-what-part-of-the-pipeline-to-work-on-next">36.4 Celling Analysis: What Part of the Pipeline to Work on Next<a hidden class="anchor" aria-hidden="true" href="#364-celling-analysis-what-part-of-the-pipeline-to-work-on-next">#</a></h4>
<ul>
<li>
<p>Through the course repeatedly said one of the most valuable resources is <em>developer time</em></p>
<ul>
<li>Pick the right thing for you and your team to work on</li>
<li>Avoid spending a lot of time to realize the work was pointless in terms of enhancing performance</li>
</ul>
</li>
<li>
<p>Photo OCR pipeline</p>
<ul>
<li>Three modules
<ul>
<li>Each one could have a small team on it</li>
<li>Where should you allocate resources?</li>
</ul>
</li>
<li>Good to have a single real number as an evaluation metric
<ul>
<li>So, character accuracy for this example</li>
<li>Find that our test set has 72% accuracy</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Ceiling analysis on our pipeline</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/yHvsgnoaFdhwqc1.png" alt="image-20200625140953845"  />
</p>
<ul>
<li>We go to the first module
<ul>
<li>Mess around with the test set - manually tell the algorithm where the text is</li>
<li>Simulate if your text detection system was 100% accurate
<ul>
<li>So we&rsquo;re feeding the character segmentation module with 100% accurate data now</li>
</ul>
</li>
<li>How does this change the accuracy of the overall system</li>
<li>Accuracy goes up to 89%</li>
</ul>
</li>
<li>Next do the same for the character segmentation
<ul>
<li>Accuracy goes up to 90% now</li>
</ul>
</li>
<li>Finally doe the same for character recognition
<ul>
<li>Goes up to 100%</li>
</ul>
</li>
<li>Having done this we can qualitatively show what the upside to improving each module would be
<ul>
<li>Perfect text detection improves accuracy by 17%!
<ul>
<li>Would bring the biggest gain if we could improve</li>
</ul>
</li>
<li>Perfect character segmentation would improve it by 1%
<ul>
<li>Not worth working on</li>
</ul>
</li>
<li>Perfect character recognition would improve it by 10%
<ul>
<li>Might be worth working on, depends if it looks easy or not</li>
</ul>
</li>
</ul>
</li>
<li>The &ldquo;ceiling&rdquo; is that each module has a ceiling by which making it perfect would improve the system overall</li>
</ul>
</li>
<li>
<p>Another example - face recognition</p>
<ul>
<li>
<p>This is not how it&rsquo;s done in practice</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/T4bX2ZhO7o19IUH.png" alt="image-20200625141059788"  />
</p>
</li>
<li>
<p>How would you do ceiling analysis for this</p>
<p><img loading="lazy" src="https://i.loli.net/2020/06/25/PiEIONCe1qlKYcB.png" alt="image-20200625141237364"  />
</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<ul>
<li>Supervised Learning
<ul>
<li>Linear regression, logistic regression, neural networks, SVMs</li>
</ul>
</li>
<li>Unsupervised Learning
<ul>
<li>K-means, PCA, Anomaly detection</li>
</ul>
</li>
<li>Special applications/special topics
<ul>
<li>Recommender systems, large scale machine learning</li>
</ul>
</li>
<li>Advice on building a machine learning system
<ul>
<li>Bias/variance, regularization; deciding what to work on next: evaluation of learning algorithms, learning curves, error analysis, ceiling analysis</li>
</ul>
</li>
<li>AND A BIG THANK YOU!</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jifan.tech/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://jifan.tech/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://jifan.tech/tags/python/">Python</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jifan.tech/posts/2020-06-30-mlf-mf-ntu-1/">
    <span class="title">« Prev Page</span>
    <br>
    <span>机器学习基石-数学篇：When Can Machines Learn?</span>
  </a>
  <a class="next" href="https://jifan.tech/posts/2020-06-24-ml-ng-9/">
    <span class="title">Next Page »</span>
    <br>
    <span>机器学习-吴恩达：学习笔记及总结（9）</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://jifan.tech">Jifan&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
