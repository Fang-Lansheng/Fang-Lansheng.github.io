<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn? | Jifan&#39;s Blog</title>
<meta name="keywords" content="ML/DL, Course Learning, Python" />
<meta name="description" content="Course Link Ôºö
 Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices &hellip;
 5.1 Recap and Preview   Recap: the statistical learning flow
 if $\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\cal A$, $E_{out}(g) \approx E_{in}(g)$   if $\cal A$ finds one $g$ with $E_{in}(g) \approx 0$  PAC guarantee for $E_{out}(g) \approx 0$ $\Rightarrow$ learning possible      Two central questions">
<meta name="author" content="jifan">
<link rel="canonical" href="https://jifan.tech/posts/2020-07-02-mlf-mf-ntu-2/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://jifan.tech/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jifan.tech/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jifan.tech/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jifan.tech/apple-touch-icon.png">
<link rel="mask-icon" href="https://jifan.tech/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?" />
<meta property="og:description" content="Course Link Ôºö
 Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices &hellip;
 5.1 Recap and Preview   Recap: the statistical learning flow
 if $\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\cal A$, $E_{out}(g) \approx E_{in}(g)$   if $\cal A$ finds one $g$ with $E_{in}(g) \approx 0$  PAC guarantee for $E_{out}(g) \approx 0$ $\Rightarrow$ learning possible      Two central questions" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jifan.tech/posts/2020-07-02-mlf-mf-ntu-2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-02T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-07-02T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?"/>
<meta name="twitter:description" content="Course Link Ôºö
 Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices &hellip;
 5.1 Recap and Preview   Recap: the statistical learning flow
 if $\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\cal A$, $E_{out}(g) \approx E_{in}(g)$   if $\cal A$ finds one $g$ with $E_{in}(g) \approx 0$  PAC guarantee for $E_{out}(g) \approx 0$ $\Rightarrow$ learning possible      Two central questions"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://jifan.tech/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?",
      "item": "https://jifan.tech/posts/2020-07-02-mlf-mf-ntu-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?",
  "name": "Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?",
  "description": "Course Link Ôºö\n Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices \u0026hellip;\n 5.1 Recap and Preview   Recap: the statistical learning flow\n if $\\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\\cal A$, $E_{out}(g) \\approx E_{in}(g)$   if $\\cal A$ finds one $g$ with $E_{in}(g) \\approx 0$  PAC guarantee for $E_{out}(g) \\approx 0$ $\\Rightarrow$ learning possible      Two central questions",
  "keywords": [
    "ML/DL", "Course Learning", "Python"
  ],
  "articleBody": " Course Link Ôºö\n Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices ‚Ä¶\n 5.1 Recap and Preview   Recap: the statistical learning flow\n if $\\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\\cal A$, $E_{out}(g) \\approx E_{in}(g)$   if $\\cal A$ finds one $g$ with $E_{in}(g) \\approx 0$  PAC guarantee for $E_{out}(g) \\approx 0$ $\\Rightarrow$ learning possible      Two central questions\n learning split to two central questions:  can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$? can we make $E_{in}(g)$ small enough?   what role dose $M$ ($\\vert \\cal H \\vert$) play for the two questions?    Trade-off on $M$\n using the right $M$ (or $\\cal H$) is important $M = \\infty$ doomed?    Preview\n  Known\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\epsilon\\right] \\leq 2 \\cdot M \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n  Todo\n  establish a finite quantity that replaces $M$\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\epsilon\\right] \\mathop{\\leq}^{?} 2 \\cdot m_{\\mathcal{H}} \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n  justify the feasibility of learning for infinite $M$\n  study $m_{\\cal H}$ to understand its trade-off for right $\\cal H$, just like $M$\n    mysterious PLA to be fully resolved after 3 more lecturesüéâ\n    5.2 Effective Number of Lines   Where did $M$ come from?\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\epsilon\\right] \\leq 2 \\cdot M \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n  BAD events $\\mathcal{B}_m : \\vert E_{in}(h_m) - E_{out}(h_m)\\vert  \\epsilon$\n  to give $\\cal A$ freedom of choice: bound $\\Bbb{P}[\\mathcal{B}_1 \\text{ or } \\cdots \\text{ or } \\mathcal{B}_M ]$\n  worst case: all $\\mathcal{B}_m$ non-overlapping\n$$ \\Bbb{P}[\\mathcal{B}_1 \\text{ or } \\cdots \\text{ or } \\mathcal{B}M ] \\underbrace{\\le}{\\text{union bound}} \\Bbb{P}[\\mathcal{B}_1] + \\cdots \\Bbb{P}[\\mathcal{B}_M] $$\n  where did uniform bound fail to consider for $M = \\infty$?\n    Where did uniform bound fail?\n$$ {\\text{union bound }} \\Bbb{P}[\\mathcal{B}_1] + \\cdots \\Bbb{P}[\\mathcal{B}_M] $$\n  BAD events $\\mathcal{B}_ m:\\vert E_{in}(h_m) - E_{out}(h_m)\\vert  \\epsilon$\n  overlapping for similar hypotheses $h_1 \\approx h_2$\n    Why?\n $E_{out}(h_1) \\approx E_{out}(h_2)$ for most $\\cal D$, $E_{in}(h_1) = E_{in}(h_2)$    union bound over-estimating\n  to account for overlap, can we group similar hypotheses by kind?\n    How many lines are there?\n$$ \\mathcal{H} = \\lbrace \\text{all lines in } \\Bbb{R}^2 \\rbrace $$\n  Effective number of lines\n   $N$ effective $(N)$     $1$ $2$   $2$ $4$   $3$ $8$   $4$ $14\\ (      maximum kinds of lines with respect to $N$ inputs $\\mathbf{x}_1, \\cdots, \\mathbf{x}_N \\Leftrightarrow$ effective numbers of lines\n must be $\\le 2^N$ finite grouping of infinitely-many lines $\\in \\cal H$ wish:  $$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\epsilon\\right] \\leq 2 \\cdot \\text{effective}(N) \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n    if\n $\\text{effective}(N)$ can replace $M$ and  $\\text{effective}(N) \\ll 2^N$      learning possible with infinite lines.\n  5.3 Effective Number of Hypothesis   Dichotomies: mini-hypothesis\n$$ \\mathcal{H} = \\lbrace \\text{hypothesis } h: \\mathcal{X} \\rightarrow \\lbrace {\\color{Red}{\\times}},{\\color{Blue}{\\circ}}\\rbrace \\rbrace $$\n  call $$ h(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) = \\left( h(\\mathbf{x}_1), \\dots, h(\\mathbf{x}_N) \\right) \\in \\lbrace {\\color{Red}{\\times}},{\\color{Blue}{\\circ}}\\rbrace^N $$\n  a dichotomy: hypothesis limited to the eyes of $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$\n  $\\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$: all dichotomies implemented by $\\cal H$ on $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$\n    hypothesis $\\cal H$ dichotomies $\\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$     e.g. all lines in $\\Bbb{R}^2$ $\\lbrace {\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}},{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Red}{\\times}},{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Red}{\\times}}{\\color{Red}{\\times}},\\dots\\rbrace$   size possibly infinite upper bounded by $2^N$      $\\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) \\vert$: candidate for replacing $M$\n    Growth function\n  $\\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) \\vert$: depends on inputs $(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$\n  growth function: remove dependence by taking $\\max$ of all possible $(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$\n$$ m_{\\cal H}(N) = \\max_{\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathcal{X}} \\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) \\vert $$\n  finite, upper-bounded by $2^N$\n    Growth function for positive rays\n $\\mathcal{X} = \\Bbb{R}$ (one dimensional) $\\cal H$ contains $h$, where each $h(x) = \\text{sign}(x-a)$ for threshold $a$ positive half of 1-D perceptrons one dichotomy for $a \\in$ each spot $(x_n, x_{n+1})$: $m_{\\cal H}(N) = N + 1$ $(N+1) \\ll 2^N$ when $N$ large!    Growth function for positive intervals\n  $\\mathcal{X} = \\Bbb{R}$ (one dimensional)\n  $\\cal H$ contains $h$, where each $h(x) = +1 \\text{ iff } x \\in [\\mathcal{l}, r), -1$ otherwise\n  one dichotomy for each interval kind:\n$$ m_{\\cal H}(N) = \\underbrace{\\frac{1}{2}N(N+1)}{\\text{interval ends in $N+1$ spots}} + \\underbrace{1}{\\text{all }\\times} $$\n  $\\frac{1}{2}N^2+\\frac{1}{2}N+1 \\ll 2^N$ when $N$ large!\n    Growth function for convex sets\n  $\\mathcal{X} = \\Bbb{R}^2$ (two dimensional)\n  $\\cal H$ contains $h$, where each $h(x) = +1 \\text{ iff $x$ in a convex region}, -1$ otherwise\n  one possible set of $N$ inputs: $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ on a big circle\n  every dichotomy can be implemented by $\\cal H$ using a convex region slightly extended from contour of positive inputs: $m_{\\cal H} = 2^N$\n  call this $N$ inputs shattered by $\\cal H$\n  $m_{\\cal H}(N) = 2^N \\Leftrightarrow$ exists $N$ inputs that can be shattered\n    5.4 Break Point   The four growth functions\n  what if $m_{\\cal H}(N)$ replaces $M$?\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\epsilon\\right] \\mathop{\\leq}^{?} 2 \\cdot m_{\\mathcal{H}}(N) \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n polynomial: good üëç exponential: bad üëé    for 2D or general perceptron, $m_{\\cal H}(N)$ polynomial (good)?\n    Break point of $\\cal H$\n what do we know about 2D perceptrons now?  three inputs: exists shatter four inputs: for all no shatter   if no $k$ inputs can be shattered by $\\cal H$, call $k$ a break point for $\\cal H$  $m_{\\cal H} $k+1, k+2, \\dots $ also break points! will study minimum break point $k$      The four beak points\n conjecture:  no break point: $m_{\\cal H} = 2^N$ (sure!) break point $k$: $m_{\\cal H} = O(N^{k-1})$  exited? wait for next lecture ;-)        6 Theory of Generalization  Test error can approximate training error if there is enough data and growth function does not grow too fast ‚Ä¶\n 6.1 Restiction of Break Point   The four break points\n growth function $m_{\\cal H}(N)$: max number of dichotomies break point $k \\Rightarrow$ break point $k + 1, \\dots$ what else?    Restriction of break point\n what must be true when minimum break point $k = 2$  $N = 1$: every $m_{\\cal H}(N) = 2$ by definition $N = 2$: every $m_{\\cal H}(N) (so maximum possible = 3)   $N = 3$: maximum possible $= 4 \\ll 2^3$   break point $k$ restricts maximum possible $m_{\\cal H}(N)$ a lot for $N  k$    6.2 Bounding Function: Basic Cases  Bounding function $B(N, k)$: maximum possible $m_{\\cal H}(N)$ when break point $= k$  combinatorial quantity:  maximum number of length-$N$ vectors with $({\\color{Red}{\\times}},{\\color{Blue}{\\circ}})$ while ‚Äúno shatter‚Äù any length-$k$ subvectors   irrelevant of the details of $\\cal H$  e.g. $B(N, 3)$ bounds both  positive intervals ($k = 3$) 1D perceptrons ($k = 3$)     new goal: $B(N, k) \\le \\text{poly}(N)$ ?    6.3 Bounding Function: Inductive Cases   Table of bounding funciton\n Known  $B(N, 1) = 1$ (see previous quiz) $B(N, k) = 2^N \\text{ for } N $B(N, k) = 2^N-1 \\text{ for } N = k$   $B(N, k) \\le B(N-1, k) +B(N-1, k-1)$  (actually ‚Äú$=$‚Äù)   Now have upper bound of bounding function    Bounding function: the theorem\n$$ B(N, k) \\leq \\underbrace{\\sum_{i=0}^{k-1}\\dbinom{N}{i}}_{\\text{highest term }N^{k-1}} $$\n simple induction using boundary and inductive formula for fixed $k$, $B(N, k)$ upper bounded by $\\text{poly}(N)$  $\\Rightarrow m_{\\cal H}(N)$ is $\\text{poly}(N)$ if break points exists   ‚Äú$\\le$‚Äù can be ‚Äú$=$‚Äù actually    The three break points\n  6.4 A Pictorial Proof   BAD Bound for General $\\cal H$\n  want:\n$$ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right\\vert \\epsilon\\right] \\leq 2 \\cdot \\quad {\\color{Orange}{m_{\\mathcal{H}}(N)}} \\cdot \\exp \\left(-2 \\quad \\epsilon^{2} N\\right) $$\n  actually, when $N$ is large enough:\n$$ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right\\vert \\epsilon\\right] \\leq 2 \\cdot {\\color{Red}{2}} {\\color{Orange}{m_{\\mathcal{H}}({\\color{Blue}{2}}N)}} \\cdot \\exp \\left(-2 \\cdot {\\color{Purple}{\\frac{1}{16}}} \\epsilon^{2} N\\right) $$\n  next: sketch of proof\n    Step 1: Replace $E_{out}$ by $E_{in}^{\\prime}$\n$$ \\begin{aligned} \u0026 \\frac{1}{2} \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in}}(h)-E_{\\text {out}}(h)\\right\\vert \\epsilon\\right] \\newline \\leq \u0026 \\ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in}}(h)- {\\color{Red}{E_{\\text {in}}^{\\prime}(h)}} \\right\\vert  {\\color{Red}{\\frac{\\epsilon}{2}}} \\right] \\end{aligned} $$\n  $E_{\\text {in}}(h)$ finitely many, $E_{\\text {out}}(h)$ infinitely many\n replace the evil $E_{\\text {out}}(h)$ first    how? sample verification set $\\cal D^{\\prime}$ of size $N$ to calculate $E_{\\text {in}}^{\\prime}$\n  BAD $h$ of $E_{\\text {in}} - E_{\\text {out}}$\n $\\stackrel{\\text { probably }}{\\Longrightarrow}$ BAD $h$ of $E_{\\text {in}} - E_{\\text {in}}^{\\prime}$      Step 2: Decompose $\\cal H$ by Kind\n$$ \\begin{aligned} \\mathrm{BAD} \u0026 \\leq {\\color{Red}{2}} \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)- {\\color{Red}{E_{\\text {in }}^{\\prime}(h)}} \\right\\vert {\\color{Red}{\\frac{\\epsilon}{2}}}\\right] \\ \u0026 \\leq {\\color{Red}{2}} m_{\\mathcal{H}}({\\color{Blue}{2}} N) \\mathbb{P}\\left[{\\color{Blue}{\\text { fixed }}} h \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-{\\color{Red}{E_{\\text {in }}^{\\prime}(h)}} \\right\\vert {\\color{Red}{\\frac{\\epsilon}{2}}}\\right] \\end{aligned} $$\n $E_{\\text {in}}$ with $\\cal D$, $E_{\\text {in}}^{\\prime}$ with $\\cal D^{\\prime}$  now $m_{\\cal H}$ comes to play   how? infinite $\\cal H$ becomes $\\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)\\vert $ kinds union bound on $m_{\\cal H}(2N)$ kinds    Step 3: Use Hoeffding without Replacement\n$$ \\begin{aligned} \\mathrm{BAD} \u0026 \\leq {\\color{Red}{2}} m_{\\mathcal{H}}({\\color{Blue}{2}} N) \\mathbb{P}\\left[{\\color{Blue}{\\text { fixed }}} h \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-{\\color{Red}{E_{\\text {in }}^{\\prime}(h)}} \\right\\vert {\\color{Red}{\\frac{\\epsilon}{2}}}\\right] \\ \u0026 \\leq {\\color{Red}{2}} m_{\\mathcal{H}}({\\color{Blue}{2}} N) \\cdot 2 \\exp \\left(-2\\left({\\color{Purple}{\\frac{\\epsilon}{4}}}\\right)^{2} N\\right) \\end{aligned} $$\n consider bin of $2N$ examples, choose $N$ for $E_{\\text{in}}$,leave others for $E_{\\text{in}}^{\\prime}$  $\\vert E_{\\text{in}} - E_{\\text{in}}^{\\prime} \\vert  \\frac{\\epsilon}{2} \\Leftrightarrow \\left \\vert E_{\\text{in}} - \\frac{E_{\\text{in}}+E_{\\text{in}}^{\\prime}}{2} \\right\\vert  \\frac{\\epsilon}{4}$   so? just similar bin, smaller $\\epsilon$, and Hoeffding without replacement    That‚Äôs All! $\\Rightarrow$ Vapnik-Chervonenkis( VC) bound:\n$$ \\begin{aligned} \u0026\\ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right\\vert \\epsilon\\right] \\newline \\leq \u0026 \\ {\\color{Red}{4}} {\\color{Orange}{m_{\\mathcal{H}}({\\color{Blue}{2}}N)}} \\cdot \\exp \\left(- {\\color{Purple}{\\frac{1}{8}}} \\epsilon^{2} N\\right) \\end{aligned} $$\n replace $E_{\\text{out}}$ by $E_{\\text{in}}^{\\prime}$ decompose $\\cal H$ by kind use Hoeffding without replacement 2D perceptrons:  break point? 4 $m_{\\cal H}(N)$? $O(N^3)$ learning with 2D perceptrons feasible! üéâ      7 The VC Dimension  Learning happens if there is finite model complexity (called VC dimension), enough data, and low training error ‚Ä¶\n 7.1 Definition of VC Dimension   Recap: More on Growth Function\n  Recap: More on Vapnik-Chervonenkis(VC) Bound\n if:  ‚ë† $m_{\\cal H}(N)$ breaks at $k$ (good $\\cal H$) ‚ë° $N$ large enough (good $\\cal D$)   $\\Rightarrow$ probably generalized $E_{out} \\approx E_{in}$, and if:  ‚ë¢ $\\cal A$ picks a $g$ with small $E_{in}$ (good $\\cal A$)   $\\Rightarrow$ probably learned! üéâ    VC Dimension\n  the formal name of **maximum non-**break point\n  Definition: VC dimension of $\\cal H$, denoted $d_{\\text{VC}}(\\cal H)$ is\n$$ \\text{largest } N \\text{ for which } m_{\\cal H}(N) = 2^N $$\n the most inputs $\\cal H$ that can shatter $d_{\\text{VC}} = \\min(k) - 1$    $N \\leq d_{\\text{VC}} \\Rightarrow$\n $\\cal H$ can shatter some $N$ inputs    $N  d_{\\text{VC}} \\Rightarrow$\n $N$ is a break point for $\\cal H$    $\\text{if } N \\ge 2, d_{\\text{VC}} \\ge 2, m_{\\cal H} \\leq N^{d_{\\text{VC}}}$\n    The Four VC Dimensions\n  VC Dimension and Learning\n good $\\Rightarrow$ finite $d_{\\text{VC}}$ $\\Rightarrow$ $g$ ‚Äúwill‚Äù generalize $(E_{out}(g) \\approx E_{in}(g))$ regardless of learning algorithm $\\cal A$ regardless of input distribution $P$ regardless of target function $f$    7.2 VC Dimension of Perceptrons   2D PLA Revisited\n general PLA for $\\mathbf{x}$ with more that 2 features?    VC Dimension of Perceptrons\n 1D perceptron (pos/neg rays): $d_{\\text{VC}} = 2$ 2D perceptron: $d_{\\text{VC}} = 3$ $d$-D perceptrons: $d_{\\text{VC}} \\mathop{=}^{?}d+1$      $d_{\\text{VC}} \\ge d + 1$\n  There are some $d+1$ inputs we can shatter\n  some ‚Äútrivial‚Äù inputs:\n$$ \\mathbf{X}=\\left[\\begin{array}{c} -\\mathbf{x}{1}^{T}- \\ -\\mathbf{x}{2}^{T}- \\ -\\mathbf{x}{3}^{T}- \\ \\vdots \\ -\\mathbf{x}{d+1}^{T}- \\end{array}\\right]=\\left[\\begin{array}{ccccc} 1 \u0026 0 \u0026 0 \u0026 \\cdots \u0026 0 \\ 1 \u0026 1 \u0026 0 \u0026 \\cdots \u0026 0 \\ 1 \u0026 0 \u0026 1 \u0026 \u0026 0 \\ \\vdots \u0026 \\vdots \u0026 \u0026 \\ddots \u0026 0 \\ 1 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 1 \\end{array}\\right] $$\n    note: $\\mathbf{X}$ invertible\n  to shatter ‚Ä¶\n for any $\\bf y \\in \\Bbb{R}^{d+1}$, find $\\mathbf{w}$ such that  $\\text{sign}(\\mathbf{Xw})=\\mathbf{y} \\Leftarrow \\bf{Xw=y} \\stackrel{X\\text{ invertible!}}{\\Longleftrightarrow} w= X^{-1}y $        $d_{\\text{VC}} \\ge d+1$\n  A 2D special case\n  $d$-D general case\n  general $\\bf X$ no-shatter $\\Rightarrow d_{\\text{VC}}\\le d+1$\n    7.3 Physical Intuition of VC Dimension   Degrees of Freedom\n hypothesis parameters $\\mathbf{w}=(w_0, w_1, \\dots, w_d)$: creates degrees of freedom hypothesis quantity $M = \\vert \\mathcal{H} \\vert$: ‚Äúanalog‚Äù degrees of freedom hypothesis power $d_{\\text{VC}}=d+1$: effective ‚Äúbinary‚Äù degrees of freedom $d_{\\text{VC}}(\\cal H)$: powerfulness of $\\cal H$    Two Old Friends\n  Positive rays ($d_{\\text{VC}}=1$)\n free parameters: $a$    Positive intervals ($d_{\\text{VC}}=2$):\n free parameters: $\\mathcal{l}, r$    pratical rule of thumb: $d_{\\text{VC}} \\approx \\text{#free parameters}$ (but not always)\n    $M$ and $d_{\\text{VC}}$\n using the right $d_{\\text{VC}}$ (or $\\cal H$) is important      VC Bound Rephrase: Penalty for Model Complexity\n  For any ${\\color{Red}{g}={\\color{Red}{\\cal A}}}({\\color{Blue}{\\cal D}}) \\in \\mathcal {\\color{Orange}{H}}$ and statistical large $\\cal D$, for $d_{\\text{VC}} \\ge 2$:\n$$ {\\mathbb{P}_ {\\color{Blue} \\mathcal{D} } } [\\underbrace{\\left\\vert E_{\\text {in }}({\\color{Red} g} )-E_{\\text {out }}({\\color{Red} g} )\\right\\vert \\epsilon }_ { \\mathbf{BAD} } ] \\leq \\underbrace{ {4} {\\color{Orange} (2 N)^{d_\\text {VC }}} \\exp \\left(-\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\right)}_ {\\color{Purple} \\delta} $$\n  Rephrase: with prbability $\\ge 1 - \\delta$, GOOD: $\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\leq \\epsilon$\n$$ \\begin{aligned} \\operatorname{set} \\quad {\\color{Purple} \\delta} \u0026=4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} \\exp \\left(-\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\right) \\ \\frac{\\color{Purple} \\delta }{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} } \u0026=\\exp \\left(-\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\right) \\ \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}} }}{\\color{Purple} \\delta }\\right) \u0026=\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\ \\sqrt{\\frac{8}{\\color{Blue} N } \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta }\\right)} \u0026=\\epsilon \\end{aligned} $$\n  generalization error $\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\le \\sqrt{\\frac{8}{\\color{Blue} N } \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta }\\right)} $\n  $E_{\\text {in }}(g)- \\sqrt{\\frac{8}{\\color{Blue} N} \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta} \\right)} \\le E_{\\text {out }}(g) \\le E_{\\text{in}}(g) + \\sqrt{\\frac{8}{\\color{Blue} N} \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta} \\right)}$\n  $\\underbrace{\\sqrt{\\cdots}}_{\\Omega(N, \\mathcal{H}, \\delta)}$Ôºö penalty for model complexity\n    THE VC Message\n with a high probability, $E_{\\text {out }}(g) \\le E_{\\text{in}}(g) + \\underbrace{\\sqrt{\\frac{8}{\\color{Blue} N} \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta }\\right)}}_{\\Omega(N, \\mathcal{H}, \\delta)}$   powerful $\\cal H$ not always good!    VC Bound Rephrase: Sample Complexity\n practical rule of thumb:  $N \\approx 10 d_{\\text{VC}}$ often enough!      Looseness of VC Bound\n  theory: $N \\approx 10,000\\ d_{\\text{VC}}$; practice: $N \\approx 10\\ d_{\\text{VC}}$\n  Why?\n  philosophical message of VC bound: important for improving ML\n    8 Noise and Error  Learning can still happen within a noisy environment and different error measures ‚Ä¶\n 8.1 Noise and Probabilistic Target   Recap: The Learning Flow\n what if there is noise?    Noise\n  Probabilistic Marbles\n    Target Distribution $P(y\\vert\\mathbf{x})$: characterizes behavior of ‚Äúmini-target‚Äù on one $\\bf x$\n goal of learning:  predict ideal mini-target (w.r.t. $P(y\\vert \\mathbf{x})$) on often-seen inputs (w.r.t $P(\\mathbf{x})$)      The New Learning Flow\n  8.2 Error Measure   Error Measure: final hypothesis $g \\approx f$\n  how well? previously, considered out-of-sample measure\n$$ E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}[\\mathbf{g}(\\mathbf{x}) \\neq f(\\mathbf{x})] $$\n  more generally, error measure $E(g, f)$\n  naturally considered\n out-of-sample: averaged over unknown $\\bf x$ pointwise: evaluated on one $\\bf x$ classification: $[\\text{prediction} \\ne \\text{target}]$    classification error often also called ‚Äú0/1 error‚Äù\n    Pointwise Error Measure\n  can often express $E(g,f)$ = averaged $\\mathop{\\text{err}}(g(\\mathbf{x}),f(\\mathbf{x}))$, like\n$$ E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}, \\underbrace{[g(\\mathbf{x}) \\neq f(\\mathbf{x})]}_{\\operatorname{err}(g(\\mathbf{x}), f(\\mathbf{x}))} $$\n $\\text{err}$: called pointwise error measure    in-sample:\n$$ E_{\\mathrm{in}}(g)=\\frac{1}{N} \\sum_{n=1}^{N} \\operatorname{err}\\left(g \\left( \\mathbf{x}{n} \\right), f \\left(\\mathbf{x}{n}\\right) \\right) $$\n  out-of-sample:\n$$ E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}, \\operatorname{err}\\left(g \\left( \\mathbf{x}{n} \\right), f \\left(\\mathbf{x}{n}\\right) \\right) $$\n  will mainly consider pointwise $\\operatorname{err}$ for simplicity\n    Two Important Pointwise Error Measure: $\\operatorname{err}(g(\\mathbf{x}), f(\\mathbf{x})) = \\operatorname{err}(\\tilde{y}, y)$\n 0/1 error: $\\operatorname{err}(\\tilde{y}, y)=[\\tilde{y} \\neq y]$  correct or incorrect? often for classification   squared error: $\\operatorname{err}(\\tilde{y}, y)=(\\tilde{y}-y)^{2}$  how far is $\\tilde{y}$ from $y$? often for regression   how does $\\operatorname{err}$ ‚Äúguide‚Äù learning?    Ideal Mini-Target\n  interplay between noise and error\n $P(y\\vert \\mathbf{x})$ and $\\text{err}$ define ideal mini-target $f(\\mathbf{x})$    e.g.\n    Learning Flow with Error Measure\n extended VC theory/‚Äúphilosophy‚Äù works for most $\\cal H$ and $\\text{err}$    8.3 Algorithmic Error Measure   Choice of Error Measure\n  two types of error: false acceptand false reject  0/1 error penalizes both types equally\n  Fingerprint verification for supermarket\n supermarket: fingerprint for discount false reject: very unhappy customer, lose future business false accept: give away a minor discount, intruder left fingerprint    Fingerprint verification for CIA\n CIA: fingerprint for entrance false accept: very serious consequences! false reject: unhappy employee, but so what?      Take-home Message for Now: $\\text{err}$ is application/user-dependent\n Algorithmic Error Measures $\\widehat{\\text{err}}$  true: just $\\text{err}$ plausible:  0/1: minimum ‚Äúflipping noise‚Äù‚ÄîNP-hard to optimize, remember? squared: minimum Gaussian noise   friendly: easy to optimize for $\\cal A$  closed-form solution convex objective function     $\\widehat{\\text{err}}$: more in next lectures    Learning Flow with Algorithmic Error Measure\n  8.4 Weighted Classification   Weighted Classification\n  CIA cost (error, loss) matrix\n  out-of-sample\n$$ E_{\\text {out}}(h)= \\underset{(\\mathbf{x},y) \\sim P}{\\mathcal{E}} \\left\\lbrace \\begin{array}{cc} {\\color{Purple} 1} \u0026 \\text { if } y=+1 \\ {\\color{Orange} 1000} \u0026 \\text { if } y=-1 \\end{array}\\right\\rbrace \\cdot {\\color{Blue} [y \\neq h(\\mathbf{x}) ] } $$\n  in-sample\n$$ E_{\\text {in}}(h) = \\frac{1}{N} \\sum_{n=1}^{N} \\left\\lbrace \\begin{array}{cc} {\\color{Purple} 1} \u0026 \\text { if } y_ n =+1 \\ {\\color{Orange} 1000} \u0026 \\text { if } y_ n =-1 \\end{array} \\right\\rbrace \\cdot { \\color{Blue} \\left[ y _ n \\neq h \\left(\\mathbf{x} _ n \\right) \\right] } $$\n  weighted classification: different ‚Äúweight‚Äù for different $(\\mathbf{x},y)$\n    Minimizing $E_{\\text{in}}$ for Weighted Classification\n$$ {\\color{Red} E_{\\mathrm{in}}^{\\mathrm{w}}(h)} =\\frac{1}{N} \\sum_{n=1}^{N}\\left\\lbrace \\begin{array}{cc} {\\color{Purple} 1} \u0026 \\text { if } y_ n = +1 \\ {\\color{Orange} 1000} \u0026 \\text { if } y_ n = -1 \\end{array}\\right\\rbrace \\cdot{\\color{Blue} \\left[y_ n \\neq h\\left(\\mathbf{x}_ n\\right)\\right]} $$\n Na√Øve Thoughts PLA: doesn‚Äôt matter if linear separable  pocket: modify pocket-replacement rule  if $\\mathbf{w}_ {t+1}$ reaches smaller $ E_ {\\mathrm{in}}^{\\mathrm{w}}$ than $\\hat{\\mathbf{w}}$, replace $\\hat{\\bf w}$ by $\\mathbf{w}_ {t+1}$     But  pocket: some guarantee on $ E_{\\mathrm{in}}^{0/1}$ modified pocket: similar guarantee on $ E_{\\mathrm{in}}^{\\mathrm{w}}$?      Systematic Route: Connect $E_{\\mathrm{in}}^{\\mathrm{w}}$ and $E_{\\mathrm{in}}^{\\mathrm{0/1}}$\n after copying ‚Äú-1‚Äù examples 1,000 times, $E_{\\mathrm{in}}^{\\mathrm{w}} \\text{ for LHA} \\equiv E_{\\mathrm{in}}^{\\mathrm{0/1}} \\text{ for RHS}$!    Weighted Pocket Algorithm\n using ‚Äúvirtual copying‚Äù, weighted pocket algorithm include:  weighted PLA:  randomly check ‚Äú-1‚Äù example mistakes with 1000 times more probability   weighted pocket replacement:  if $\\mathbf{w}_ {t+1}$ reaches smaller $ E_ {\\mathrm{in}}^{\\mathrm{w}}$ than $\\hat{\\mathbf{w}}$, replace $\\hat{\\bf w}$ by $\\mathbf{w}_ {t+1}$     systematic route (called ‚Äúreduction‚Äù):  can be applied to many other algorithms!      ",
  "wordCount" : "2904",
  "inLanguage": "en",
  "datePublished": "2020-07-02T00:00:00Z",
  "dateModified": "2020-07-02T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "jifan"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jifan.tech/posts/2020-07-02-mlf-mf-ntu-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jifan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jifan.tech/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jifan.tech" accesskey="h" title="Jifan&#39;s Blog (Alt + H)">Jifan&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jifan.tech/archives/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://jifan.tech/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://jifan.tech/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?
    </h1>
    <div class="post-meta"><span title='2020-07-02 00:00:00 +0000 UTC'>July 2, 2020</span>&nbsp;¬∑&nbsp;jifan

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#5-training-versus-testing" aria-label="5 Training versus Testing">5 Training versus Testing</a><ul>
                        <ul>
                        
                <li>
                    <a href="#51-recap-and-preview" aria-label="5.1 Recap and Preview">5.1 Recap and Preview</a></li>
                <li>
                    <a href="#52-effective-number-of-lines" aria-label="5.2 Effective Number of Lines">5.2 Effective Number of Lines</a></li>
                <li>
                    <a href="#53-effective-number-of-hypothesis" aria-label="5.3 Effective Number of Hypothesis">5.3 Effective Number of Hypothesis</a></li>
                <li>
                    <a href="#54-break-point" aria-label="5.4 Break Point">5.4 Break Point</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#6-theory-of-generalization" aria-label="6 Theory of Generalization">6 Theory of Generalization</a><ul>
                        <ul>
                        
                <li>
                    <a href="#61-restiction-of-break-point" aria-label="6.1 Restiction of Break Point">6.1 Restiction of Break Point</a></li>
                <li>
                    <a href="#62-bounding-function-basic-cases" aria-label="6.2 Bounding Function: Basic Cases">6.2 Bounding Function: Basic Cases</a></li>
                <li>
                    <a href="#63-bounding-function-inductive-cases" aria-label="6.3 Bounding Function: Inductive Cases">6.3 Bounding Function: Inductive Cases</a></li>
                <li>
                    <a href="#64-a-pictorial-proof" aria-label="6.4 A Pictorial Proof">6.4 A Pictorial Proof</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#7-the-vc-dimension" aria-label="7 The VC Dimension">7 The VC Dimension</a><ul>
                        <ul>
                        
                <li>
                    <a href="#71-definition-of-vc-dimension" aria-label="7.1 Definition of VC Dimension">7.1 Definition of VC Dimension</a></li>
                <li>
                    <a href="#72-vc-dimension-of-perceptrons" aria-label="7.2 VC Dimension of Perceptrons">7.2 VC Dimension of Perceptrons</a></li>
                <li>
                    <a href="#73-physical-intuition-of-vc-dimension" aria-label="7.3 Physical Intuition of VC Dimension">7.3 Physical Intuition of VC Dimension</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#8-noise-and-error" aria-label="8 Noise and Error">8 Noise and Error</a><ul>
                        <ul>
                        
                <li>
                    <a href="#81-noise-and-probabilistic-target" aria-label="8.1 Noise and Probabilistic Target">8.1 Noise and Probabilistic Target</a></li>
                <li>
                    <a href="#82-error-measure" aria-label="8.2 Error Measure">8.2 Error Measure</a></li>
                <li>
                    <a href="#83-algorithmic-error-measure" aria-label="8.3 Algorithmic Error Measure">8.3 Algorithmic Error Measure</a></li>
                <li>
                    <a href="#84-weighted-classification" aria-label="8.4 Weighted Classification">8.4 Weighted Classification</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Course Link Ôºö</p>
<ul>
<li><a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/week/5">Week 5 - Training versus Testing</a></li>
<li><a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/week/6">Week 6 - Theory of Generalization</a></li>
<li><a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/week/7">Week 7 - The VC Dimension</a></li>
<li><a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/week/8">Week 8 - Noise and Error</a></li>
</ul>
</blockquote>
<h2 id="5-training-versus-testing">5 Training versus Testing<a hidden class="anchor" aria-hidden="true" href="#5-training-versus-testing">#</a></h2>
<blockquote>
<p>What we pay in choosing hypotheses during training: the growth function for representing effective number of choices &hellip;</p>
</blockquote>
<!-- raw HTML omitted -->
<h4 id="51-recap-and-preview">5.1 Recap and Preview<a hidden class="anchor" aria-hidden="true" href="#51-recap-and-preview">#</a></h4>
<ul>
<li>
<p>Recap: the <em>statistical</em> learning flow</p>
<ul>
<li>if $\mathcal{H} = M$ finite, $N$ large enough
<ul>
<li>for whatever $g$ picked by $\cal A$, $E_{out}(g) \approx E_{in}(g)$</li>
</ul>
</li>
<li>if $\cal A$ finds one $g$ with $E_{in}(g) \approx 0$
<ul>
<li>PAC guarantee for $E_{out}(g) \approx 0$ $\Rightarrow$ learning possible</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Two central questions</p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/02/oS3QdmTfN6WhCxv.png" alt="image-20200702145131269"  />
</p>
<ul>
<li>learning split to two central questions:
<ul>
<li>can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$?</li>
<li>can we make $E_{in}(g)$ small enough?</li>
</ul>
</li>
<li>what role dose $M$ ($\vert \cal H \vert$) play for the two questions?</li>
</ul>
</li>
<li>
<p>Trade-off on $M$</p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/02/XrAy6CfvUnBFSKO.png" alt="image-20200702145507168"  />
</p>
<ul>
<li>using the right $M$ (or $\cal H$) is important</li>
<li>$M = \infty$ doomed?</li>
</ul>
</li>
<li>
<p>Preview</p>
<ul>
<li>
<p>Known</p>
<p>$$
\mathbb{P}\left[\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert &gt;\epsilon\right] \leq 2 \cdot M \cdot \exp \left(-2 \epsilon^{2} N\right)
$$</p>
</li>
<li>
<p>Todo</p>
<ul>
<li>
<p>establish <strong>a finite quantity</strong> that replaces $M$</p>
<p>$$
\mathbb{P}\left[\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert &gt;\epsilon\right] \mathop{\leq}^{?} 2 \cdot m_{\mathcal{H}} \cdot \exp \left(-2 \epsilon^{2} N\right)
$$</p>
</li>
<li>
<p>justify the feasibility of learning for infinite $M$</p>
</li>
<li>
<p>study $m_{\cal H}$ to understand its trade-off for <em>right</em> $\cal H$, just like $M$</p>
</li>
</ul>
</li>
<li>
<p>mysterious PLA to be fully resolved <!-- raw HTML omitted -->after 3 more lectures<!-- raw HTML omitted --> üéâ</p>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="52-effective-number-of-lines">5.2 Effective Number of Lines<a hidden class="anchor" aria-hidden="true" href="#52-effective-number-of-lines">#</a></h4>
<ul>
<li>
<p>Where did $M$ come from?</p>
<p>$$
\mathbb{P}\left[\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert &gt;\epsilon\right] \leq 2 \cdot M \cdot \exp \left(-2 \epsilon^{2} N\right)
$$</p>
<ul>
<li>
<p><strong>BAD events</strong> $\mathcal{B}_m : \vert E_{in}(h_m) - E_{out}(h_m)\vert &gt; \epsilon$</p>
</li>
<li>
<p>to give $\cal A$ freedom of choice: bound $\Bbb{P}[\mathcal{B}_1 \text{ or } \cdots \text{ or } \mathcal{B}_M ]$</p>
</li>
<li>
<p>worst case: all $\mathcal{B}_m$ <strong>non-overlapping</strong></p>
<p>$$
\Bbb{P}[\mathcal{B}_1 \text{ or } \cdots \text{ or } \mathcal{B}<em>M ]
\underbrace{\le}</em>{\text{union bound}} \Bbb{P}[\mathcal{B}_1] + \cdots \Bbb{P}[\mathcal{B}_M]
$$</p>
</li>
<li>
<p>where did <strong>uniform bound fail</strong> to consider for $M = \infty$?</p>
</li>
</ul>
</li>
<li>
<p>Where did uniform bound fail?</p>
<p>$$
{\text{union bound }} \Bbb{P}[\mathcal{B}_1] + \cdots \Bbb{P}[\mathcal{B}_M]
$$</p>
<ul>
<li>
<p><strong>BAD events</strong> $\mathcal{B}_ m:\vert E_{in}(h_m) - E_{out}(h_m)\vert &gt; \epsilon$</p>
<ul>
<li>
<p>overlapping for similar hypotheses $h_1 \approx h_2$</p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<p>Why?</p>
<ul>
<li>$E_{out}(h_1) \approx E_{out}(h_2)$</li>
<li>for most $\cal D$, $E_{in}(h_1) = E_{in}(h_2)$</li>
</ul>
</li>
<li>
<p>union bound <strong>over-estimating</strong></p>
</li>
<li>
<p>to account for overlap, can we group similar hypotheses by <strong>kind</strong>?</p>
</li>
</ul>
</li>
<li>
<p>How many lines are there?</p>
<p>$$
\mathcal{H} = \lbrace \text{all lines in } \Bbb{R}^2 \rbrace
$$</p>
</li>
<li>
<p>Effective number of lines</p>
<table>
<thead>
<tr>
<th>$N$</th>
<th>effective $(N)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$1$</td>
<td>$2$</td>
</tr>
<tr>
<td>$2$</td>
<td>$4$</td>
</tr>
<tr>
<td>$3$</td>
<td>$8$</td>
</tr>
<tr>
<td>$4$</td>
<td>$14\  (&lt;2^N)$</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>maximum kinds of lines with respect to $N$ inputs $\mathbf{x}_1, \cdots, \mathbf{x}_N \Leftrightarrow$ <strong>effective numbers of lines</strong></p>
<ul>
<li>must be $\le 2^N$</li>
<li>finite <em>grouping</em> of infinitely-many lines $\in \cal H$</li>
<li>wish:</li>
</ul>
<p>$$
\mathbb{P}\left[\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert &gt;\epsilon\right] \leq 2 \cdot \text{effective}(N) \cdot \exp \left(-2 \epsilon^{2} N\right)
$$</p>
</li>
</ul>
</li>
<li>
<p>if</p>
<ul>
<li>$\text{effective}(N)$ can replace $M$ and
<ul>
<li>$\text{effective}(N) \ll 2^N$</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>learning possible with infinite lines.</strong></p>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="53-effective-number-of-hypothesis">5.3 Effective Number of Hypothesis<a hidden class="anchor" aria-hidden="true" href="#53-effective-number-of-hypothesis">#</a></h4>
<ul>
<li>
<p>Dichotomies: mini-hypothesis</p>
<p>$$
\mathcal{H} = \lbrace \text{hypothesis } h: \mathcal{X} \rightarrow \lbrace {\color{Red}{\times}},{\color{Blue}{\circ}}\rbrace \rbrace
$$</p>
<ul>
<li>
<p>call
$$
h(\mathbf{x}_1, \dots, \mathbf{x}_N) = \left( h(\mathbf{x}_1), \dots, h(\mathbf{x}_N) \right) \in \lbrace {\color{Red}{\times}},{\color{Blue}{\circ}}\rbrace^N
$$</p>
</li>
<li>
<p>a <strong>dichotomy</strong>: hypothesis <em>limited</em> to the eyes of $\mathbf{x}_1, \dots, \mathbf{x}_N$</p>
</li>
<li>
<p>$\mathcal{H}(\mathbf{x}_1, \dots, \mathbf{x}_N)$: <strong>all dichotomies implemented by $\cal H$ on $\mathbf{x}_1, \dots, \mathbf{x}_N$</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>hypothesis $\cal H$</th>
<th>dichotomies $\mathcal{H}(\mathbf{x}_1, \dots, \mathbf{x}_N)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>e.g.</td>
<td>all lines in $\Bbb{R}^2$</td>
<td>$\lbrace {\color{Blue}{\circ}}{\color{Blue}{\circ}}{\color{Blue}{\circ}}{\color{Blue}{\circ}},{\color{Blue}{\circ}}{\color{Blue}{\circ}}{\color{Blue}{\circ}}{\color{Red}{\times}},{\color{Blue}{\circ}}{\color{Blue}{\circ}}{\color{Red}{\times}}{\color{Red}{\times}},\dots\rbrace$</td>
</tr>
<tr>
<td>size</td>
<td>possibly infinite</td>
<td>upper bounded by $2^N$</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>$\vert \mathcal{H}(\mathbf{x}_1, \dots, \mathbf{x}_N) \vert$: candidate for <strong>replacing</strong> $M$</p>
</li>
</ul>
</li>
<li>
<p>Growth function</p>
<ul>
<li>
<p>$\vert \mathcal{H}(\mathbf{x}_1, \dots, \mathbf{x}_N) \vert$: depends on inputs $(\mathbf{x}_1, \dots, \mathbf{x}_N)$</p>
</li>
<li>
<p>growth function: remove dependence by <strong>taking $\max$ of all possible</strong> $(\mathbf{x}_1, \dots, \mathbf{x}_N)$</p>
<p>$$
m_{\cal H}(N) = \max_{\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathcal{X}} \vert \mathcal{H}(\mathbf{x}_1, \dots, \mathbf{x}_N) \vert
$$</p>
</li>
<li>
<p>finite, upper-bounded by $2^N$</p>
</li>
</ul>
</li>
<li>
<p>Growth function for positive rays</p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/02/syock3iRCM6nH54.png" alt="image-20200702155748213"  />
</p>
<ul>
<li>$\mathcal{X} = \Bbb{R}$ (one dimensional)</li>
<li>$\cal H$ contains $h$, where <strong>each $h(x) = \text{sign}(x-a)$ for threshold $a$</strong></li>
<li><em>positive half</em> of 1-D perceptrons</li>
<li>one dichotomy for $a \in$ each spot $(x_n, x_{n+1})$: $m_{\cal H}(N) = N + 1$</li>
<li>$(N+1) \ll 2^N$ when $N$ large!</li>
</ul>
</li>
<li>
<p>Growth function for positive intervals</p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/02/GCZYbNXk1hVtiKa.png" alt="image-20200702160043359"  />
</p>
<ul>
<li>
<p>$\mathcal{X} = \Bbb{R}$ (one dimensional)</p>
</li>
<li>
<p>$\cal H$ contains $h$, where <strong>each $h(x) = +1  \text{ iff } x \in [\mathcal{l}, r), -1$ otherwise</strong></p>
</li>
<li>
<p>one dichotomy for each <em>interval kind</em>:</p>
<p>$$
m_{\cal H}(N) = \underbrace{\frac{1}{2}N(N+1)}<em>{\text{interval ends in $N+1$ spots}} + \underbrace{1}</em>{\text{all }\times}
$$</p>
</li>
<li>
<p>$\frac{1}{2}N^2+\frac{1}{2}N+1 \ll 2^N$ when $N$ large!</p>
</li>
</ul>
</li>
<li>
<p>Growth function for convex sets</p>
<ul>
<li>
<p>$\mathcal{X} = \Bbb{R}^2$ (two dimensional)</p>
</li>
<li>
<p>$\cal H$ contains $h$, where <strong>each $h(x) = +1  \text{ iff $x$ in a convex region}, -1$ otherwise</strong></p>
</li>
<li>
<p>one possible set of $N$ inputs: $\mathbf{x}_1, \dots, \mathbf{x}_N$ on a big circle</p>
<!-- raw HTML omitted -->
</li>
<li>
<p><strong>every dichotomy can be implemented</strong> by $\cal H$ using a convex region slightly extended from contour of positive inputs: $m_{\cal H} = 2^N$</p>
</li>
<li>
<p>call this $N$ inputs <strong>shattered</strong> by $\cal H$</p>
</li>
<li>
<p>$m_{\cal H}(N) = 2^N \Leftrightarrow$ <strong>exists</strong> $N$ inputs that can be shattered</p>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="54-break-point">5.4 Break Point<a hidden class="anchor" aria-hidden="true" href="#54-break-point">#</a></h4>
<ul>
<li>
<p>The four growth functions</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/07/02/NqPrXF.png" alt="NqPrXF.png"  />
</p>
<ul>
<li>
<p>what if $m_{\cal H}(N)$ replaces $M$?</p>
<p>$$
\mathbb{P}\left[\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert &gt;\epsilon\right] \mathop{\leq}^{?} 2 \cdot m_{\mathcal{H}}(N) \cdot \exp \left(-2 \epsilon^{2} N\right)
$$</p>
<ul>
<li>polynomial: good üëç</li>
<li>exponential: bad üëé</li>
</ul>
</li>
<li>
<p>for 2D or general perceptron, $m_{\cal H}(N)$ polynomial (good)?</p>
</li>
</ul>
</li>
<li>
<p>Break point of $\cal H$</p>
<ul>
<li>what do we know about 2D perceptrons now?
<ul>
<li>three inputs: <em>exists</em> shatter</li>
<li>four inputs: <em>for all</em> no shatter</li>
</ul>
</li>
<li>if no $k$ inputs can be shattered by $\cal H$, call $k$ a <strong>break point</strong> for $\cal H$
<ul>
<li>$m_{\cal H} &lt; 2^k$</li>
<li>$k+1, k+2, \dots $ also break points!</li>
<li>will study minimum break point $k$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>The four beak points</p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/02/QGjbxpo7s1tLczf.png" alt="image-20200702162231928"  />
</p>
<ul>
<li>conjecture:
<ul>
<li>no break point: $m_{\cal H} = 2^N$ (sure!)</li>
<li>break point $k$: $m_{\cal H} = O(N^{k-1})$
<ul>
<li>exited? wait for next lecture ;-)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="6-theory-of-generalization">6 Theory of Generalization<a hidden class="anchor" aria-hidden="true" href="#6-theory-of-generalization">#</a></h2>
<blockquote>
<p>Test error can approximate training error if there is enough data and growth function does not grow too fast &hellip;</p>
</blockquote>
<!-- raw HTML omitted -->
<h4 id="61-restiction-of-break-point">6.1 Restiction of Break Point<a hidden class="anchor" aria-hidden="true" href="#61-restiction-of-break-point">#</a></h4>
<ul>
<li>
<p>The four break points</p>
<!-- raw HTML omitted -->
<ul>
<li>growth function $m_{\cal H}(N)$: max number of dichotomies</li>
<li>break point $k \Rightarrow$ break point $k + 1, \dots$</li>
<li>what else?</li>
</ul>
</li>
<li>
<p>Restriction of break point</p>
<ul>
<li>what <em>must be true</em> when <strong>minimum break point</strong> $k = 2$
<ul>
<li>$N = 1$: every $m_{\cal H}(N) = 2$ by definition</li>
<li>$N = 2$: every $m_{\cal H}(N) &lt; 4$ by definition
<ul>
<li>(so maximum possible = 3)</li>
</ul>
</li>
<li>$N = 3$: maximum possible $= 4 \ll 2^3$</li>
</ul>
</li>
<li>break point $k$ <strong>restricts maximum possible $m_{\cal H}(N)$ a lot</strong> for $N &gt; k$</li>
</ul>
<p><img loading="lazy" src="https://i.loli.net/2020/07/02/PbDGstTUkVueK3m.png" alt="image-20200702172002753"  />
</p>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="62-bounding-function-basic-cases">6.2 Bounding Function: Basic Cases<a hidden class="anchor" aria-hidden="true" href="#62-bounding-function-basic-cases">#</a></h4>
<ul>
<li><strong>Bounding function</strong> $B(N, k)$: maximum possible $m_{\cal H}(N)$ when break point $= k$
<ul>
<li>combinatorial quantity:
<ul>
<li>maximum number of length-$N$ vectors with $({\color{Red}{\times}},{\color{Blue}{\circ}})$ while <strong>&ldquo;no shatter&rdquo; any length-$k$</strong> subvectors</li>
</ul>
</li>
<li>irrelevant of the details of $\cal H$
<ul>
<li>e.g. $B(N, 3)$ bounds both
<ul>
<li>positive intervals ($k = 3$)</li>
<li>1D perceptrons ($k = 3$)</li>
</ul>
</li>
</ul>
</li>
<li>new goal: $B(N, k) \le \text{poly}(N)$ ?</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="63-bounding-function-inductive-cases">6.3 Bounding Function: Inductive Cases<a hidden class="anchor" aria-hidden="true" href="#63-bounding-function-inductive-cases">#</a></h4>
<ul>
<li>
<p>Table of bounding funciton</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/07/02/Nq75KP.png" alt="Nq75KP.png"  />
</p>
<ul>
<li>Known
<ul>
<li>$B(N, 1) = 1$ (see previous quiz)</li>
<li>$B(N, k) = 2^N \text{ for } N &lt; k$</li>
<li>$B(N, k) = 2^N-1 \text{ for } N = k$</li>
</ul>
</li>
<li>$B(N, k) \le B(N-1, k) +B(N-1, k-1)$
<ul>
<li>(actually &ldquo;$=$&rdquo;)</li>
</ul>
</li>
<li>Now have <strong>upper bound</strong> of bounding function</li>
</ul>
</li>
<li>
<p>Bounding function: the theorem</p>
<p>$$
B(N, k) \leq \underbrace{\sum_{i=0}^{k-1}\dbinom{N}{i}}_{\text{highest term }N^{k-1}}
$$</p>
<ul>
<li>simple induction using <strong>boundary and inductive formula</strong></li>
<li>for fixed $k$, $B(N, k)$ upper bounded by $\text{poly}(N)$
<ul>
<li>$\Rightarrow m_{\cal H}(N)$ is $\text{poly}(N)$ if break points exists</li>
</ul>
</li>
<li>&ldquo;$\le$&rdquo; can be &ldquo;$=$&rdquo; actually</li>
</ul>
</li>
<li>
<p>The three break points</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/07/02/Nq7Du6.png" alt="Nq7Du6.png"  />
</p>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="64-a-pictorial-proof">6.4 A Pictorial Proof<a hidden class="anchor" aria-hidden="true" href="#64-a-pictorial-proof">#</a></h4>
<ul>
<li>
<p>BAD Bound for General $\cal H$</p>
<ul>
<li>
<p>want:</p>
<p>$$
\mathbb{P}\left[\exists h \in \mathcal{H} \text { s.t. }\left\vert E_{\text {in }}(h)-E_{\text {out }}(h)\right\vert &gt;\epsilon\right] \leq 2 \cdot \quad {\color{Orange}{m_{\mathcal{H}}(N)}} \cdot \exp \left(-2 \quad \epsilon^{2} N\right)
$$</p>
</li>
<li>
<p>actually, <strong>when $N$ is large enough:</strong></p>
<p>$$
\mathbb{P}\left[\exists h \in \mathcal{H} \text { s.t. }\left\vert E_{\text {in }}(h)-E_{\text {out }}(h)\right\vert &gt;\epsilon\right] \leq 2 \cdot {\color{Red}{2}} {\color{Orange}{m_{\mathcal{H}}({\color{Blue}{2}}N)}} \cdot \exp \left(-2 \cdot {\color{Purple}{\frac{1}{16}}} \epsilon^{2} N\right)
$$</p>
</li>
<li>
<p>next: <strong>sketch</strong> of proof</p>
</li>
</ul>
</li>
<li>
<p>Step 1: <strong>Replace $E_{out}$ by $E_{in}^{\prime}$</strong></p>
<p>$$
\begin{aligned}
&amp; \frac{1}{2} \mathbb{P}\left[\exists h \in \mathcal{H} \text { s.t. }\left\vert E_{\text {in}}(h)-E_{\text {out}}(h)\right\vert &gt;\epsilon\right] \newline
\leq &amp; \  \mathbb{P}\left[\exists h \in \mathcal{H} \text { s.t. }\left\vert E_{\text {in}}(h)- {\color{Red}{E_{\text {in}}^{\prime}(h)}} \right\vert &gt; {\color{Red}{\frac{\epsilon}{2}}} \right]
\end{aligned}
$$</p>
<ul>
<li>
<p>$E_{\text {in}}(h)$ finitely many, $E_{\text {out}}(h)$ infinitely many</p>
<ul>
<li>replace the evil $E_{\text {out}}(h)$ first</li>
</ul>
</li>
<li>
<p>how? sample <strong>verification set</strong> $\cal D^{\prime}$ of size $N$ to calculate $E_{\text {in}}^{\prime}$</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>BAD $h$ of $E_{\text {in}} - E_{\text {out}}$</p>
<ul>
<li>$\stackrel{\text { probably }}{\Longrightarrow}$ BAD $h$ of $E_{\text {in}}  - E_{\text {in}}^{\prime}$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Step 2: Decompose $\cal H$ by Kind</p>
<p>$$
\begin{aligned}
\mathrm{BAD} &amp; \leq {\color{Red}{2}} \mathbb{P}\left[\exists h \in \mathcal{H} \text { s.t. }\left\vert E_{\text {in }}(h)- {\color{Red}{E_{\text {in }}^{\prime}(h)}} \right\vert &gt;{\color{Red}{\frac{\epsilon}{2}}}\right] \
&amp; \leq {\color{Red}{2}} m_{\mathcal{H}}({\color{Blue}{2}} N) \mathbb{P}\left[{\color{Blue}{\text { fixed }}} h \text { s.t. }\left\vert E_{\text {in }}(h)-{\color{Red}{E_{\text {in }}^{\prime}(h)}} \right\vert &gt;{\color{Red}{\frac{\epsilon}{2}}}\right]
\end{aligned}
$$</p>
<ul>
<li>$E_{\text {in}}$ with $\cal D$, $E_{\text {in}}^{\prime}$ with $\cal D^{\prime}$
<ul>
<li>now $m_{\cal H}$ comes to play</li>
</ul>
</li>
<li>how? infinite $\cal H$ becomes $\vert \mathcal{H}(\mathbf{x}_1, \dots, \mathbf{x}_N)\vert $ kinds</li>
<li>union bound on $m_{\cal H}(2N)$ kinds</li>
</ul>
</li>
<li>
<p>Step 3: Use Hoeffding without Replacement</p>
<p>$$
\begin{aligned}
\mathrm{BAD} &amp; \leq {\color{Red}{2}} m_{\mathcal{H}}({\color{Blue}{2}} N) \mathbb{P}\left[{\color{Blue}{\text { fixed }}} h \text { s.t. }\left\vert E_{\text {in }}(h)-{\color{Red}{E_{\text {in }}^{\prime}(h)}} \right\vert &gt;{\color{Red}{\frac{\epsilon}{2}}}\right] \
&amp; \leq {\color{Red}{2}} m_{\mathcal{H}}({\color{Blue}{2}} N) \cdot 2 \exp \left(-2\left({\color{Purple}{\frac{\epsilon}{4}}}\right)^{2} N\right)
\end{aligned}
$$</p>
<ul>
<li>consider bin of $2N$ examples, choose $N$ for $E_{\text{in}}$,leave others for $E_{\text{in}}^{\prime}$
<ul>
<li>$\vert E_{\text{in}} - E_{\text{in}}^{\prime} \vert &gt; \frac{\epsilon}{2} \Leftrightarrow \left \vert E_{\text{in}} - \frac{E_{\text{in}}+E_{\text{in}}^{\prime}}{2} \right\vert &gt; \frac{\epsilon}{4}$</li>
</ul>
</li>
<li>so? just <em>similar bin</em>, <em>smaller $\epsilon$</em>, and Hoeffding without replacement</li>
</ul>
</li>
<li>
<p>That&rsquo;s All! $\Rightarrow$ <strong>Vapnik-Chervonenkis( VC) bound</strong>:</p>
<p>$$
\begin{aligned}
&amp;\ \mathbb{P}\left[\exists h \in \mathcal{H} \text { s.t. }\left\vert E_{\text {in }}(h)-E_{\text {out }}(h)\right\vert &gt;\epsilon\right] \newline \leq &amp; \ {\color{Red}{4}} {\color{Orange}{m_{\mathcal{H}}({\color{Blue}{2}}N)}} \cdot \exp \left(-  {\color{Purple}{\frac{1}{8}}} \epsilon^{2} N\right)
\end{aligned}
$$</p>
<ul>
<li>replace $E_{\text{out}}$ by $E_{\text{in}}^{\prime}$</li>
<li>decompose $\cal H$ by kind</li>
<li>use Hoeffding without replacement</li>
<li>2D perceptrons:
<ul>
<li>break point? 4</li>
<li>$m_{\cal H}(N)$? $O(N^3)$</li>
<li>learning with 2D perceptrons feasible! üéâ</li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="7-the-vc-dimension">7 The VC Dimension<a hidden class="anchor" aria-hidden="true" href="#7-the-vc-dimension">#</a></h2>
<blockquote>
<p>Learning happens if there is finite model complexity (called VC dimension), enough data, and low training error &hellip;</p>
</blockquote>
<!-- raw HTML omitted -->
<h4 id="71-definition-of-vc-dimension">7.1 Definition of VC Dimension<a hidden class="anchor" aria-hidden="true" href="#71-definition-of-vc-dimension">#</a></h4>
<ul>
<li>
<p>Recap: More on Growth Function</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/07/02/Nqv9yj.png" alt="Nqv9yj.png"  />
</p>
</li>
<li>
<p>Recap: More on Vapnik-Chervonenkis(VC) Bound</p>
<p><img loading="lazy" src="https://s1.ax1x.com/2020/07/02/Nqvl01.png" alt="Nqvl01.png"  />
</p>
<ul>
<li>if:
<ul>
<li>‚ë† $m_{\cal H}(N)$ breaks at $k$  (good $\cal H$)</li>
<li>‚ë° $N$ large enough (good $\cal D$)</li>
</ul>
</li>
<li>$\Rightarrow$ <strong>probably</strong> generalized $E_{out} \approx E_{in}$,</li>
<li>and if:
<ul>
<li>‚ë¢ $\cal A$ picks a $g$ with small $E_{in}$ (good $\cal A$)</li>
</ul>
</li>
<li>$\Rightarrow$ <strong>probably</strong> learned! üéâ</li>
</ul>
</li>
<li>
<p>VC Dimension</p>
<ul>
<li>
<p>the formal name of **maximum non-**break point</p>
</li>
<li>
<p>Definition: VC dimension of $\cal H$, denoted $d_{\text{VC}}(\cal H)$ is</p>
<p>$$
\text{largest } N \text{ for which } m_{\cal H}(N) = 2^N
$$</p>
<ul>
<li>the <strong>most</strong> inputs $\cal H$ that can shatter</li>
<li>$d_{\text{VC}} = \min(k) - 1$</li>
</ul>
</li>
<li>
<p>$N \leq d_{\text{VC}} \Rightarrow$</p>
<ul>
<li>$\cal H$ can shatter some $N$ inputs</li>
</ul>
</li>
<li>
<p>$N &gt; d_{\text{VC}} \Rightarrow$</p>
<ul>
<li>$N$ is a break point for $\cal H$</li>
</ul>
</li>
<li>
<p>$\text{if } N \ge 2, d_{\text{VC}} \ge 2, m_{\cal H} \leq N^{d_{\text{VC}}}$</p>
</li>
</ul>
</li>
<li>
<p>The Four VC Dimensions</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>VC Dimension and Learning</p>
<ul>
<li><strong>good</strong> $\Rightarrow$ <strong>finite $d_{\text{VC}}$</strong> $\Rightarrow$ $g$ <strong>&ldquo;will&rdquo; generalize</strong> $(E_{out}(g) \approx E_{in}(g))$</li>
<li>regardless of learning algorithm $\cal A$</li>
<li>regardless of input distribution $P$</li>
<li>regardless of target function $f$</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="72-vc-dimension-of-perceptrons">7.2 VC Dimension of Perceptrons<a hidden class="anchor" aria-hidden="true" href="#72-vc-dimension-of-perceptrons">#</a></h4>
<ul>
<li>
<p>2D PLA Revisited</p>
<!-- raw HTML omitted -->
<ul>
<li>general PLA for $\mathbf{x}$ with <strong>more that 2 features</strong>?</li>
</ul>
</li>
<li>
<p>VC Dimension of Perceptrons</p>
<ul>
<li>1D perceptron (pos/neg rays): $d_{\text{VC}} = 2$</li>
<li>2D perceptron: $d_{\text{VC}} = 3$</li>
<li>$d$-D perceptrons: $d_{\text{VC}} \mathop{=}^{?}d+1$</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>$d_{\text{VC}} \ge d + 1$</p>
<ul>
<li>
<p>There are <strong>some</strong> $d+1$ <strong>inputs</strong> we can shatter</p>
</li>
<li>
<p>some &ldquo;trivial&rdquo; inputs:</p>
<p>$$
\mathbf{X}=\left[\begin{array}{c}
-\mathbf{x}<em>{1}^{T}- \
-\mathbf{x}</em>{2}^{T}- \
-\mathbf{x}<em>{3}^{T}- \
\vdots \
-\mathbf{x}</em>{d+1}^{T}-
\end{array}\right]=\left[\begin{array}{ccccc}
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \
1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \
1 &amp; 0 &amp; 1 &amp; &amp; 0 \
\vdots &amp; \vdots &amp; &amp; \ddots &amp; 0 \
1 &amp; 0 &amp; \cdots &amp; 0 &amp; 1
\end{array}\right]
$$</p>
</li>
</ul>
</li>
<li>
<p>note: $\mathbf{X}$ invertible</p>
</li>
<li>
<p>to shatter &hellip;</p>
<ul>
<li>for any $\bf y \in \Bbb{R}^{d+1}$, find $\mathbf{w}$ such that
<ul>
<li>$\text{sign}(\mathbf{Xw})=\mathbf{y} \Leftarrow \bf{Xw=y} \stackrel{X\text{ invertible!}}{\Longleftrightarrow} w= X^{-1}y $</li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>$d_{\text{VC}} \ge d+1$</p>
<ul>
<li>
<p>A 2D special case</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>$d$-D general case</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>general $\bf X$ no-shatter $\Rightarrow d_{\text{VC}}\le d+1$</p>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="73-physical-intuition-of-vc-dimension">7.3 Physical Intuition of VC Dimension<a hidden class="anchor" aria-hidden="true" href="#73-physical-intuition-of-vc-dimension">#</a></h4>
<ul>
<li>
<p>Degrees of Freedom</p>
<ul>
<li>hypothesis parameters $\mathbf{w}=(w_0, w_1, \dots, w_d)$: creates degrees of freedom</li>
<li>hypothesis quantity $M = \vert \mathcal{H} \vert$: &ldquo;analog&rdquo; degrees of freedom</li>
<li>hypothesis <em>power</em> $d_{\text{VC}}=d+1$: <strong>effective &ldquo;binary&rdquo; degrees of freedom</strong></li>
<li>$d_{\text{VC}}(\cal H)$: powerfulness of $\cal H$</li>
</ul>
</li>
<li>
<p>Two Old Friends</p>
<ul>
<li>
<p>Positive rays ($d_{\text{VC}}=1$)</p>
<!-- raw HTML omitted -->
<ul>
<li>free parameters: $a$</li>
</ul>
</li>
<li>
<p>Positive intervals ($d_{\text{VC}}=2$):</p>
<!-- raw HTML omitted -->
<ul>
<li>free parameters: $\mathcal{l}, r$</li>
</ul>
</li>
<li>
<p>pratical rule of thumb: $d_{\text{VC}} \approx \text{#free parameters}$  (but not always)</p>
</li>
</ul>
</li>
<li>
<p>$M$ and $d_{\text{VC}}$</p>
<!-- raw HTML omitted -->
<ul>
<li>using the right $d_{\text{VC}}$ (or $\cal H$) is important</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>VC Bound Rephrase: Penalty for Model Complexity</p>
<ul>
<li>
<p>For any ${\color{Red}{g}={\color{Red}{\cal A}}}({\color{Blue}{\cal D}}) \in \mathcal {\color{Orange}{H}}$ and statistical large $\cal D$, for $d_{\text{VC}} \ge 2$:</p>
<p>$$
{\mathbb{P}_ {\color{Blue} \mathcal{D} } } [\underbrace{\left\vert E_{\text {in }}({\color{Red} g} )-E_{\text {out }}({\color{Red} g} )\right\vert &gt;\epsilon }_ { \mathbf{BAD} } ] \leq \underbrace{ {4} {\color{Orange} (2 N)^{d_\text {VC }}}  \exp \left(-\frac{1}{8} \epsilon^{2} {\color{Blue} N} \right)}_ {\color{Purple} \delta}
$$</p>
</li>
<li>
<p>Rephrase: with prbability $\ge 1 - \delta$, <strong>GOOD:</strong> $\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert \leq \epsilon$</p>
<p>$$
\begin{aligned}
\operatorname{set} \quad {\color{Purple} \delta}  &amp;=4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}}  \exp \left(-\frac{1}{8} \epsilon^{2} {\color{Blue} N} \right) \
\frac{\color{Purple} \delta }{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}} } &amp;=\exp \left(-\frac{1}{8} \epsilon^{2} {\color{Blue} N} \right) \
\ln \left(\frac{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}} }}{\color{Purple} \delta }\right) &amp;=\frac{1}{8} \epsilon^{2} {\color{Blue} N}  \
\sqrt{\frac{8}{\color{Blue} N } \ln \left(\frac{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}} }{\color{Purple} \delta }\right)} &amp;=\epsilon
\end{aligned}
$$</p>
</li>
<li>
<p>generalization error $\left\vert E_{\text {in }}(g)-E_{\text {out }}(g)\right\vert  \le \sqrt{\frac{8}{\color{Blue} N } \ln \left(\frac{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}} }{\color{Purple} \delta }\right)} $</p>
</li>
<li>
<p>$E_{\text {in }}(g)- \sqrt{\frac{8}{\color{Blue} N}  \ln \left(\frac{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}} }{\color{Purple} \delta} \right)} \le E_{\text {out }}(g) \le E_{\text{in}}(g) + \sqrt{\frac{8}{\color{Blue} N}  \ln \left(\frac{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}} }{\color{Purple} \delta} \right)}$</p>
</li>
<li>
<p>$\underbrace{\sqrt{\cdots}}_{\Omega(N, \mathcal{H}, \delta)}$Ôºö penalty for <strong>model complexity</strong></p>
</li>
</ul>
</li>
<li>
<p>THE VC Message</p>
<ul>
<li>with a high probability, $E_{\text {out }}(g) \le E_{\text{in}}(g) + \underbrace{\sqrt{\frac{8}{\color{Blue} N}  \ln \left(\frac{4{\color{Orange} (2 N)^{d_{\mathrm{VC}}}} }{\color{Purple} \delta }\right)}}_{\Omega(N, \mathcal{H}, \delta)}$</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>powerful $\cal H$ not always good!</li>
</ul>
</li>
<li>
<p>VC Bound Rephrase: Sample Complexity</p>
<!-- raw HTML omitted -->
<ul>
<li>practical rule of thumb:
<ul>
<li>$N \approx 10 d_{\text{VC}}$ <strong>often enough!</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Looseness of VC Bound</p>
<ul>
<li>
<p>theory: $N \approx 10,000\ d_{\text{VC}}$; practice: $N \approx 10\ d_{\text{VC}}$</p>
</li>
<li>
<p>Why?</p>
<!-- raw HTML omitted -->
</li>
<li>
<p><strong>philosophical message</strong> of VC bound: important for improving ML</p>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="8-noise-and-error">8 Noise and Error<a hidden class="anchor" aria-hidden="true" href="#8-noise-and-error">#</a></h2>
<blockquote>
<p>Learning can still happen within a noisy environment and different error measures &hellip;</p>
</blockquote>
<!-- raw HTML omitted -->
<h4 id="81-noise-and-probabilistic-target">8.1 Noise and Probabilistic Target<a hidden class="anchor" aria-hidden="true" href="#81-noise-and-probabilistic-target">#</a></h4>
<ul>
<li>
<p>Recap: The Learning Flow</p>
<!-- raw HTML omitted -->
<ul>
<li>what if there is <strong>noise</strong>?</li>
</ul>
</li>
<li>
<p>Noise</p>
<ul>
<li>
<p>Probabilistic Marbles</p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<p>Target Distribution $P(y\vert\mathbf{x})$: characterizes behavior of &ldquo;mini-target&rdquo; on one $\bf x$</p>
<!-- raw HTML omitted -->
<ul>
<li>goal of learning:
<ul>
<li>predict <strong>ideal mini-target</strong> (w.r.t. $P(y\vert \mathbf{x})$) on <strong>often-seen inputs</strong> (w.r.t $P(\mathbf{x})$)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>The New Learning Flow</p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/03/fcu8DHgaSNqC6vY.png" alt="image-20200703103311023"  />
</p>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="82-error-measure">8.2 Error Measure<a hidden class="anchor" aria-hidden="true" href="#82-error-measure">#</a></h4>
<ul>
<li>
<p>Error Measure: final hypothesis $g \approx f$</p>
<ul>
<li>
<p>how well? previously, considered out-of-sample measure</p>
<p>$$
E_{\text {out }}(g)=\underset{\mathbf{x} \sim P}{\mathcal{E}}[\mathbf{g}(\mathbf{x}) \neq f(\mathbf{x})]
$$</p>
</li>
<li>
<p>more generally, <strong>error measure</strong> $E(g, f)$</p>
</li>
<li>
<p>naturally considered</p>
<ul>
<li>out-of-sample: averaged over unknown $\bf x$</li>
<li>pointwise: evaluated on one $\bf x$</li>
<li>classification: $[\text{prediction} \ne \text{target}]$</li>
</ul>
</li>
<li>
<p>classification error often also called <strong>&ldquo;0/1 error&rdquo;</strong></p>
</li>
</ul>
</li>
<li>
<p>Pointwise Error Measure</p>
<ul>
<li>
<p>can often express $E(g,f)$ = averaged $\mathop{\text{err}}(g(\mathbf{x}),f(\mathbf{x}))$, like</p>
<p>$$
E_{\text {out }}(g)=\underset{\mathbf{x} \sim P}{\mathcal{E}}, \underbrace{[g(\mathbf{x}) \neq f(\mathbf{x})]}_{\operatorname{err}(g(\mathbf{x}), f(\mathbf{x}))}
$$</p>
<ul>
<li>$\text{err}$: called <strong>pointwise error measure</strong></li>
</ul>
</li>
<li>
<p>in-sample:</p>
<p>$$
E_{\mathrm{in}}(g)=\frac{1}{N} \sum_{n=1}^{N} \operatorname{err}\left(g \left( \mathbf{x}<em>{n} \right), f \left(\mathbf{x}</em>{n}\right) \right)
$$</p>
</li>
<li>
<p>out-of-sample:</p>
<p>$$
E_{\text {out }}(g)=\underset{\mathbf{x} \sim P}{\mathcal{E}}, \operatorname{err}\left(g \left( \mathbf{x}<em>{n} \right), f \left(\mathbf{x}</em>{n}\right) \right)
$$</p>
</li>
<li>
<p>will mainly consider pointwise $\operatorname{err}$ for simplicity</p>
</li>
</ul>
</li>
<li>
<p>Two Important Pointwise Error Measure: $\operatorname{err}(g(\mathbf{x}), f(\mathbf{x})) = \operatorname{err}(\tilde{y}, y)$</p>
<ul>
<li>0/1 error: $\operatorname{err}(\tilde{y}, y)=[\tilde{y} \neq y]$
<ul>
<li>correct or incorrect?</li>
<li>often for <strong>classification</strong></li>
</ul>
</li>
<li>squared error: $\operatorname{err}(\tilde{y}, y)=(\tilde{y}-y)^{2}$
<ul>
<li>how far is $\tilde{y}$ from $y$?</li>
<li>often for <strong>regression</strong></li>
</ul>
</li>
<li>how does $\operatorname{err}$ &ldquo;guide&rdquo; learning?</li>
</ul>
</li>
<li>
<p>Ideal Mini-Target</p>
<ul>
<li>
<p>interplay between <strong>noise</strong> and <strong>error</strong></p>
<ul>
<li>$P(y\vert \mathbf{x})$ and $\text{err}$ define <strong>ideal mini-target</strong> $f(\mathbf{x})$</li>
</ul>
</li>
<li>
<p>e.g.</p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<p>Learning Flow with Error Measure</p>
<!-- raw HTML omitted -->
<ul>
<li>extended VC theory/&ldquo;philosophy&rdquo; works for most $\cal H$ and $\text{err}$</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="83-algorithmic-error-measure">8.3 Algorithmic Error Measure<a hidden class="anchor" aria-hidden="true" href="#83-algorithmic-error-measure">#</a></h4>
<ul>
<li>
<p>Choice of Error Measure</p>
<!-- raw HTML omitted -->
<ul>
<li>
<p>two types of error: <!-- raw HTML omitted -->false accept<!-- raw HTML omitted --> and <!-- raw HTML omitted -->false reject<!-- raw HTML omitted --></p>
<p><img loading="lazy" src="https://i.loli.net/2020/07/03/4bcHEIRo8MQNmAx.png" alt="image-20200703105815077"  />
</p>
</li>
<li>
<p>0/1 error penalizes both types <strong>equally</strong></p>
</li>
<li>
<p>Fingerprint verification for supermarket</p>
<!-- raw HTML omitted -->
<ul>
<li>supermarket: fingerprint for discount</li>
<li><!-- raw HTML omitted -->false reject<!-- raw HTML omitted -->: very unhappy customer, lose future business</li>
<li><!-- raw HTML omitted -->false accept<!-- raw HTML omitted -->: give away a minor discount, intruder left fingerprint</li>
</ul>
</li>
<li>
<p>Fingerprint verification for CIA</p>
<!-- raw HTML omitted -->
<ul>
<li>CIA: fingerprint for entrance</li>
<li><!-- raw HTML omitted -->false accept<!-- raw HTML omitted -->: very serious consequences!</li>
<li><!-- raw HTML omitted -->false reject<!-- raw HTML omitted -->: unhappy employee, but so what?</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Take-home Message for Now: $\text{err}$ is <strong>application/user-dependent</strong></p>
<ul>
<li>Algorithmic Error Measures $\widehat{\text{err}}$
<ul>
<li>true: just $\text{err}$</li>
<li>plausible:
<ul>
<li>0/1: minimum &ldquo;flipping noise&rdquo;‚ÄîNP-hard to optimize, remember?</li>
<li>squared: minimum Gaussian noise</li>
</ul>
</li>
<li>friendly: easy to optimize for $\cal A$
<ul>
<li>closed-form solution</li>
<li>convex objective function</li>
</ul>
</li>
</ul>
</li>
<li>$\widehat{\text{err}}$: more in next lectures</li>
</ul>
</li>
<li>
<p>Learning Flow with Algorithmic Error Measure</p>
<!-- raw HTML omitted -->
</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="84-weighted-classification">8.4 Weighted Classification<a hidden class="anchor" aria-hidden="true" href="#84-weighted-classification">#</a></h4>
<ul>
<li>
<p>Weighted Classification</p>
<ul>
<li>
<p>CIA cost (error, loss) matrix</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>out-of-sample</p>
<p>$$
E_{\text {out}}(h)= \underset{(\mathbf{x},y) \sim P}{\mathcal{E}}
\left\lbrace
\begin{array}{cc}
{\color{Purple} 1}  &amp; \text { if } y=+1 \
{\color{Orange} 1000}  &amp; \text { if } y=-1
\end{array}\right\rbrace
\cdot {\color{Blue} [y \neq h(\mathbf{x}) ] }
$$</p>
</li>
<li>
<p>in-sample</p>
<p>$$
E_{\text {in}}(h)
= \frac{1}{N} \sum_{n=1}^{N}
\left\lbrace \begin{array}{cc}
{\color{Purple} 1}  &amp; \text { if } y_ n =+1 \
{\color{Orange} 1000}  &amp; \text { if } y_ n =-1
\end{array} \right\rbrace
\cdot { \color{Blue} \left[ y _ n \neq h \left(\mathbf{x} _ n \right) \right] }
$$</p>
</li>
<li>
<p>weighted classification: different &ldquo;weight&rdquo; for different $(\mathbf{x},y)$</p>
</li>
</ul>
</li>
<li>
<p>Minimizing $E_{\text{in}}$ for Weighted Classification</p>
<p>$$
{\color{Red} E_{\mathrm{in}}^{\mathrm{w}}(h)}
=\frac{1}{N} \sum_{n=1}^{N}\left\lbrace \begin{array}{cc}
{\color{Purple} 1}  &amp; \text { if } y_ n = +1 \
{\color{Orange} 1000}  &amp; \text { if } y_ n = -1
\end{array}\right\rbrace \cdot{\color{Blue} \left[y_ n \neq h\left(\mathbf{x}_ n\right)\right]}
$$</p>
<ul>
<li>Na√Øve Thoughts</li>
<li>PLA: doesn‚Äôt matter if linear separable
<ul>
<li>pocket: modify <strong>pocket-replacement rule</strong>
<ul>
<li>if $\mathbf{w}_ {t+1}$ reaches smaller $ E_ {\mathrm{in}}^{\mathrm{w}}$ than $\hat{\mathbf{w}}$, replace $\hat{\bf w}$ by $\mathbf{w}_ {t+1}$</li>
</ul>
</li>
</ul>
</li>
<li>But
<ul>
<li>pocket: some guarantee on $ E_{\mathrm{in}}^{0/1}$</li>
<li>modified pocket: similar guarantee on $  E_{\mathrm{in}}^{\mathrm{w}}$?</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Systematic Route: Connect $E_{\mathrm{in}}^{\mathrm{w}}$ and $E_{\mathrm{in}}^{\mathrm{0/1}}$</p>
<!-- raw HTML omitted -->
<ul>
<li>after copying &ldquo;-1&rdquo; examples 1,000 times, $E_{\mathrm{in}}^{\mathrm{w}} \text{ for LHA} \equiv E_{\mathrm{in}}^{\mathrm{0/1}} \text{ for RHS}$!</li>
</ul>
</li>
<li>
<p>Weighted Pocket Algorithm</p>
<ul>
<li>using &ldquo;virtual copying&rdquo;, weighted pocket algorithm include:
<ul>
<li>weighted PLA:
<ul>
<li>randomly check &ldquo;-1&rdquo; example mistakes with 1000 times more probability</li>
</ul>
</li>
<li>weighted pocket replacement:
<ul>
<li>if $\mathbf{w}_ {t+1}$ reaches smaller $ E_ {\mathrm{in}}^{\mathrm{w}}$ than $\hat{\mathbf{w}}$, replace $\hat{\bf w}$ by $\mathbf{w}_ {t+1}$</li>
</ul>
</li>
</ul>
</li>
<li>systematic route (called &ldquo;reduction&rdquo;):
<ul>
<li><strong>can be applied to many other algorithms!</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jifan.tech/tags/ml/dl/">ML/DL</a></li>
      <li><a href="https://jifan.tech/tags/course-learning/">Course Learning</a></li>
      <li><a href="https://jifan.tech/tags/python/">Python</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://jifan.tech/posts/2020-06-30-mlf-mf-ntu-1/">
    <span class="title">Next Page ¬ª</span>
    <br>
    <span>Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhen Can Machines Learn?</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://jifan.tech">Jifan&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
