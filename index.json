[{"content":" Course Link Ôºö\n Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices \u0026hellip;\n 5.1 Recap and Preview   Recap: the statistical learning flow\n if $\\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\\cal A$, $E_{out}(g) \\approx E_{in}(g)$   if $\\cal A$ finds one $g$ with $E_{in}(g) \\approx 0$  PAC guarantee for $E_{out}(g) \\approx 0$ $\\Rightarrow$ learning possible      Two central questions\n learning split to two central questions:  can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$? can we make $E_{in}(g)$ small enough?   what role dose $M$ ($\\vert \\cal H \\vert$) play for the two questions?    Trade-off on $M$\n using the right $M$ (or $\\cal H$) is important $M = \\infty$ doomed?    Preview\n  Known\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \u0026gt;\\epsilon\\right] \\leq 2 \\cdot M \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n  Todo\n  establish a finite quantity that replaces $M$\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \u0026gt;\\epsilon\\right] \\mathop{\\leq}^{?} 2 \\cdot m_{\\mathcal{H}} \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n  justify the feasibility of learning for infinite $M$\n  study $m_{\\cal H}$ to understand its trade-off for right $\\cal H$, just like $M$\n    mysterious PLA to be fully resolved after 3 more lecturesüéâ\n    5.2 Effective Number of Lines   Where did $M$ come from?\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \u0026gt;\\epsilon\\right] \\leq 2 \\cdot M \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n  BAD events $\\mathcal{B}_m : \\vert E_{in}(h_m) - E_{out}(h_m)\\vert \u0026gt; \\epsilon$\n  to give $\\cal A$ freedom of choice: bound $\\Bbb{P}[\\mathcal{B}_1 \\text{ or } \\cdots \\text{ or } \\mathcal{B}_M ]$\n  worst case: all $\\mathcal{B}_m$ non-overlapping\n$$ \\Bbb{P}[\\mathcal{B}_1 \\text{ or } \\cdots \\text{ or } \\mathcal{B}M ] \\underbrace{\\le}{\\text{union bound}} \\Bbb{P}[\\mathcal{B}_1] + \\cdots \\Bbb{P}[\\mathcal{B}_M] $$\n  where did uniform bound fail to consider for $M = \\infty$?\n    Where did uniform bound fail?\n$$ {\\text{union bound }} \\Bbb{P}[\\mathcal{B}_1] + \\cdots \\Bbb{P}[\\mathcal{B}_M] $$\n  BAD events $\\mathcal{B}_ m:\\vert E_{in}(h_m) - E_{out}(h_m)\\vert \u0026gt; \\epsilon$\n  overlapping for similar hypotheses $h_1 \\approx h_2$\n    Why?\n $E_{out}(h_1) \\approx E_{out}(h_2)$ for most $\\cal D$, $E_{in}(h_1) = E_{in}(h_2)$    union bound over-estimating\n  to account for overlap, can we group similar hypotheses by kind?\n    How many lines are there?\n$$ \\mathcal{H} = \\lbrace \\text{all lines in } \\Bbb{R}^2 \\rbrace $$\n  Effective number of lines\n   $N$ effective $(N)$     $1$ $2$   $2$ $4$   $3$ $8$   $4$ $14\\ (\u0026lt;2^N)$      maximum kinds of lines with respect to $N$ inputs $\\mathbf{x}_1, \\cdots, \\mathbf{x}_N \\Leftrightarrow$ effective numbers of lines\n must be $\\le 2^N$ finite grouping of infinitely-many lines $\\in \\cal H$ wish:  $$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \u0026gt;\\epsilon\\right] \\leq 2 \\cdot \\text{effective}(N) \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n    if\n $\\text{effective}(N)$ can replace $M$ and  $\\text{effective}(N) \\ll 2^N$      learning possible with infinite lines.\n  5.3 Effective Number of Hypothesis   Dichotomies: mini-hypothesis\n$$ \\mathcal{H} = \\lbrace \\text{hypothesis } h: \\mathcal{X} \\rightarrow \\lbrace {\\color{Red}{\\times}},{\\color{Blue}{\\circ}}\\rbrace \\rbrace $$\n  call $$ h(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) = \\left( h(\\mathbf{x}_1), \\dots, h(\\mathbf{x}_N) \\right) \\in \\lbrace {\\color{Red}{\\times}},{\\color{Blue}{\\circ}}\\rbrace^N $$\n  a dichotomy: hypothesis limited to the eyes of $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$\n  $\\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$: all dichotomies implemented by $\\cal H$ on $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$\n    hypothesis $\\cal H$ dichotomies $\\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$     e.g. all lines in $\\Bbb{R}^2$ $\\lbrace {\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}},{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Red}{\\times}},{\\color{Blue}{\\circ}}{\\color{Blue}{\\circ}}{\\color{Red}{\\times}}{\\color{Red}{\\times}},\\dots\\rbrace$   size possibly infinite upper bounded by $2^N$      $\\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) \\vert$: candidate for replacing $M$\n    Growth function\n  $\\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) \\vert$: depends on inputs $(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$\n  growth function: remove dependence by taking $\\max$ of all possible $(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$\n$$ m_{\\cal H}(N) = \\max_{\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathcal{X}} \\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) \\vert $$\n  finite, upper-bounded by $2^N$\n    Growth function for positive rays\n $\\mathcal{X} = \\Bbb{R}$ (one dimensional) $\\cal H$ contains $h$, where each $h(x) = \\text{sign}(x-a)$ for threshold $a$ positive half of 1-D perceptrons one dichotomy for $a \\in$ each spot $(x_n, x_{n+1})$: $m_{\\cal H}(N) = N + 1$ $(N+1) \\ll 2^N$ when $N$ large!    Growth function for positive intervals\n  $\\mathcal{X} = \\Bbb{R}$ (one dimensional)\n  $\\cal H$ contains $h$, where each $h(x) = +1 \\text{ iff } x \\in [\\mathcal{l}, r), -1$ otherwise\n  one dichotomy for each interval kind:\n$$ m_{\\cal H}(N) = \\underbrace{\\frac{1}{2}N(N+1)}{\\text{interval ends in $N+1$ spots}} + \\underbrace{1}{\\text{all }\\times} $$\n  $\\frac{1}{2}N^2+\\frac{1}{2}N+1 \\ll 2^N$ when $N$ large!\n    Growth function for convex sets\n  $\\mathcal{X} = \\Bbb{R}^2$ (two dimensional)\n  $\\cal H$ contains $h$, where each $h(x) = +1 \\text{ iff $x$ in a convex region}, -1$ otherwise\n  one possible set of $N$ inputs: $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ on a big circle\n  every dichotomy can be implemented by $\\cal H$ using a convex region slightly extended from contour of positive inputs: $m_{\\cal H} = 2^N$\n  call this $N$ inputs shattered by $\\cal H$\n  $m_{\\cal H}(N) = 2^N \\Leftrightarrow$ exists $N$ inputs that can be shattered\n    5.4 Break Point   The four growth functions\n  what if $m_{\\cal H}(N)$ replaces $M$?\n$$ \\mathbb{P}\\left[\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \u0026gt;\\epsilon\\right] \\mathop{\\leq}^{?} 2 \\cdot m_{\\mathcal{H}}(N) \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) $$\n polynomial: good üëç exponential: bad üëé    for 2D or general perceptron, $m_{\\cal H}(N)$ polynomial (good)?\n    Break point of $\\cal H$\n what do we know about 2D perceptrons now?  three inputs: exists shatter four inputs: for all no shatter   if no $k$ inputs can be shattered by $\\cal H$, call $k$ a break point for $\\cal H$  $m_{\\cal H} \u0026lt; 2^k$ $k+1, k+2, \\dots $ also break points! will study minimum break point $k$      The four beak points\n conjecture:  no break point: $m_{\\cal H} = 2^N$ (sure!) break point $k$: $m_{\\cal H} = O(N^{k-1})$  exited? wait for next lecture ;-)        6 Theory of Generalization  Test error can approximate training error if there is enough data and growth function does not grow too fast \u0026hellip;\n 6.1 Restiction of Break Point   The four break points\n growth function $m_{\\cal H}(N)$: max number of dichotomies break point $k \\Rightarrow$ break point $k + 1, \\dots$ what else?    Restriction of break point\n what must be true when minimum break point $k = 2$  $N = 1$: every $m_{\\cal H}(N) = 2$ by definition $N = 2$: every $m_{\\cal H}(N) \u0026lt; 4$ by definition  (so maximum possible = 3)   $N = 3$: maximum possible $= 4 \\ll 2^3$   break point $k$ restricts maximum possible $m_{\\cal H}(N)$ a lot for $N \u0026gt; k$    6.2 Bounding Function: Basic Cases  Bounding function $B(N, k)$: maximum possible $m_{\\cal H}(N)$ when break point $= k$  combinatorial quantity:  maximum number of length-$N$ vectors with $({\\color{Red}{\\times}},{\\color{Blue}{\\circ}})$ while \u0026ldquo;no shatter\u0026rdquo; any length-$k$ subvectors   irrelevant of the details of $\\cal H$  e.g. $B(N, 3)$ bounds both  positive intervals ($k = 3$) 1D perceptrons ($k = 3$)     new goal: $B(N, k) \\le \\text{poly}(N)$ ?    6.3 Bounding Function: Inductive Cases   Table of bounding funciton\n Known  $B(N, 1) = 1$ (see previous quiz) $B(N, k) = 2^N \\text{ for } N \u0026lt; k$ $B(N, k) = 2^N-1 \\text{ for } N = k$   $B(N, k) \\le B(N-1, k) +B(N-1, k-1)$  (actually \u0026ldquo;$=$\u0026rdquo;)   Now have upper bound of bounding function    Bounding function: the theorem\n$$ B(N, k) \\leq \\underbrace{\\sum_{i=0}^{k-1}\\dbinom{N}{i}}_{\\text{highest term }N^{k-1}} $$\n simple induction using boundary and inductive formula for fixed $k$, $B(N, k)$ upper bounded by $\\text{poly}(N)$  $\\Rightarrow m_{\\cal H}(N)$ is $\\text{poly}(N)$ if break points exists   \u0026ldquo;$\\le$\u0026rdquo; can be \u0026ldquo;$=$\u0026rdquo; actually    The three break points\n  6.4 A Pictorial Proof   BAD Bound for General $\\cal H$\n  want:\n$$ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right\\vert \u0026gt;\\epsilon\\right] \\leq 2 \\cdot \\quad {\\color{Orange}{m_{\\mathcal{H}}(N)}} \\cdot \\exp \\left(-2 \\quad \\epsilon^{2} N\\right) $$\n  actually, when $N$ is large enough:\n$$ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right\\vert \u0026gt;\\epsilon\\right] \\leq 2 \\cdot {\\color{Red}{2}} {\\color{Orange}{m_{\\mathcal{H}}({\\color{Blue}{2}}N)}} \\cdot \\exp \\left(-2 \\cdot {\\color{Purple}{\\frac{1}{16}}} \\epsilon^{2} N\\right) $$\n  next: sketch of proof\n    Step 1: Replace $E_{out}$ by $E_{in}^{\\prime}$\n$$ \\begin{aligned} \u0026amp; \\frac{1}{2} \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in}}(h)-E_{\\text {out}}(h)\\right\\vert \u0026gt;\\epsilon\\right] \\newline \\leq \u0026amp; \\ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in}}(h)- {\\color{Red}{E_{\\text {in}}^{\\prime}(h)}} \\right\\vert \u0026gt; {\\color{Red}{\\frac{\\epsilon}{2}}} \\right] \\end{aligned} $$\n  $E_{\\text {in}}(h)$ finitely many, $E_{\\text {out}}(h)$ infinitely many\n replace the evil $E_{\\text {out}}(h)$ first    how? sample verification set $\\cal D^{\\prime}$ of size $N$ to calculate $E_{\\text {in}}^{\\prime}$\n  BAD $h$ of $E_{\\text {in}} - E_{\\text {out}}$\n $\\stackrel{\\text { probably }}{\\Longrightarrow}$ BAD $h$ of $E_{\\text {in}} - E_{\\text {in}}^{\\prime}$      Step 2: Decompose $\\cal H$ by Kind\n$$ \\begin{aligned} \\mathrm{BAD} \u0026amp; \\leq {\\color{Red}{2}} \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)- {\\color{Red}{E_{\\text {in }}^{\\prime}(h)}} \\right\\vert \u0026gt;{\\color{Red}{\\frac{\\epsilon}{2}}}\\right] \\ \u0026amp; \\leq {\\color{Red}{2}} m_{\\mathcal{H}}({\\color{Blue}{2}} N) \\mathbb{P}\\left[{\\color{Blue}{\\text { fixed }}} h \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-{\\color{Red}{E_{\\text {in }}^{\\prime}(h)}} \\right\\vert \u0026gt;{\\color{Red}{\\frac{\\epsilon}{2}}}\\right] \\end{aligned} $$\n $E_{\\text {in}}$ with $\\cal D$, $E_{\\text {in}}^{\\prime}$ with $\\cal D^{\\prime}$  now $m_{\\cal H}$ comes to play   how? infinite $\\cal H$ becomes $\\vert \\mathcal{H}(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)\\vert $ kinds union bound on $m_{\\cal H}(2N)$ kinds    Step 3: Use Hoeffding without Replacement\n$$ \\begin{aligned} \\mathrm{BAD} \u0026amp; \\leq {\\color{Red}{2}} m_{\\mathcal{H}}({\\color{Blue}{2}} N) \\mathbb{P}\\left[{\\color{Blue}{\\text { fixed }}} h \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-{\\color{Red}{E_{\\text {in }}^{\\prime}(h)}} \\right\\vert \u0026gt;{\\color{Red}{\\frac{\\epsilon}{2}}}\\right] \\ \u0026amp; \\leq {\\color{Red}{2}} m_{\\mathcal{H}}({\\color{Blue}{2}} N) \\cdot 2 \\exp \\left(-2\\left({\\color{Purple}{\\frac{\\epsilon}{4}}}\\right)^{2} N\\right) \\end{aligned} $$\n consider bin of $2N$ examples, choose $N$ for $E_{\\text{in}}$,leave others for $E_{\\text{in}}^{\\prime}$  $\\vert E_{\\text{in}} - E_{\\text{in}}^{\\prime} \\vert \u0026gt; \\frac{\\epsilon}{2} \\Leftrightarrow \\left \\vert E_{\\text{in}} - \\frac{E_{\\text{in}}+E_{\\text{in}}^{\\prime}}{2} \\right\\vert \u0026gt; \\frac{\\epsilon}{4}$   so? just similar bin, smaller $\\epsilon$, and Hoeffding without replacement    That\u0026rsquo;s All! $\\Rightarrow$ Vapnik-Chervonenkis( VC) bound:\n$$ \\begin{aligned} \u0026amp;\\ \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left\\vert E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right\\vert \u0026gt;\\epsilon\\right] \\newline \\leq \u0026amp; \\ {\\color{Red}{4}} {\\color{Orange}{m_{\\mathcal{H}}({\\color{Blue}{2}}N)}} \\cdot \\exp \\left(- {\\color{Purple}{\\frac{1}{8}}} \\epsilon^{2} N\\right) \\end{aligned} $$\n replace $E_{\\text{out}}$ by $E_{\\text{in}}^{\\prime}$ decompose $\\cal H$ by kind use Hoeffding without replacement 2D perceptrons:  break point? 4 $m_{\\cal H}(N)$? $O(N^3)$ learning with 2D perceptrons feasible! üéâ      7 The VC Dimension  Learning happens if there is finite model complexity (called VC dimension), enough data, and low training error \u0026hellip;\n 7.1 Definition of VC Dimension   Recap: More on Growth Function\n  Recap: More on Vapnik-Chervonenkis(VC) Bound\n if:  ‚ë† $m_{\\cal H}(N)$ breaks at $k$ (good $\\cal H$) ‚ë° $N$ large enough (good $\\cal D$)   $\\Rightarrow$ probably generalized $E_{out} \\approx E_{in}$, and if:  ‚ë¢ $\\cal A$ picks a $g$ with small $E_{in}$ (good $\\cal A$)   $\\Rightarrow$ probably learned! üéâ    VC Dimension\n  the formal name of **maximum non-**break point\n  Definition: VC dimension of $\\cal H$, denoted $d_{\\text{VC}}(\\cal H)$ is\n$$ \\text{largest } N \\text{ for which } m_{\\cal H}(N) = 2^N $$\n the most inputs $\\cal H$ that can shatter $d_{\\text{VC}} = \\min(k) - 1$    $N \\leq d_{\\text{VC}} \\Rightarrow$\n $\\cal H$ can shatter some $N$ inputs    $N \u0026gt; d_{\\text{VC}} \\Rightarrow$\n $N$ is a break point for $\\cal H$    $\\text{if } N \\ge 2, d_{\\text{VC}} \\ge 2, m_{\\cal H} \\leq N^{d_{\\text{VC}}}$\n    The Four VC Dimensions\n  VC Dimension and Learning\n good $\\Rightarrow$ finite $d_{\\text{VC}}$ $\\Rightarrow$ $g$ \u0026ldquo;will\u0026rdquo; generalize $(E_{out}(g) \\approx E_{in}(g))$ regardless of learning algorithm $\\cal A$ regardless of input distribution $P$ regardless of target function $f$    7.2 VC Dimension of Perceptrons   2D PLA Revisited\n general PLA for $\\mathbf{x}$ with more that 2 features?    VC Dimension of Perceptrons\n 1D perceptron (pos/neg rays): $d_{\\text{VC}} = 2$ 2D perceptron: $d_{\\text{VC}} = 3$ $d$-D perceptrons: $d_{\\text{VC}} \\mathop{=}^{?}d+1$      $d_{\\text{VC}} \\ge d + 1$\n  There are some $d+1$ inputs we can shatter\n  some \u0026ldquo;trivial\u0026rdquo; inputs:\n$$ \\mathbf{X}=\\left[\\begin{array}{c} -\\mathbf{x}{1}^{T}- \\ -\\mathbf{x}{2}^{T}- \\ -\\mathbf{x}{3}^{T}- \\ \\vdots \\ -\\mathbf{x}{d+1}^{T}- \\end{array}\\right]=\\left[\\begin{array}{ccccc} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\ 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; \u0026amp; 0 \\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\ddots \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \\end{array}\\right] $$\n    note: $\\mathbf{X}$ invertible\n  to shatter \u0026hellip;\n for any $\\bf y \\in \\Bbb{R}^{d+1}$, find $\\mathbf{w}$ such that  $\\text{sign}(\\mathbf{Xw})=\\mathbf{y} \\Leftarrow \\bf{Xw=y} \\stackrel{X\\text{ invertible!}}{\\Longleftrightarrow} w= X^{-1}y $        $d_{\\text{VC}} \\ge d+1$\n  A 2D special case\n  $d$-D general case\n  general $\\bf X$ no-shatter $\\Rightarrow d_{\\text{VC}}\\le d+1$\n    7.3 Physical Intuition of VC Dimension   Degrees of Freedom\n hypothesis parameters $\\mathbf{w}=(w_0, w_1, \\dots, w_d)$: creates degrees of freedom hypothesis quantity $M = \\vert \\mathcal{H} \\vert$: \u0026ldquo;analog\u0026rdquo; degrees of freedom hypothesis power $d_{\\text{VC}}=d+1$: effective \u0026ldquo;binary\u0026rdquo; degrees of freedom $d_{\\text{VC}}(\\cal H)$: powerfulness of $\\cal H$    Two Old Friends\n  Positive rays ($d_{\\text{VC}}=1$)\n free parameters: $a$    Positive intervals ($d_{\\text{VC}}=2$):\n free parameters: $\\mathcal{l}, r$    pratical rule of thumb: $d_{\\text{VC}} \\approx \\text{#free parameters}$ (but not always)\n    $M$ and $d_{\\text{VC}}$\n using the right $d_{\\text{VC}}$ (or $\\cal H$) is important      VC Bound Rephrase: Penalty for Model Complexity\n  For any ${\\color{Red}{g}={\\color{Red}{\\cal A}}}({\\color{Blue}{\\cal D}}) \\in \\mathcal {\\color{Orange}{H}}$ and statistical large $\\cal D$, for $d_{\\text{VC}} \\ge 2$:\n$$ {\\mathbb{P}_ {\\color{Blue} \\mathcal{D} } } [\\underbrace{\\left\\vert E_{\\text {in }}({\\color{Red} g} )-E_{\\text {out }}({\\color{Red} g} )\\right\\vert \u0026gt;\\epsilon }_ { \\mathbf{BAD} } ] \\leq \\underbrace{ {4} {\\color{Orange} (2 N)^{d_\\text {VC }}} \\exp \\left(-\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\right)}_ {\\color{Purple} \\delta} $$\n  Rephrase: with prbability $\\ge 1 - \\delta$, GOOD: $\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\leq \\epsilon$\n$$ \\begin{aligned} \\operatorname{set} \\quad {\\color{Purple} \\delta} \u0026amp;=4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} \\exp \\left(-\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\right) \\ \\frac{\\color{Purple} \\delta }{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} } \u0026amp;=\\exp \\left(-\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\right) \\ \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}} }}{\\color{Purple} \\delta }\\right) \u0026amp;=\\frac{1}{8} \\epsilon^{2} {\\color{Blue} N} \\ \\sqrt{\\frac{8}{\\color{Blue} N } \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta }\\right)} \u0026amp;=\\epsilon \\end{aligned} $$\n  generalization error $\\left\\vert E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right\\vert \\le \\sqrt{\\frac{8}{\\color{Blue} N } \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta }\\right)} $\n  $E_{\\text {in }}(g)- \\sqrt{\\frac{8}{\\color{Blue} N} \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta} \\right)} \\le E_{\\text {out }}(g) \\le E_{\\text{in}}(g) + \\sqrt{\\frac{8}{\\color{Blue} N} \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta} \\right)}$\n  $\\underbrace{\\sqrt{\\cdots}}_{\\Omega(N, \\mathcal{H}, \\delta)}$Ôºö penalty for model complexity\n    THE VC Message\n with a high probability, $E_{\\text {out }}(g) \\le E_{\\text{in}}(g) + \\underbrace{\\sqrt{\\frac{8}{\\color{Blue} N} \\ln \\left(\\frac{4{\\color{Orange} (2 N)^{d_{\\mathrm{VC}}}} }{\\color{Purple} \\delta }\\right)}}_{\\Omega(N, \\mathcal{H}, \\delta)}$   powerful $\\cal H$ not always good!    VC Bound Rephrase: Sample Complexity\n practical rule of thumb:  $N \\approx 10 d_{\\text{VC}}$ often enough!      Looseness of VC Bound\n  theory: $N \\approx 10,000\\ d_{\\text{VC}}$; practice: $N \\approx 10\\ d_{\\text{VC}}$\n  Why?\n  philosophical message of VC bound: important for improving ML\n    8 Noise and Error  Learning can still happen within a noisy environment and different error measures \u0026hellip;\n 8.1 Noise and Probabilistic Target   Recap: The Learning Flow\n what if there is noise?    Noise\n  Probabilistic Marbles\n    Target Distribution $P(y\\vert\\mathbf{x})$: characterizes behavior of \u0026ldquo;mini-target\u0026rdquo; on one $\\bf x$\n goal of learning:  predict ideal mini-target (w.r.t. $P(y\\vert \\mathbf{x})$) on often-seen inputs (w.r.t $P(\\mathbf{x})$)      The New Learning Flow\n  8.2 Error Measure   Error Measure: final hypothesis $g \\approx f$\n  how well? previously, considered out-of-sample measure\n$$ E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}[\\mathbf{g}(\\mathbf{x}) \\neq f(\\mathbf{x})] $$\n  more generally, error measure $E(g, f)$\n  naturally considered\n out-of-sample: averaged over unknown $\\bf x$ pointwise: evaluated on one $\\bf x$ classification: $[\\text{prediction} \\ne \\text{target}]$    classification error often also called \u0026ldquo;0/1 error\u0026rdquo;\n    Pointwise Error Measure\n  can often express $E(g,f)$ = averaged $\\mathop{\\text{err}}(g(\\mathbf{x}),f(\\mathbf{x}))$, like\n$$ E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}, \\underbrace{[g(\\mathbf{x}) \\neq f(\\mathbf{x})]}_{\\operatorname{err}(g(\\mathbf{x}), f(\\mathbf{x}))} $$\n $\\text{err}$: called pointwise error measure    in-sample:\n$$ E_{\\mathrm{in}}(g)=\\frac{1}{N} \\sum_{n=1}^{N} \\operatorname{err}\\left(g \\left( \\mathbf{x}{n} \\right), f \\left(\\mathbf{x}{n}\\right) \\right) $$\n  out-of-sample:\n$$ E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}, \\operatorname{err}\\left(g \\left( \\mathbf{x}{n} \\right), f \\left(\\mathbf{x}{n}\\right) \\right) $$\n  will mainly consider pointwise $\\operatorname{err}$ for simplicity\n    Two Important Pointwise Error Measure: $\\operatorname{err}(g(\\mathbf{x}), f(\\mathbf{x})) = \\operatorname{err}(\\tilde{y}, y)$\n 0/1 error: $\\operatorname{err}(\\tilde{y}, y)=[\\tilde{y} \\neq y]$  correct or incorrect? often for classification   squared error: $\\operatorname{err}(\\tilde{y}, y)=(\\tilde{y}-y)^{2}$  how far is $\\tilde{y}$ from $y$? often for regression   how does $\\operatorname{err}$ \u0026ldquo;guide\u0026rdquo; learning?    Ideal Mini-Target\n  interplay between noise and error\n $P(y\\vert \\mathbf{x})$ and $\\text{err}$ define ideal mini-target $f(\\mathbf{x})$    e.g.\n    Learning Flow with Error Measure\n extended VC theory/\u0026ldquo;philosophy\u0026rdquo; works for most $\\cal H$ and $\\text{err}$    8.3 Algorithmic Error Measure   Choice of Error Measure\n  two types of error: false acceptand false reject  0/1 error penalizes both types equally\n  Fingerprint verification for supermarket\n supermarket: fingerprint for discount false reject: very unhappy customer, lose future business false accept: give away a minor discount, intruder left fingerprint    Fingerprint verification for CIA\n CIA: fingerprint for entrance false accept: very serious consequences! false reject: unhappy employee, but so what?      Take-home Message for Now: $\\text{err}$ is application/user-dependent\n Algorithmic Error Measures $\\widehat{\\text{err}}$  true: just $\\text{err}$ plausible:  0/1: minimum \u0026ldquo;flipping noise\u0026rdquo;‚ÄîNP-hard to optimize, remember? squared: minimum Gaussian noise   friendly: easy to optimize for $\\cal A$  closed-form solution convex objective function     $\\widehat{\\text{err}}$: more in next lectures    Learning Flow with Algorithmic Error Measure\n  8.4 Weighted Classification   Weighted Classification\n  CIA cost (error, loss) matrix\n  out-of-sample\n$$ E_{\\text {out}}(h)= \\underset{(\\mathbf{x},y) \\sim P}{\\mathcal{E}} \\left\\lbrace \\begin{array}{cc} {\\color{Purple} 1} \u0026amp; \\text { if } y=+1 \\ {\\color{Orange} 1000} \u0026amp; \\text { if } y=-1 \\end{array}\\right\\rbrace \\cdot {\\color{Blue} [y \\neq h(\\mathbf{x}) ] } $$\n  in-sample\n$$ E_{\\text {in}}(h) = \\frac{1}{N} \\sum_{n=1}^{N} \\left\\lbrace \\begin{array}{cc} {\\color{Purple} 1} \u0026amp; \\text { if } y_ n =+1 \\ {\\color{Orange} 1000} \u0026amp; \\text { if } y_ n =-1 \\end{array} \\right\\rbrace \\cdot { \\color{Blue} \\left[ y _ n \\neq h \\left(\\mathbf{x} _ n \\right) \\right] } $$\n  weighted classification: different \u0026ldquo;weight\u0026rdquo; for different $(\\mathbf{x},y)$\n    Minimizing $E_{\\text{in}}$ for Weighted Classification\n$$ {\\color{Red} E_{\\mathrm{in}}^{\\mathrm{w}}(h)} =\\frac{1}{N} \\sum_{n=1}^{N}\\left\\lbrace \\begin{array}{cc} {\\color{Purple} 1} \u0026amp; \\text { if } y_ n = +1 \\ {\\color{Orange} 1000} \u0026amp; \\text { if } y_ n = -1 \\end{array}\\right\\rbrace \\cdot{\\color{Blue} \\left[y_ n \\neq h\\left(\\mathbf{x}_ n\\right)\\right]} $$\n Na√Øve Thoughts PLA: doesn‚Äôt matter if linear separable  pocket: modify pocket-replacement rule  if $\\mathbf{w}_ {t+1}$ reaches smaller $ E_ {\\mathrm{in}}^{\\mathrm{w}}$ than $\\hat{\\mathbf{w}}$, replace $\\hat{\\bf w}$ by $\\mathbf{w}_ {t+1}$     But  pocket: some guarantee on $ E_{\\mathrm{in}}^{0/1}$ modified pocket: similar guarantee on $ E_{\\mathrm{in}}^{\\mathrm{w}}$?      Systematic Route: Connect $E_{\\mathrm{in}}^{\\mathrm{w}}$ and $E_{\\mathrm{in}}^{\\mathrm{0/1}}$\n after copying \u0026ldquo;-1\u0026rdquo; examples 1,000 times, $E_{\\mathrm{in}}^{\\mathrm{w}} \\text{ for LHA} \\equiv E_{\\mathrm{in}}^{\\mathrm{0/1}} \\text{ for RHS}$!    Weighted Pocket Algorithm\n using \u0026ldquo;virtual copying\u0026rdquo;, weighted pocket algorithm include:  weighted PLA:  randomly check \u0026ldquo;-1\u0026rdquo; example mistakes with 1000 times more probability   weighted pocket replacement:  if $\\mathbf{w}_ {t+1}$ reaches smaller $ E_ {\\mathrm{in}}^{\\mathrm{w}}$ than $\\hat{\\mathbf{w}}$, replace $\\hat{\\bf w}$ by $\\mathbf{w}_ {t+1}$     systematic route (called \u0026ldquo;reduction\u0026rdquo;):  can be applied to many other algorithms!      ","permalink":"https://fang-lansheng.github.io/posts/2020-07-02-mlf-mf-ntu-2/","summary":"Course Link Ôºö\n Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices \u0026hellip;\n 5.1 Recap and Preview   Recap: the statistical learning flow\n if $\\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\\cal A$, $E_{out}(g) \\approx E_{in}(g)$   if $\\cal A$ finds one $g$ with $E_{in}(g) \\approx 0$  PAC guarantee for $E_{out}(g) \\approx 0$ $\\Rightarrow$ learning possible      Two central questions","title":"Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhy Can Machines Learn?"},{"content":" Course Link Ôºö\n Week 1 - The Learning Problem Week 2 - Learning to Answer Yes/No Week 3 - Types of Learning Week 4 - Feasibility of Learning   1 The Learning Problem  What machine learning is and its connection to applications and other fields \u0026hellip;\n 1.1 Course Introduction   Machine Learning: a mixture of theoretical and practical tools\n theory oriented  derive everything deeply for solid understanding less interesting to general audience   techniques oriented  flash over the sexiest techniques broadly for shiny converge too many techniques, hard to choose, hard to use properly   **Our approach: **foundation oriented    Foundation Oriented ML Course\n  mixture of philosophical illustrations, key theory, core techniques, usage in practice and hopefully jokes\n\u0026mdash;- what every machine learning user should know\n  story-like:\n When Can Machines Learn? (illustrative + technical) Why Can Machines Learn? (theoretical + illustrative) How Can Machines Learn? (technical + practical) How Can Machines Learn Better? (practical + theoretical)    allows students to learn \u0026ldquo;future/untaught\u0026rdquo; techniques or study deeper theory easily\n    1.2 What is Machine Learning   From Learning to Machine Learning\n  learning: acquiring skillwith experience accumulated from observations  machine learning: acquiring skillwith experience accumulated/computed from data  what is skill ?\n improve some performance measure(e.g. prediction accuracy)      A more concrete definition:\n  machine learning: improving some performance measurewith experience computedfrom data  For example:\n  Yet another application: tree recognition\n \u0026ldquo;define\u0026rdquo; trees and hand-program: difficult learn from data (observations) and recognize: a 3-year-old can do so \u0026ldquo;ML-based tree recognition system\u0026rdquo; can be easier to build than hand-programmed system      The machine learning route\n ML: an alternative route to build complicated systems Some use scenarios:  When human cannot program the system manually:  navigating on Mars   When human cannot \u0026ldquo;define the solution\u0026rdquo; easily:  speech/visual recognition   When needing rapid decisions that humans cannot do  high-frequency trading   When needing to be user-oriented in a massive scale  consumer-targeted marketing         Key essence of ML\n exists some \u0026ldquo;underlying pattern\u0026rdquo; to be learned so \u0026ldquo;performance measure\u0026rdquo; can be improved   but no programmable (easy) definition  so \u0026ldquo;ML\u0026rdquo; is needed   somehow there is data about the pattern  so ML has some \u0026ldquo;inputs\u0026rdquo; to learn      1.3 Applications of Machine Learning   Daily needs: food, clothing, housing, transportation\n Food (Sadilek et al., 2013)  data: twitter data (words + location) skill: tell food poisoning likeliness of restaurant properly   Clothing (Abu-Mostafa, 2012)  data: sales figures + client surveys skill: give good fashion recommendations to clients   Housing: (Tsanas and Xifara, 2012)  data: characteristics of buildings and their energy load skill: predicted energy load of other buildings closely   Transportation: (Stallkamp et al., 2012)  data: some traffic sign images and meanings skill: recognize traffic signs accurately      And more:\n  Education\n data: students\u0026rsquo; records on quizzes on a Math tutoring system skill: predict whether a student can give a correct answer to another quiz question A possible ML solution: answer correctly $\\approx$ [recent strength of students $\u0026gt;$ difficulty of question]  give ML 9 million records from 3,000 students ML determines (reverse-engineers) strength and difficulty automatically      Entertainment: recommender system\n  data: how many users have rated some movies\n  skill: predict how a user would rate an unrated movie\n  A hot problem:\n competition held by Netflix in 2006  100,480,507 ratings that 480,189 users gave to 17,770 movies 10% improvement = 1 million dollar prize   similar competition (movies ‚Üí songs) held by Yahoo! in KDDCup 2011  252,800,275 ratings that 1,000,990 users gave to 624,961 songs      A possible ML solution:\n pattern: rating ‚Üê viewer/movie factors learning: known rating  learned factors unknown rating prediction          1.4 Components of Machine Learning  Metaphor using credit approval\n   Application information:\n  Unknown pattern to be learned: \u0026ldquo;approve credit card good for bank?\u0026rdquo;\n  Formalize the learning problem:\n  basic notations:\n input: $\\bf{x} \\in \\cal{X}$ (customer application) output: $y \\in \\cal{Y}$ (good/bad after approving credit card) unknown pattern to be learned $\\Leftrightarrow$ target function: $f: \\cal{X} \\rightarrow \\cal{Y}$ (ideal credit approval formula) data$\\Leftrightarrow$ training examples: $\\mathcal{D} = \\left\\lbrace (\\mathbf{x}_1, y_1), \\mathbf{x}_2, y_2), \\dots, \\mathbf{x}_N, y_N) \\right\\rbrace$ (historical records in bank) hypothesis$\\Leftrightarrow$ skillwith hopefully good performance: $g: \\mathcal{X} \\rightarrow \\mathcal{Y}$ (\u0026ldquo;learned\u0026rdquo; formula to be used)    learning flow to credit approval\n target $f$ unknown (i.e. no programmable definition) hypothesis $g$ hopefully $\\approx f$, but possibly different from $f$ (perfection \u0026ldquo;impossible\u0026rdquo; when $f$ unknown) What does $g$ look like?      The learning model\n assume $g \\in \\mathcal{H} =\\lbrace h_k \\rbrace$, i.e. approving if  $h_1$: annual salary \u0026gt; $800,000 $h_2$: debt \u0026gt; $100,000 (really?) $h_3$: year in job $\\le$ 2 (really?)   hypothesis set $\\cal H$:  can contain good or bad hypothesis      Practical definition of machine learning\n use data to compute hypothesis $g$ that approximates target $f$    1.5 Machine Learning and Other Fields  Machine Learning and Data Mining  Definition  Machine learning use data to compute hypothesis $g$ that approximates target $f$ Data mining use (huge) data to find property that is interesting   if \u0026ldquo;interesting property\u0026rdquo; same as \u0026ldquo;hypothesis that approximate target\u0026rdquo;  ML = DM   if \u0026ldquo;interesting property\u0026rdquo; related to \u0026ldquo;hypothesis that approximate target\u0026rdquo;  DM can help ML, and vice versa (often, but not always)   traditional DM also focuses on efficient computation in large database difficult to distinguish ML and DM in reality   Machine Learning and Artificial Intelligence  Definition  Machine learning use data to compute hypothesis $g$ that approximates target $f$ Artificial Intelligence compute something that shows intelligent behavior   $g \\approx f$ is something that shows intelligent behavior  ML can realize AI, among other routes e.g. chess playing  traditional AI: game tree ML for AI: \u0026ldquo;learning from board data\u0026rdquo;     ML is one possible route to realize AI   Machine Learning and Statistics  Definition  Machine learning use data to compute hypothesis $g$ that approximates target $f$ Statistics use data to make inference about an unknown process   $g$ is an inference outcome; $f$ is something unknown  Statistics can be used to achieve ML   traditional statistics also focus on provable results with math assumptions, and care less about computation Statistics: many useful tools for ML     Your first learning algorithm (and the world\u0026rsquo;s first!) that \u0026ldquo;draws the line\u0026rdquo; between yes and no by adaptively searching for a good line based on data \u0026hellip;\n 2.1 Perceptron Hypothesis Set   A simple hypothesis set: the \u0026ldquo;Perceptron\u0026rdquo;\n  for $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$ \u0026ldquo;feature of customer\u0026rdquo;, compute a weighted \u0026ldquo;score\u0026rdquo; and:\n approve credit if $\\sum_{i=1}^d w_ix_i \u0026gt; \\text{threshold}$ deny credit if $\\sum_{i=1}^d w_ix_i \u0026lt; \\text{threshold}$    $\\mathcal{Y}:\\lbrace +1(\\text{good}), -1(\\text{bad}) \\rbrace$, (0 ignored) - linear formula $h \\in \\cal{H}$ are:\n$$ h(x) = \\text{sign}\\left( \\sum_{i=1}^dw_ix_i - \\text{threshold}\\right) $$\n called perceptron hypothesis historically      Vector form of perceptron hypothesis\n$$ \\begin{aligned} h(x) \u0026amp;= \\text{sign}\\left( \\sum_{i=1}^dw_ix_i - \\text{threshold}\\right) \\newline \u0026amp;= \\text{sign}\\left( \\sum_{i=1}^dw_ix_i + \\underbrace{(-\\text{threshold})}{w_0} \\cdot \\underbrace{(+1)}{x_0} \\right) \\newline \u0026amp;= \\text{sign}\\left( \\sum_{i=0}^dw_ix_i\\right) = \\text{sign}(\\mathbf{w}^T\\mathbf{x}) \\end{aligned} $$\n each \u0026ldquo;tall\u0026rdquo; $\\mathbb{w}$ represents a hypothesis $h$ \u0026amp; is multiplied with \u0026ldquo;tall\u0026rdquo; $\\mathbf{x}$ \u0026ndash; will use tall version to simplify notation what do perceptron $h$ \u0026ldquo;look like\u0026rdquo;?    Perceptron in $\\Bbb{R}^2$\n  $h(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^T\\mathbf{x}) = \\text{sign}(w_0+w_1x_1+w_2x_2)$\n  customer features $\\mathbf{x}$: points on the plane (or points in $\\Bbb{R}^d$)\n  labels $y$: $\\color{blue}{\\circ (+1)}, \\color{red}{\\times (-1)}$\n  hypothesis $h$: lines(or hypothesis in $\\Bbb{R}^d$)\n          Select $g$ from $\\cal{H}$\n  $\\cal H$ = all possible perceptrons, $g = ?$\n  want: $g \\approx f$ (hard when $f$ unknown)\n  almost necessary: $g \\approx f$ on $\\cal D$, ideally $g(\\mathbf{x}_n) = f(\\mathbf{x}_n)=y_n$\n  difficult: $\\cal H$ is of infinite size\n  idea: start from some $g_0$, and \u0026ldquo;correct\u0026rdquo; its mistakes on $\\cal D$\n  will present $g_0$ by its weight vector $\\mathbf{w}_0$\n    Perceptron Learning Algorithm\n  start from some $\\mathbf{w}_0$ (say, $\\mathbf{0}$), and \u0026ldquo;correct\u0026rdquo; its mistakes on $\\cal D$\n  For $t = 0, 1, \\dots$\n  find a mistake of $\\mathbf{w}t$, called $(x{n(t)}, y_{n(t)})$, and $\\text{sign}\\left( \\mathbf{w}t^T\\mathbf{x}{n(t)} \\right) \\ne y_{n(t)}$\n  (try to) correct the mistake by $\\mathbf{w}{t+1} \\leftarrow \\mathbf{w}t + y{n(t)}x{n(t)}$\n  \u0026hellip; until no more mistakes\n  return last $\\mathbf{w}$ called $\\mathbf{w}_{PLA}$ as $g$\n    Cyclic PLA\n next can follow na√Øve cycle $(1, \\dots, N)$ or precomputed random cycle      Some remaining issues of PLA\n \u0026ldquo;correct\u0026rdquo; mistakes on $\\cal D$ until no mistakes Algorithmic: halt (with no mistakes)?  na√Øve cyclic: ?? random cyclic: ?? other variant: ??   Learning: $g \\approx f$ ?  on $\\cal D$, if halt, yes (no mistake) outside $\\cal D$: ?? if not halting: ??   if (\u0026hellip;), after enough corrections, any PLA variant halts    2.3 Guarantee of PLA   Linear separability\n if PLA halts (i.e. no more mistakes) (necessary condition) $\\cal {D}$ allows some $\\bf w$ to make no mistake call such $\\cal D$ linear separable   assume linear separable $\\cal D$, dose PLA always halt?    PLA fact: $\\mathbf{w}_t$ gets more aligned with $\\mathbf{w}_f$\n  linear separable $\\Leftrightarrow$ exists perfect $\\mathbf{w}_f$ such that $y_n = \\text{sign}(\\mathbf{w}_f^T\\mathbf{x}_n)$\n  $\\mathbf{w}_f$ perfect hence every $\\mathbf{x}_n$ correctly away from line:\n$$ {y}_{n(t)}\\mathbf{w}f^T\\mathbf{x}{n(t)} \\ge \\min_n y_n \\mathbf{w}_f^T\\mathbf{x}_n \u0026gt; 0 $$\n  by updating with any $\\left(\\mathbf{x}{n(t)}, y{n(t)}\\right)$,\n  $$ \\begin{aligned} \\mathbf{w}f^T\\mathbf{w}{t+1} \u0026amp;= \\mathbf{w}f^T(\\mathbf{w}t+y{n(t)}\\mathbf{x}{n(t)}) \\newline \u0026amp;\\ge \\mathbf{w}f^T\\mathbf{w}t + \\min_n y{n}\\mathbf{x}{n} \\newline \u0026amp;\u0026gt; \\mathbf{w}_f^T\\mathbf{w}_t + 0. \\end{aligned} $$\n  PLA fact: $\\mathbf{w}_t$ does not grow too fast\n  $\\mathbf{w}t$ change only when mistake $\\Leftrightarrow$ $\\text{sign}(\\mathbf{w}t^T\\mathbf{x}{n(t)}) \\ne y{n(t)}$ $\\Leftrightarrow$ $y_{n(t)}\\mathbf{w}t^T\\mathbf{x}{n(t)} \\le 0$\n  mistake limits $|\\mathbf{w}_t|^2$ growth, even when updating with longest $\\mathbf{x}_n$\n$$ \\begin{aligned} |\\mathbf{w}{t+1}|^2 \u0026amp;= |\\mathbf{w}t+y{n(t)}\\mathbf{x}{n(t)} |^2 \\newline \u0026amp;= |\\mathbf{w}t|^2 +{2y{n(t)}\\mathbf{w}t^T\\mathbf{x}{n(t)}} + {|y_{n(t)}\\mathbf{x}{n(t)}|^2 }\\newline \u0026amp;\\le |\\mathbf{w}t|^2 +0 + |y{n(t)}\\mathbf{x}{n(t)}|^2 \\newline \u0026amp;\\le |\\mathbf{w}t|^2 +\\max_n | y{n} \\mathbf{x}_n|^2 \\end{aligned} $$\n  start from $\\mathbf{w}_0=0$, after $T$ mistake corrections,\n$$ \\frac{\\mathbf{w}_f^T}{|\\mathbf{w}_f|} \\frac{\\mathbf{w}_T}{|\\mathbf{w}_T|} \\ge \\sqrt{T} \\cdot \\text{constant} $$\n    2.4 Non-Separable Data   More about PLA\n Guarantee: as long as linear separableand correct by mistake inner product of $\\mathbf{w}_f$ and $\\mathbf{w}_t$ grows fast; length of $\\mathbf{w}_t$ grows slowly PLA \u0026ldquo;lines\u0026rdquo; are more and more aligned with $\\mathbf{w}_f$ $\\Rightarrow$ halts   Pros: simple to implement, fast, works in any dimension $d$ Cons:  \u0026ldquo;assumes\u0026rdquo; linear separable $\\cal D$ to halt  property unknown in advance (no need for PLA if we know $\\mathbf{w}_f$)   not fully sure how long halting takes ($\\rho$ depends on $\\mathbf{w}_f$)  though practically fast     What if $\\cal D$ not linear separable?    Learning with Noisy Data\n how to at least get $g \\approx f$ on noisy $\\cal D$?    Line with noise tolerance\n  assume little noise: $y_n = f(\\mathbf{x}_n)$ usually\n  if so, $g \\approx f$ on $\\cal D$ $\\Leftrightarrow$ $y_n=g(\\mathbf{x}_n)$ usually\n  How about $$ \\mathbf{w}g \\leftarrow \\mathop{\\text{argmin}}{\\mathbf{w}} \\sum_{n=1}^N \\left[ y_{n} \\neq \\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{x}_n\\right) \\right] $$\n NP-hard to solve, unfortunately    can we modify PLA to get an approximately good $g$?\n    Pocket Algorithm\n  Modify PLA algorithm (black lines) by keeping best weights in pocket\n  Initialize pocket weights $\\mathbf{\\hat{w}}$\n  find a (random) mistake of $\\mathbf{w}t$, called $\\left(\\mathbf{x}{n(t)}, y_{n(t)}\\right)$\n  (try to) correct the mistake by $$ \\mathbf{w}{t+1} \\leftarrow \\mathbf{w}t + y{n(t)}\\mathbf{x}{n(t)} $$\n  if $\\mathbf{w}{t+1}$ makes fewer mistakes that $\\mathbf{\\hat{w}}$, replace $\\mathbf{\\hat{w}}$ by $\\mathbf{w}{t+1}$\n  \u0026hellip; until enough iterations\n  return $\\mathbf{\\hat{w}}$ (called $\\mathbf{w}_{\\text{POCKET}}$) as $g$\n      3 Types of Learning  Learning comes with many possibilities in different applications, with our focus being binary classification or regression from a batch of supervised data with concrete features \u0026hellip;\n 3.1 Learning with Different Output Space   More binary classification problems\n like:  credit approve/disapprove email spam/non-spam patient sick/not sick ad profitable/not profitable answer correct/incorrect   core and important problem with many tools as building block of other tools    Multiclass classification\n  coin recognition problem\n classify US coins (1c, 5c, 10c, 25c) by (size, mass)   $\\mathcal{Y} = \\lbrace 1, 5, 10, 25\\rbrace$    binary classification: special case with $K = 2$\n  Other multiclass classification problems:\n written digits $\\Rightarrow$ 0, 1, 2, \u0026hellip;, 9 pictures $\\Rightarrow$ apple, orange, strawberry, \u0026hellip; emails $\\Rightarrow$ spam, primary, social, promotion, update \u0026hellip;    many applications in practice, especially for recognition\n    Regression:\n patient recovery prediction problem  binary classification: patient features $\\Rightarrow$ sick or not multiclass classification: patient features $\\Rightarrow$ which type of cancer regression: patient features $\\Rightarrow$ how many days before recovery $\\mathcal{Y}=\\Bbb{R}$ or $\\mathcal{Y}=[\\text{lower},\\text{upper}]$ $\\subset \\Bbb{R}$ (bounded regression)  deeply studied in statistics     Other regression problems:  company data $\\Rightarrow$ stock price climate data $\\Rightarrow$ temperature   also core and important with math statistical tools as building block of other tools    Structured Learning: sequence tagging problem\n  e.g. NLP\n multiclass classification $\\Rightarrow$ word class structured learning:  sentence $\\Rightarrow$ structure (class of each word)   $\\mathcal{Y} = \\lbrace PVN, PVP, NVN, PV, \\cdots \\rbrace$, not including $VVVVVV$ huge multiclass classification problem (structure $\\equiv$ hyperclass) without explicit class definition    Other structured learning problems:\n protein data $\\Rightarrow$ protein folding speech data $\\Rightarrow$ speech parse tree    a fancy but complicated learning problem\n    Mini Summary: learning with different output space $\\cal Y$\n binary classification: $\\mathcal{Y} = \\lbrace -1, +1 \\rbrace$ multiclass classification: $\\mathcal{Y} = \\lbrace 1, 2, \\cdots, K \\rbrace$ regression: $\\mathcal{Y} = \\Bbb{R}$ structured learning: $\\mathcal{Y} = \\text{structures}$ \u0026hellip; and a lot more!    3.2 Learning with Different Data Label   Supervised Learning\n coin recognition revisited  supervised multiclass classification      Unsupervised Learning\n coin recognition without $y_n$  unsupervised multiclass classification $\\Leftrightarrow$ clustering   Other clustering problems:  articles $\\Rightarrow$ topics consumer profiles $\\Rightarrow$ consumer groups   clustering: a challenging but useful problem    Other unsupervised learning problems\n clustering: $\\lbrace \\mathbf{x}_n \\rbrace \\Rightarrow \\text{cluster}(\\mathbf{x})$  $\\approx$ unsupervised multiclass classification   density estimation: $\\lbrace \\mathbf{x}_n \\rbrace \\Rightarrow \\text{density}(\\mathbf{x})$  $\\approx$ unsupervised bounded regression e.g., traffic reports with location $\\Rightarrow$ dangerous areas   outlier detection: $\\lbrace \\mathbf{x}_n \\rbrace \\Rightarrow \\text{unusual}(\\mathbf{x})$  $\\approx$ extreme unsupervised binary classification e.g., Internet logs $\\Rightarrow$ intrusion alert   \u0026hellip; and a lot more!    Semi-supervised Learning:\n coin regression with some $y_n$ Other semi-supervised learning problems  face images with a few labeled $\\Rightarrow$ face identifier medicine data with a few labeled $\\Rightarrow$ medicine effect predictor   leverage unlabeled data to avoid expensive labeling    Reinforcement Learning: a very different but natural way of learning\n Teach your dog: say \u0026ldquo;sit down\u0026rdquo;  if  The dog pees on the ground  BAD DOG. THAT\u0026rsquo;S A VERY WRONG ACTION.   The dog sits down.  Good Dog. Let me give you some cookies     cannot easily show the dog that $y_n$ = \u0026ldquo;sit\u0026rdquo;, when $\\mathbf{x}_n$ = \u0026ldquo;sit down\u0026rdquo; but can \u0026ldquo;punish\u0026rdquo; to say $\\tilde{y}_n$ = \u0026ldquo;pee is wrong\u0026rdquo; but can \u0026ldquo;reward\u0026rdquo; to say $\\tilde{y}_n$ = \u0026ldquo;sit is good\u0026rdquo;   Other RL problems using $(\\mathbf{x}, \\tilde{y}, \\text{goodness})$  (customer, ad choice, ad click earning) $\\Rightarrow$ ad system (cards, strategy, winning amount) $\\Rightarrow$ black jack agent   RL: learn with partial/implicit information (often sequentially)    Mini Summary: learning with different data label $y_n$\n supervised: all $y_n$ unsupervised: no $y_n$ semi-supervised: some $y_n$ reinforcement: implicit $y_n$ by goodness ($\\tilde{y}_n$) \u0026hellip; and more!    3.3 Learning with Different Protocol  Batch Learning:  coin recognition revisited batch supervised multiclass classification: learn from all known data More batch learning problems  batch of (email,spam?) $\\Rightarrow$ spam filter batch of (patient,cancer) $\\Rightarrow$ cancer classifier batch of patient data $\\Rightarrow$ group of patients     Online Learning:  spam filter that improve  batch spam filter:  learn with known (email, spam?) pairs, and predict with fixed $g$   online spam filter, which sequentially:  observe an email $\\mathbf{x}_t$ predict spam status with current $g_t(\\mathbf{x}_t)$ receive desired label $y_t$ from user, and then update $g_t$ with $(\\mathbf{x}_t, y_t)$     Connection to what we\u0026rsquo;ve learned  PLA can be easily adapted to online protocol (how?) RL learning is often done online (why?)   Online learning: hypothesis improves through receiving data instances sequentially   Active Learning: learning by asking  protocol $\\Leftrightarrow$ learning philosophy  batch: \u0026ldquo;duck feeding\u0026rdquo; online: \u0026ldquo;passive sequential\u0026rdquo; active: \u0026ldquo;question asking\u0026rdquo; (sequentially)  query the $y_n$ of the chosen $\\mathbf{x}_n$     active: improve hypothesis with fewer labels (hopefully) by asking questions strategically   Mini Summary: learning with different protocol $\\Rightarrow$ $(\\mathbf{x}_n, y_n)$  batch: all known data online: sequential (passive) data active: strategically-observed data \u0026hellip; and more!    3.4 Learning with Different Input Space   Concrete features: each dimension of $\\cal{X} \\subseteq \\Bbb{R}^d$\n  More on concrete features\n (size, mass) for coin classification customer info for credit approval patient info for cancer diagnosis    often including human intelligence on the learning task\n  concrete features: the easy ones for ML\n    Raw features:\n  digit recognition problem\n features $\\Rightarrow$ meaning of digit a typical supervised multiclass classification problem    Other problems with raw features\n image pixels, speech signal, etc.    raw features: often need human or machines to convert to concrete ones\n    Abstract features\n rating prediction problem  given previous (userid, itemid, rating) tuples, predict the ratingthat some useridwould give to itemid? a regression problem with $\\cal{Y} \\subseteq \\Bbb R$ as rating and $\\mathcal{X} \\subseteq N \\times N$ as (userid, itemid) no physical meaning; thus even more difficult for ML   Other problems with abstract features  student ID in online tutoring system advertisement ID in online ad system   abstract: again need feature conversion/extraction/construction    Mini Summary: Learning with different space $\\cal X$\n concrete: sophisticated (and related) physical meaning raw: simple physical meaning abstract: no (or little) physical meaning \u0026hellip; and more!    4 Feasibility of Learning  Learning can be \u0026ldquo;probably approximately correct\u0026rdquo; when given enough statistical data and finite number of hypotheses \u0026hellip;\n 4.1 Learning is Impossible   Two controversial answers\n all valid reasons, your adversarial teacher can always call you didn‚Äôt learn. üò¢    A simple binary classification problem\n pick $g \\in \\cal{H}$ with all $g(\\mathbf{x}_n)=y_n$ (like PLA), does $g \\approx f$?   learning from $\\cal D$ (to infer something outside $\\cal D$) is doomed if any unknown $f$ can happen. üò¢    4.2 Probability to the Rescue   Inferring something unknown\n difficult to infer unknown target $f$ outside $\\cal D$ in learning; can we infer something unknown in other scenarios? consider a bin of many many orangeand greenmarbles do we know the orangeportion (probability)\u0026gt; No! can you infer the orangeprobability?    Statistics 101: Inferring orangeprobability\n Possible versus Probable: does in-sample $v$ say anything about out-of-sample $\\mu$?  possibly not: sample can be mostly greenwhile bin is mostly orange probably yes: in-sample $\\nu$ likely close to unknown $\\mu$   formally, what does $\\nu$ say about $\\mu$?    Hoeffding‚Äôs Inequality $\\epsilon$: tolerance    4.3 Connection to Learning   Connection to Learning\n  Added components\n for any fixed $h$, can probably infer  unknown $E_{out}(h) = \\mathop{\\varepsilon}_\\limits{\\mathbf{x}\\sim P} [h(\\mathbf{x}) \\neq f(\\mathbf{x})]$ (out-of-sample error) by known $E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N [h(\\mathbf{x}_n) \\neq y_n]$ (in-sample error)      The formal guarantee\n  for any fixed $h$, in big data ($N$ large), in-sample error $E_{in}(h)$ is probably close to out-of-sample error $E_{out}(h)$ (within $\\epsilon$) $$ \\Bbb{R} \\left[|E_{in}(h) - E_{out}(h)| \u0026gt; \\epsilon \\right] \\leq 2 \\exp(-2\\epsilon^2N) $$\n  same as the \u0026ldquo;bin\u0026rdquo; analogy\n valid for all $N$ and $\\epsilon$ does not depend on $E_{out}(h)$, no need to \u0026ldquo;know\u0026rdquo; $E_{out}(h)$  $f$ and $P$ can stay unknown      if $E_{in}(h) \\approx E_{out}(h)$ and $E_{in}(h)$ small $\\Rightarrow$ $E_{out}(h)$ small $\\Rightarrow$ $h \\approx f$ with respect to $P$\n    Verification of one $h$\n for any fixed $h$, when data large enough, $E_{in}(h) \\approx E_{out}(h)$ can we claim \u0026ldquo;good learning\u0026rdquo; ($g \\approx f)$? Yes!  if $E_{in}(h)$ is small for the fixed $h$ and $\\cal A$ pick the $h$ as $g$ $\\Rightarrow$ $g = f$ PAC   No!  if $\\cal A$ forced to pick THE $h$ as $g$ $\\Rightarrow$ $E_{in}(h)$ almost always not small $\\Rightarrow$ $g \\ne f$ PAC!   real learning:  $\\cal A$ shall make choices $\\in \\cal H$ (like PLA) rather than being forced to pick one $h$      The verification flow\n can now use historical records (data) to verify \u0026ldquo;one candidate formula\u0026rdquo; $h$    4.4 Connection to Real Learning   Multiple $h$\n real learning (say like PLA):  BINGO when getting all green marbles?      Coin game\n Q: if everyone in size-150 NTU ML class flips a coin 5 times, and one of the students gets 5 heads for her coin $g$. Is $g$ really magical? A: No. Even if all coins are fair, the probability that one of the coins results in 5 heads is $1 - \\left(\\frac{31}{32}\\right)^{150} \u0026gt; 99%$. BAD sample: $E_{in}$ and $E_{out}$ far away  can get worse when involving choice      BAD Sample and BAD Data\n  BAD Sample:\n e.g., $E_{out} = \\frac{1}{2}$, but getting all heads ($E_{in} = 0$)!    BAD Data for One $h$\n $E_{out}(h)$ and $E_{in}(h)$ far away e.g., $E_{out}(h)$ big (far from $f$), but $E_{in}$ small (correct on most examples)    Hoeffding: $\\Bbb{P}_{\\cal D}[{\\color{Orange}{\\text{BAD }}} \\mathcal{D} \\text{ for } h] \\le \\ \u0026hellip;$\n $$ \\Bbb{P}{\\cal D}[{\\color{Orange}{\\text{BAD }}} \\mathcal{D}] = \\mathop{\\sum}\\limits{\\text{all possible }\\mathcal{D}} \\Bbb{P}(\\mathcal{D})\\cdot [{\\color{Orange}{\\text{BAD }}}\\mathcal{D}] $$      BAD Data for Many $h$\n $\\Leftrightarrow$ no \u0026ldquo;freedom of choice\u0026rdquo; by $\\cal A$ $\\Leftrightarrow$ there exists some $h$ such that $E_{out}(h)$ and $E_{in}(h)$ far away for $M$ hypothesis, bound of $\\Bbb{P}_{\\cal D}[{\\color{Orange}{\\text{BAD }}} \\mathcal{D} ]$?    Bound of BAD Data $$ \\begin{aligned} \u0026amp;\\ \\Bbb{P}{\\cal D} [{\\color{Orange}{\\text{BAD }}} \\mathcal{D}] \\newline = \u0026amp;\\ \\Bbb{P}{\\cal D} [{\\color{Orange}{\\text{BAD }}} \\mathcal{D} \\text{ for $h_1$ or } \\dots {\\text{ or }\\color{Orange}{\\text{BAD }}} \\mathcal{D} \\text{ for $h_M$} ] \\newline \\le \u0026amp;\\ \\Bbb{P}{\\cal D} [{\\color{Orange}{\\text{BAD }}} \\mathcal{D} \\text{ for $h_1$ or }] + \\cdots + \\Bbb{P}{\\cal D} [{\\color{Orange}{\\text{BAD }}} \\mathcal{D} \\text{ for $h_M$} ] \\newline \\le \u0026amp;\\ 2\\exp{\\left(-2\\epsilon^2N \\right)} + \\cdots + 2\\exp{\\left(-2\\epsilon^2N \\right)} \\newline = \u0026amp;\\ 2M\\exp{\\left(-2\\epsilon^2N \\right)} \\end{aligned} $$\n finite-bin version of Hoeffding, valid for all $M$, $N$ and $\\epsilon$ does not depend on any $E_{out} (h_m )$, no need to \u0026ldquo;know\u0026rdquo; $E_{out }(h_m)$  $f$ and $P$ can stay unknown   $E_{in}(g) = E_{out}(g)$ is PAC, regardless of $\\cal A$ most reasonable $\\cal A$ (like PLA/pocket):  pick the $h_m$ with lowest $E_{in}(h_m )$ as $g$      The Statistical Learning Flow\n if $\\vert \\mathcal{H} \\vert= M$ finite, $N$ large enough  for whatever $g$ picked by $\\cal A$, $E_{out}(h) \\approx E_{in}(g)$   if $\\cal A$ finds one $g$ with $E_{in}(g) \\approx 0$,  PAC guarantee for $E_{out}(g) \\approx 0$ $\\Rightarrow$ learning possible :happy:   if $M = \\infty$ (like perceptrons)?  see you in the next lectures      ","permalink":"https://fang-lansheng.github.io/posts/2020-06-30-mlf-mf-ntu-1/","summary":"Course Link Ôºö\n Week 1 - The Learning Problem Week 2 - Learning to Answer Yes/No Week 3 - Types of Learning Week 4 - Feasibility of Learning   1 The Learning Problem  What machine learning is and its connection to applications and other fields \u0026hellip;\n 1.1 Course Introduction   Machine Learning: a mixture of theoretical and practical tools\n theory oriented  derive everything deeply for solid understanding less interesting to general audience   techniques oriented  flash over the sexiest techniques broadly for shiny converge too many techniques, hard to choose, hard to use properly   **Our approach: **foundation oriented    Foundation Oriented ML Course","title":"Êú∫Âô®Â≠¶‰π†Âü∫Áü≥-Êï∞Â≠¶ÁØáÔºöWhen Can Machines Learn?"},{"content":" Course Link ÔºöWeek10 - Large Scale Machine Learning \u0026amp; Week11 - Application Example: Photo OCR\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data\n However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data\n But learning with large datasets comes with its own computational problems    Because of the computational cost of this massive summation, we\u0026rsquo;ll look at more efficient ways around this\n Either using a different approach Optimizing to avoid the summation    First thing to do is ask if we can train on 1000 examples instead of 100,000,000\n Randomly pick a small selection Can you develop a system which performs as well?  Sometimes yes - if this is the case you can avoid a lot of the headaches associated with big data      To see if taking a smaller sample works, you can sanity check by plotting error vs. training set size\n  If our plot looked like this\n  Looks like a high variance problem\n More examples should improve performance    If plot looked like this\n  This looks like a high bias problem\n More examples may not actually help - save a lot of time and effort if we know this before hand One natural thing to do here might be to:  Add extra features Add extra hidden units (if using neural networks)        34.2 Stochastic Gradient Descent   For many learning algorithms, we derived them by coming up with an optimization objective (cost function) and using an algorithm to minimize that cost function\n When you have a large dataset, gradient descent becomes very expensive So here we\u0026rsquo;ll define a different way to optimize for large data sets which will allow us to scale the algorithms    Suppose you\u0026rsquo;re training a linear regression model with gradient descent\n  We will use linear regression for our algorithmic example here when talking about stochastic gradient descent, although the ideas apply to other algorithms too, such as\n Logistic regression Neural networks    Below we have a contour plot for gradient descent showing iteration to a global minimum\n  As mentioned, if $m$ is large gradient descent can be very expensive\n  Although so far we just referred to it as gradient descent, this kind of gradient descent is called batch gradient descent\n This just means we look at all the examples at the same time    Batch gradient descent is not great for huge datasets\n If you have 300,000,000 records you need to read in all the records into memory from disk because you can\u0026rsquo;t store them all in memory  By reading all the records, you can move one step (iteration) through the algorithm   Then repeat for EVERY step  This means it take a LONG time to converge Especially because disk I/O is typically a system bottleneck anyway, and this will inevitably require a huge number of reads      What we\u0026rsquo;re going to do here is come up with a different algorithm which only needs to look at single example at a time\n    Stochastic gradient descent\n Define our cost function slightly differently, as $\\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)=\\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$  So the function represents the cost of $\\theta$ with respect to a specific example $(x^{(i)}, y^{(i)})$  And we calculate this value as one half times the squared error on that example   Measures how well the hypothesis works on a single example   The overall cost function can now be re-written in the following form: $J_{t r a i n}(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)$  This is equivalent to the batch gradient descent cost function   With this slightly modified (but equivalent) view of linear regression we can write out how stochastic gradient descent works So what\u0026rsquo;s going on here?  The term $(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ Is the same as that found in the summation for batch gradient descent It\u0026rsquo;s possible to show that this term is equal to the partial derivative with respect to the parameter $Œ∏_j$ of the $\\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)$   What stochastic gradient descent algorithm is doing is scanning through each example  The inner for loop does something like this\u0026hellip;  Looking at example 1, take a step with respect to the cost of just the 1st training example  Having done this, we go on to the second training example   Now take a second step in parameter space to try and fit the second training example better  Now move onto the third training example   And so on\u0026hellip; Until it gets to the end of the data   We may now repeat this who procedure and take multiple passes over the data   The randomly shuffling at the start means we ensure the data is in a random order so we don\u0026rsquo;t bias the movement  Randomization should speed up convergence a little bit   Although stochastic gradient descent is a lot like batch gradient descent, rather than waiting to sum up the gradient terms over all $m$ examples, we take just one example and make progress in improving the parameters already  Means we update the parameters on EVERY step through data, instead of at the end of each loop through all the data      What does the algorithm do to the parameters?\n  As we saw, batch gradient descent does something like this to get to a global minimum\n  With stochastic gradient descent every iteration is much faster, but every iteration is flitting a single example\n What you find is that you \u0026ldquo;generally\u0026rdquo; move in the direction of the global minimum, but not always Never actually converges like batch gradient descent does, but ends up wandering around some region close to the global minimum  In practice, this isn\u0026rsquo;t a problem - as long as you\u0026rsquo;re close to the minimum that\u0026rsquo;s probably OK        One final implementation note\n May need to loop over the entire dataset 1-10 times If you have a truly massive dataset it\u0026rsquo;s possible that by the time you\u0026rsquo;ve taken a single pass through the dataset you may already have a perfectly good hypothesis  In which case the inner loop might only need to happen 1 if $m$ is very very large      If we contrast this to batch gradient descent\n We have to make $k$ passes through the entire dataset, where $k$ is the number of steps needed to move through the data    34.3 Mini-Batch Gradient Descent   Mini-batch gradient descent is an additional approach which can work even faster than stochastic gradient descent\n  To summarize our approaches so far\n Batch gradient descent: Use all $m$ examples in each iteration Stochastic gradient descent: Use 1 example in each iteration Mini-batch gradient descent: Use $b$ examples in each iteration  $b$ = mini-batch size (typical range for $b$ = 2-100 (10 maybe))      For example\n $b$ = 10 Get 10 examples from training set Perform gradient descent update using the ten examples    Mini-batch algorithm\n‚Äã\n We for-loop through $b$-size batches of $m$ Compared to batch gradient descent this allows us to get through data in a much more efficient way  After just $b$ examples we begin to improve our parameters Don\u0026rsquo;t have to update parameters after every example, and don\u0026rsquo;t have to wait until you cycled through all the data      Mini-batch gradient descent vs. stochastic gradient descent\n Why should we use mini-batch?  Allows you to have a vectorized implementation Means implementation is much more efficient Can partially parallelize your computation (i.e. do 10 at once)   A disadvantage of mini-batch gradient descent is the optimization of the parameter $b$  But this is often worth it!   To be honest, stochastic gradient descent and batch gradient descent are just specific forms of batch-gradient descent  For mini-batch gradient descent, $b$ is somewhere in between 1 and $m$ and you can try to optimize for it      34.4 Stochastic Gradient Descent Convergence  With batch gradient descent, we could plot cost function vs number of iterations  Should decrease on every iteration This works when the training set size was small because we could sum over all examples  Doesn\u0026rsquo;t work when you have a massive dataset     With stochastic gradient descent  We don\u0026rsquo;t want to have to pause the algorithm periodically to do a summation over all data Moreover, the whole point of stochastic gradient descent is to avoid those whole-data summations   For stochastic gradient descent, we have to do something different  Take cost function definition $\\operatorname{cost}\\left(\\theta,\\left(x^{(i)}, y^{(i)}\\right)\\right)=\\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$  One half the squared error on a single example   While the algorithm is looking at the example $\\left(x^{(i)}, y^{(i)}\\right)$, but before it has updated $\\theta$ we can compute the cost of the example $cost\\left(x^{(i)}, y^{(i)}\\right)$  i.e. we compute how well the hypothesis is working on the training example  Need to do this before we update $\\theta$ because if we did it after $\\theta$ was updated the algorithm would be performing a bit better (because we\u0026rsquo;d have just used $\\left(x^{(i)}, y^{(i)}\\right)$ to improve $\\theta$)     To check for the convergence, every 1000 iterations we can plot the costs averaged over the last 1000 examples  Gives a running estimate of how well we\u0026rsquo;ve done on the last 1000 estimates By looking at the plots we should be able to check convergence is happening        In general\n Might be a bit noisy (1000 examples isn\u0026rsquo;t that much)    If you get a figure like this\n That\u0026rsquo;s a pretty decent run Algorithm may have convergence    If you use a smaller learning rate you may get an even better final solution\n This is because the parameter oscillate around the global minimum A smaller learning rate means smaller oscillations    If you average over 1000 examples and 5000 examples you may get a smoother curve\n The disadvantage of a larger average means you get less frequent feedback    Sometimes you may get a plot that looks like this\n Looks like cost is not decreasing at all But if you then increase to averaging over a larger number of examples you do see this general trend  Means the blue line was too noisy, and that noise is ironed out by taking a greater number of entries per average   Of course, it may not decrease, even with a large number    If you see a curve the looks like its increasing then the algorithm may be displaying divergence\n Should use a smaller learning rate    Learning rate $\\alpha$ is typically held constant. Can slowly decrease over time if we want $\\theta$ to converge. (E.g. $\\alpha = \\frac{\\text{const1}}{\\text{iterationNumber}+\\text{const2}}$)\n  35 Advanced Topics 35.1 Online Learning   New setting\n Allows us to model problems where you have a continuous stream of data you want an algorithm to learn from Similar idea of stochastic gradient descent, in that you do slow updates Web companies use various types of online learning algorithms to learn from traffic  Can (for example) learn about user preferences and hence optimize your website      Example - Shipping service\n Users come and tell you origin and destination You offer to ship the package for some amount of money ($10 - $50) Based on the price you offer, sometimes the user uses your service ($y = 1$), sometimes they don\u0026rsquo;t ($y = 0$) Build an algorithm to optimize what price we offer to the users  Capture  Information about user Origin and destination   Work out  What the probability of a user selecting the service is   We want to optimize the price   To model this probability we have something like  $p(y=1|x;\\theta)$  Probability that $y =1$, given $x$, parameterized by $\\theta$   Build this model with something like  Logistic regression Neural network     If you have a website that runs continuously an online learning algorithm would do something like this  User comes - is represented as an $(x,y)$ pair where  $x$ - feature vector including price we offer, origin, destination $y$ - if they chose to use our service or not   The algorithm updates $\\theta$ using just the $(x,y)$ pair: $\\theta_j := \\theta_j - \\alpha \\left(h_{\\theta}(x) - y \\right)x_j \\quad (j=0, \\dots, n)$ So we basically update all the $\\theta$ parameters every time we get some new data   While in previous examples we might have described the data example as $(x^{(i)}, y^{(i)})$ for an online learning problem we discard this idea of a data \u0026ldquo;set\u0026rdquo; - instead we have a continuous stream of data so indexing is largely irrelevant as you\u0026rsquo;re not storing the data (although presumably you could store it)    If you have a major website where you have a massive stream of data then this kind of algorithm is pretty reasonable\n You\u0026rsquo;re free of the need to deal with all your training data    If you had a small number of users you could save their data and then run a normal algorithm on a dataset\n  An online algorithm can adapt to changing user preferences\n So over time users may become more price sensitive The algorithm adapts and learns to this So your system is dynamic    Another example - product search\n  Other things you can do\n Special offers to show the user Show news articles - learn what users like Product recommendation    These problems could have been formulated using standard techniques, but they are the kinds of problems where you have so much data that this is a better way to do things\n  35.2 Map Reduce and Data Parallelism   More generally map reduce uses the following scheme (e.g. where you split into 4)\n  The bulk of the work in gradient descent is the summation\n Now, because each of the computers does a quarter of the work at the same time, you get a $4\\times$ speedup Of course, in practice, because of network latency, combining results, it\u0026rsquo;s slightly less than $4\\times$, but still good!    Important thing to ask is: \u0026ldquo;Can algorithm be expressed as computing sums of functions of the training set?\u0026rdquo;\n Many algorithms can    More broadly, by taking algorithms which compute sums you can scale them to very large datasets through parallelization\n Parallelization can come from  Multiple machines Multiple CPUs Multiple cores in each CPU   So even on a single compute can implement parallelization    The advantage of thinking about Map Reduce here is because you don\u0026rsquo;t need to worry about network issues\n It\u0026rsquo;s all internal to the same machine    Finally caveat/thought\n Depending on implementation detail, certain numerical linear algebra libraries can automatically parallelize your calculations across multiple cores So, if this is the case and you have a good vectorization implementation you can not worry about local Parallelization and the local libraries sort optimization out for you    36 Photo OCR 36.1 Problem Description and Pipeline   Case study focused around Photo OCR\n  Three reasons to do this\n   Look at how a complex system can be put together    The idea of a machine learning pipeline   What to do next How to do it    Some more interesting ideas   Applying machine learning to tangible problems Artificial data synthesis      What is the photo OCR problem?\n Photo OCR = Photo Optical Character Recognition  With growth of digital photography, lots of digital pictures One idea which has interested many people is getting computers to understand those photos The photo OCR problem is getting computers to read text in an image  Possible applications for this would include  Make searching easier (e.g. searching for photos based on words in them) Car navigation       OCR of documents is a comparatively easy problem  From photos it\u0026rsquo;s really hard      OCR pipeline\n   Look through image and find text    Do character segmentation    Do character classification    some may do spell check after this too (Optional)   We\u0026rsquo;re not focusing on such systems though      Pipelines are common in machine learning\n Separate modules which may each be a machine learning component or data processing component    If you\u0026rsquo;re designing a machine learning system, pipeline design is one of the most important questions\n Performance of pipeline and each module often has a big impact on the overall performance a problem You would often have different engineers working on each module  Offers a natural way to divide up the workload      36.2 Sliding Windows   As mentioned, stage 1 is text detection\n Unusual problem in computer vision - different rectangles (which surround text) may have different aspect ratios (aspect ratio being height : width)  Text may be short (few words) or long (many words) Tall or short font Text might be straight on Slanted      Pedestrian detection\n  Want to take an image and find pedestrians in the image\n  This is a slightly simpler problem because the aspect ration remains pretty constant\n  Building our detection system\n  Have $82 \\times 36$ aspect ratio\n This is a typical aspect ratio for a standing human    Collect training set of positive and negative examples\n  Could have 1000 - 10 000 training examples\n  Train a neural network to take an image and classify that image as pedestrian or not\n Gives you a way to train your system      Now we have a new image - how do we find pedestrians in it?\n  Start by taking a rectangular $82 \\times 36$ patch in the image\n Run patch through classifier - hopefully in this example it will return $y = 0$    Next slide the rectangle over to the right a little bit and re-run\n Then slide again The amount you slide each rectangle over is a parameter called the step-size or stride  Could use 1 pixel  Best, but computationally expensive   More commonly 5-8 pixels used   So, keep stepping rectangle along all the way to the right  Eventually get to the end   Then move back to the left hand side but step down a bit too Repeat until you\u0026rsquo;ve covered the whole image    Now, we initially started with quite a small rectangle\n So now we can take a larger image patch (of the same aspect ratio) Each time we process the image patch, we\u0026rsquo;re resizing the larger patch to a smaller image, then running that smaller image through the classifier    Hopefully, by changing the patch size and rastering repeatedly across the image, you eventually recognize all the pedestrians in the picture\n      Text detection example\n  Like pedestrian detection, we generate a labeled training set with\n Positive examples (some kind of text) Negative examples (not text)    Having trained the classifier we apply it to an image\n So, run a sliding window classifier at a fixed rectangle size If you do that end up with something like this    White region show where text detection system thinks text is\n Different shades of gray correspond to probability associated with how sure the classifier is the section contains text  Black - no text White - text   For text detection, we want to draw rectangles around all the regions where there is text in the image    Take classifier output and apply an expansion algorithm\n  Takes each of white regions and expands it\n  How do we implement this\n Say, for every pixel, is it within some distance of a white pixel? If yes then color it white      Look at connected white regions in the image above\n  Draw rectangles around those which make sense as text (i.e. tall thin boxes don\u0026rsquo;t make sense)\n    This example misses a piece of text on the door because the aspect ratio is wrong\n Very hard to read        Stage 2: character segmentation\n  Use supervised learning algorithm\n  Look in a defined image patch and decide, is there a split between two characters?\n So, for example, our first training data item below looks like there is such a split Similarly, the negative examples are either empty or hold a full characters    We train a classifier to try and classify between positive and negative examples\n Run that classifier on the regions detected as containing text in the previous section    Use a 1-dimensional sliding window to move along text regions\n Does each window snapshot look like the split between two characters?  If yes insert a split If not move on   So we have something that looks like this      Character classification\n Standard OCR, where you apply standard supervised learning which takes an input and identify which character we decide it is  Multi-class characterization problem      36.3 Getting Lots of Data and Artificial Data   We\u0026rsquo;ve seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias algorithm and train on a massive data set\n Where do we get so much data from In ML artifice data synthesis  Doesn\u0026rsquo;t apply to every problem If it applies to your problem can be a great way to generate loads of data      Two main principles\n   Creating data from scratch    If we already have a small labeled training set can we amplify it into a larger training set      Character recognition as an example of data synthesis\n  If we go and collect a large labeled data set will look like this\n Goal is to take an image patch and have the system recognize the character Treat the images as gray-scale (makes it a bit easer)    How can we amplify this\n Modern computers often have a big font library If you go to websites, huge free font libraries For more training data, take characters from different fonts, paste these characters again random backgrounds    After some work, can build a synthetic training set\n Random background Maybe some blurring/distortion filters Takes thought and work to make it look realistic  If you do a sloppy job this won\u0026rsquo;t help! So unlimited supply of training examples   This is an example of creating new data from scratch    Other way is to introduce distortion into existing data\n  e.g. take a character and warp it\n 16 new examples Allows you amplify existing training set    This, again, takes though and insight in terms of deciding how to amplify\n      Another example: speech recognition\n Learn from audio clip - what were the words  Have a labeled training example Introduce audio distortions into the examples   So only took one example  Created lots of new ones!   When introducing distortion, they should be reasonable relative to the issues your classifier may encounter    Getting more data\n Before creating new data, make sure you have a low bias classifier  Plot learning curve   If not a low bias classifier increase number of features  Then create large artificial training set   Very important question: How much work would it be to get $10\\times$ data as we currently have?  Often the answer is, \u0026ldquo;Not that hard\u0026rdquo; This is often a huge way to improve an algorithm Good question to ask yourself or ask the team   How many minutes/hours does it take to get a certain number of examples  Say we have 1000 examples 10 seconds to label an example So we need another 9000 - 90000 seconds Comes to a few days (25 hours!)   Crowd sourcing is also a good way to get data  Risk or reliability issues Cost Example: Amazon Mechanical Turk (MTurk)      36.4 Celling Analysis: What Part of the Pipeline to Work on Next   Through the course repeatedly said one of the most valuable resources is developer time\n Pick the right thing for you and your team to work on Avoid spending a lot of time to realize the work was pointless in terms of enhancing performance    Photo OCR pipeline\n Three modules  Each one could have a small team on it Where should you allocate resources?   Good to have a single real number as an evaluation metric  So, character accuracy for this example Find that our test set has 72% accuracy      Ceiling analysis on our pipeline\n We go to the first module  Mess around with the test set - manually tell the algorithm where the text is Simulate if your text detection system was 100% accurate  So we\u0026rsquo;re feeding the character segmentation module with 100% accurate data now   How does this change the accuracy of the overall system Accuracy goes up to 89%   Next do the same for the character segmentation  Accuracy goes up to 90% now   Finally doe the same for character recognition  Goes up to 100%   Having done this we can qualitatively show what the upside to improving each module would be  Perfect text detection improves accuracy by 17%!  Would bring the biggest gain if we could improve   Perfect character segmentation would improve it by 1%  Not worth working on   Perfect character recognition would improve it by 10%  Might be worth working on, depends if it looks easy or not     The \u0026ldquo;ceiling\u0026rdquo; is that each module has a ceiling by which making it perfect would improve the system overall    Another example - face recognition\n  This is not how it\u0026rsquo;s done in practice\n  How would you do ceiling analysis for this\n     Conclusion  Supervised Learning  Linear regression, logistic regression, neural networks, SVMs   Unsupervised Learning  K-means, PCA, Anomaly detection   Special applications/special topics  Recommender systems, large scale machine learning   Advice on building a machine learning system  Bias/variance, regularization; deciding what to work on next: evaluation of learning algorithms, learning curves, error analysis, ceiling analysis   AND A BIG THANK YOU!  ","permalink":"https://fang-lansheng.github.io/posts/2020-06-25-ml-ng-10/","summary":"Course Link ÔºöWeek10 - Large Scale Machine Learning \u0026amp; Week11 - Application Example: Photo OCR\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data\n However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà10Ôºâ"},{"content":" Course Link ÔºöWeek9 - Anomaly Detection \u0026amp; Recommender Systems\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application\n Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?\n Imagine you\u0026rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you\u0026rsquo;re doing QA  Measure some features from engines (e.g. heat generated and vibration)   You now have a dataset of $x^1$ to $x^m$ (i.e. $m$ engines were tested)    More formally\n We have a dataset which contains normal (data)  How we ensure they\u0026rsquo;re normal is up to us In reality it\u0026rsquo;s OK if there are a few which aren\u0026rsquo;t actually normal   Using that dataset as a reference point we can see if other examples are anomalous    How do we do this?\n First, using our training dataset we build a model  We can access this model using $p(x)$  This asks, \u0026ldquo;What is the probability that example $x$ is normal\u0026rdquo;     Having built a model  if $p(x_{text}) \u0026lt; \\varepsilon$, flag this as an anomaly if $p(x_{text}) \\ge \\varepsilon$, this is OK $\\epsilon$ is some threshold probability value which we define, depending on how sure we need/want to be      Applications\n Fraud detection  $x^{(i)}$ = features of user $i$\u0026rsquo;s activities Model $p(x)$ from data Identify unusual user by checking which have $p(x) \u0026lt; \\varepsilon$   Manufacturing Monitoring computers in a data center  $x^{(i)}$ = features of machine $i$      28.2 Gaussian Distribution  Also called the normal distribution $X \\sim N(\\mu, \\sigma^2)$  $p(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left( - \\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$ $\\mu$: mean $\\sigma^2$: variance  $\\sigma$: the standard deviation   examples:    Parameter estimation  Dataset: $\\lbrace x^{(1)}, x^{(2)}, \\dots, x^{(m)} \\rbrace \\quad x^{(i)}\\in\\Bbb{R}$ $\\mu = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}$ $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m \\left( x^{(i)}-\\mu \\right)^2$  or $\\sigma^2 = \\frac{1}{m-1} \\sum_{i=1}^m \\left( x^{(i)}-\\mu \\right)^2$      28.3 Algorithm   Density estimation\n  Training set: $\\lbrace x^{(1)}, \\dots, x^{(m)}$, each example is $x \\in \\Bbb{R}^n$\n  $x_1 \\sim N(\\mu_1, \\sigma_1^2), \\dots, x_n \\sim N(\\mu_n, \\sigma_n^2)$\n  So\n$$ p(x) = p(x_1;\\mu_1, \\sigma_1^2)\\cdots p(x_n; \\mu_n, \\sigma^2_n) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) $$\n    Anomaly detection algorithm\n  Choose features $x_i$ that you think might be indicative of anomalous examples\n  Fit parameters $\\mu_1, \\dots, \\mu_n, \\sigma_1^2, \\dots, \\sigma_n^2$\n $$\\mu_j = \\frac{1}{m}\\sum_{i=1}^m x_j^{(i)}$$ $$ \\sigma^{2}{j} = \\frac{1}{m} \\sum{i=1}^m \\left( x^{(i)}_j-\\mu_j \\right)^2$$    Given new example $x$, compute $p(x)$:\n$$ p(x) = \\prod_{j=1}^n p(x_j;\\mu_j, \\sigma_j^2) = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma_j}\\exp \\left( -\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2} \\right) $$\n  Anomaly if $p(x) \u0026lt; \\varepsilon$\n    Anomaly detection example   29 Building an Anomaly Detection System 29.1 Developing and Evaluating an Anomaly Detection System   The importance of real-number evaluation\n When developing a learning algorithm (choosing features etc.), making decisions is much easier if we have a way of evaluating our learning algorithm Assume we have some labeled data, of anomalous and non-anomalous examples. ($y=0$ if normal, $y=1$ if anomalous) Training set: $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$ (assume normal examples/not anomalous) Cross validation set: $(x_{cv}^{(1)}, y_{cv}^{(1)}), \\dots, (x_{cv}^{(m_{cv})}, y_{cv}^{(m_{cv})})$ Test set: $(x_{test}^{(1)}, y_{test}^{(1)}), \\dots, (x_{test}^{(m_{test})}, y_{test}^{(m_{test})})$    Algorithm evaluation\n  Fit model $p(x)$ on training set $\\lbrace x^{(1)}, \\dots, x^{(m)} \\rbrace$\n  On a cross validation/test example $x$, predict\n$$ y = \\begin{cases} 1 \u0026amp; \\text{if $p(x) \u0026lt; \\varepsilon$ (anomaly)} \\ 0 \u0026amp; \\text{if $p(x) \\ge \\varepsilon$ (normal)} \\end{cases} $$\n    Possible evaluation metrics:\n True positive, false positive, false negative, true negative  Precision/Recall $F_1$-score      Can also use cross validation set to choose parameter $\\varepsilon$\n  29.2 Anomaly Detection vs. Supervised Learning 29.3 Choosing What Features to Use   Non-Gaussian features\n  Non-Gaussian data might look like this:\n  Can play with different transformations of the data to make it look more Gaussian:\n $x \\leftarrow \\log(x + c)$ $x \\leftarrow x^{\\text{small number}}$      Error analysis for anomaly detection\n Want:  $p(x)$ large for normal examples $x$ $p(x)$ small for anomalous examples $x$   Most common problems:  $p(x)$ is comparable (say, both large) for normal and anomalous examples   Solve (like supervised learning error analysis procedure):  Run algorithm on cross validation set See which one it got wrong Try coming up with more features to distinguish between the normal and the anomalous examples      30 Multivariate Gaussian Distribution 30.1 Multivariate Gaussian Distribution   Multivariate Gaussian (Normal) distribution\n  $x \\in \\Bbb{R}^n$. Don\u0026rsquo;t model $p(x_1), p(x_2), \\dots$, etc. separately.\n  Model $p(x)$ all in one go\n  Parameters: $\\mu \\in \\Bbb{R}^n$, $\\Sigma \\in \\Bbb{R}^{n \\times n}$ (covariance matrix)\n  Formula:\n$$ p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n    Multivariate Gaussian (Normal) examples:\n  30.2 Anomaly Detection using the Multivariate Gaussian Distribution   Multivariate Gaussian (Normal) distribution\n  Formula:\n$$ p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n Parameters $\\mu, \\Sigma$    Given training set $\\lbrace x^{(1)}, x^{(2)}, \\dots, x^{(m)} \\rbrace$\n  Parameter fitting\n  $$ \\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} $$\n  $$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} \\left( x^{(i)} - \\mu \\right)\\left( x^{(i)} - \\mu \\right)^T $$\n      Anomaly detection with the multivariate Gaussian\n  Fit model $p(x)$ by setting\n  $$ \\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} $$\n  $$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} \\left( x^{(i)} - \\mu \\right)\\left( x^{(i)} - \\mu \\right)^T $$\n    Given a new example $x$, compute\n$$ p(x )=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n    Flag an anomaly of $p(x) \u0026lt; \\varepsilon$\n  Relationship to original model\n  Original model:\n$$ p(x) = p(x_1; \\mu_1, \\sigma_1^2) \\times p(x_2; \\mu_2, \\sigma_2^2) \\times \\cdots \\times p(x_n; \\mu_n, \\sigma_n^2) $$\n  Corresponds to multivariate Gaussian:\n$$ p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) $$\n Where $$ \\Sigma = \\begin{bmatrix}\n\\sigma_{1}^2 \u0026amp; \u0026amp; \u0026amp; \\\n\u0026amp; \\sigma _2^2 \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\\n\u0026amp; \u0026amp; \u0026amp; \\sigma_n^2 \\\n\\end{bmatrix}\n$$      Comparison   31 Predicting Movie Ratings 31.1 Problem Formulation   Example: Predicting movie ratings\n  User rates movies using zero to five stars\n  And we have\n  Let\n $n_u$ = no. users $n_m$ = no. movies $r(i, j) = 1$ if user $j$ has rated movie $i$ $y(i, j)$ = rating given by user $j$ to movie $i$ (defined only if $r(i,j)=1$)    The problem is as follows\n Given $ r(i,j)$ and $y(i,j)$, go through and try and predict missing values Come up with a learning algorithm that can fill in these missing values      31.2 Content Based Recommendations   Using our example above, how do we predict?\n For each movie we have a feature which measure degree to which each film is a  Romance ($x_1$) Action ($x_2$)   If we have features like these, each film can be recommended by a feature vector  Add an extra feature which is $x_0 = 1$ for each film So for each film we have a $[3 \\times 1]$ vector, which for film number 1 (\u0026ldquo;Love at Last\u0026rdquo;) would be $x^{(1)} = \\begin{bmatrix} 1 \u0026amp; 0.9 \u0026amp; 0 \\end{bmatrix}^T$ i.e. for our dataset we have $\\lbrace x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}, x^{(5)} \\rbrace$  Where each of these is a $[3\\times 1]$ vector with an $x_0 = 1$ and then a romance and an action score     We could treat each rating for each user as a separate linear regression problem  For each user $j$ we could learn a parameter vector $\\theta^{(j)} \\in \\Bbb{R}^{n+1}$ Then predict that user $j$ will rate movie $i $ with $(Œ∏^{(j)})^T x^{(i)} = \\text{stars}$   All we\u0026rsquo;re doing here is applying a linear regression method for each user  So we determine a future rating based on their interest in romance and action based on previous films   We should also add one final piece of notation  $m^{(j)}$, number of movies rated by the user $j$      Problem formulation\n  Notations:\n $r(i, j) = 1$ if user $j$ has rated movie $i$ $y(i, j)$ = rating given by user $j$ to movie $i$ (if defined) $\\theta^{(j)} \\in \\Bbb{R}^{n+1}$ = parameters vector for user $j$ $x^{(i)}\\in \\Bbb{R}^{n+1}$ = feature vector for movie $i$ $m^{(j)}$ = no. of movies rated by user $j$    For user $j$, movie $i$, predict rating: $\\left( \\theta^{(j)} \\right)^T x^{(i)}$\n  To learn $\\theta^{(j)}$:\n$$ \\min_{\\theta^{(j)}} \\frac{1}{2m^{(j)}} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2 m^{(j)}} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n $m^{(j)}$ is a constant value - you can remove it      Optimization objective\n  To learn $\\theta^{(j)}$ (parameter for user $j$):\n$$ \\min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2 } \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n    To learn $\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(n_u)}$\n$$ \\min_{\\theta^{(1)}, \\dots, \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n  Optimization algorithm\n$$ \\min_{\\theta^{(1)}, \\dots, \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n  Gradient descent update\n  $$ \\theta_k^{(j)}:= \\theta_k^{(j)} - \\alpha \\sum_{i:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) x_k^{(i)} \\qquad \\text{for $k=0$} $$\n  $$ \\theta_k^{(j)}:= \\theta_k^{(j)} - \\alpha \\left( \\sum_{i:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) x_k^{(i)} + \\lambda \\theta^{(j)}_k \\right) \\qquad \\text{for $k \\ne 0$} $$\n      32 Collaborative Filtering 32.1 Collaborative Filtering   The collaborative filtering algorithm has a very interesting property - does feature learning\n i.e. it can learn for itself what features it needs to learn    Recall our original data set above for our five films and four raters\n Here we assume someone had calculated the \u0026ldquo;romance\u0026rdquo; and \u0026ldquo;action\u0026rdquo; amounts of the films  This can be very hard to do in reality Often want more features than just two      So - let\u0026rsquo;s change the problem and pretend we have a data set where we don\u0026rsquo;t know any of the features associated with the films\n Now let\u0026rsquo;s make a different assumption  We\u0026rsquo;ve polled each user and found out how much each user likes  Romantic films Action films   Which has generated the following parameter set Alice and Bob like romance but hate action Carol and Dave like action but hate romance      If we can get these parameters from the users we can infer the missing values from our table\n Lets look at \u0026ldquo;Love at Last\u0026rdquo;  Alice and Bob loved it Carol and Dave hated it   We know from the feature vectors Alice and Bob love romantic films, while Carol and Dave hate them  Based on the factor Alice and Bob liked \u0026ldquo;Love at Last\u0026rdquo; and Carol and Dave hated it we may be able to (correctly) conclude that \u0026ldquo;Love at Last\u0026rdquo; is a romantic film      This is a bit of a simplification in terms of the math, but what we\u0026rsquo;re really asking is\n What feature vector $x^{(1)}$ should be - so that  $(\\theta^{(1)})^T x_1$ is about 5 $(\\theta^{(2)})^T x_1$ is about 5 $(\\theta^{(3)})^T x_1$ is about 0 $(\\theta^{(4)})^T x_1$ is about 0   From this we can guess that $x^{(1)}$ may be $\\begin{bmatrix} 1 \u0026amp; 1.0 \u0026amp; 0.0 \\end{bmatrix}^T$ Using that same approach we should then be able to determine the remaining feature vectors for the other films    Formalizing the collaborative filtering problem\n  Given $\\theta^{(1)}, \\dots, \\theta^{(n_u)}$ (i.e. given the parameter vectors for each users\u0026rsquo; preferences), to learn $x^{(i)}$\n  We must minimize an optimization function which tries to identify the best parameter vector associated with a film\n$$ \\min_{x^{(i)}} \\frac{1}{2} \\sum_{j:r(i, j)=1)} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} \\left( x_k^{(i)} \\right)^2 $$\n So we\u0026rsquo;re summing over all the indices $j$ for where we have data for movie $i$ We\u0026rsquo;re minimizing this squared error    Like before, the above equation gives us a way to learn the features for one film\n We want to learn all the features for all the films - so we need an additional summation term      How does this work with the previous recommendation system\n Content based recommendation systems  Saw that if we have a set of features for movie rating you can learn a user\u0026rsquo;s preferences   Now  If you have your users preferences you can therefore determine a film\u0026rsquo;s features   This is a bit of a chicken \u0026amp; egg problem What you can do is  Randomly guess values for $\\theta$ Then use collaborative filtering to generate $x$ Then use content based recommendation to improve $\\theta$ Use that to improve $x$ And so on   This actually works  Causes your algorithm to converge on a reasonable set of parameters This is collaborative filtering   We call it collaborative filtering because in this example the users are collaborating together to help the algorithm learn better features and help the system and the other users    32.2 Collaborative Filtering Algorithm   Here we combine the ideas from before to build a collaborative filtering algorithm\n  Collaborative filtering optimization objective\n  If we\u0026rsquo;re given the film\u0026rsquo;s features we can use that to work out the users\u0026rsquo; preference:\n$$ \\text{Given $x^{(1)}, \\dots, x^{(n_m)} $, estimate $ \\theta^{(1)}, \\dots, \\theta^{(n_u)} $ :} \\ \\min_{\\theta^{(1)}, \\dots, \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 $$\n    If we\u0026rsquo;re given the users\u0026rsquo; preferences we can use them to work out the film\u0026rsquo;s features\n$$ \\text{Given $ \\theta^{(1)}, \\dots, \\theta^{(n_u)} $, estimate $x^{(1)}, \\dots, x^{(n_m)} $ :} \\ \\min_{x^{(1)}, \\dots, x^{(n_m)}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i, j)=1}\\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^n \\left(x_k^{(i)} \\right)^2 $$\n  One thing you could do is\n Randomly initialize parameter    Go back and forward\n    But there\u0026rsquo;s a more efficient algorithm which can solve $\\theta$ and $x$ simultaneously\n Define a new optimization objective which is a function of $x$ and $\\theta$  $$ \\text{Minimizing $ x^{(1)}, \\dots, x^{(n_m)}$ and $\\theta^{(1)}, \\dots, \\theta^{(n_u)} $ simultaneously :} \\\n\\begin{aligned} J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)}) \u0026amp;= \\frac{1}{2} \\sum_{(i, j):r(i, j)=1} \\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2 + \\ \u0026amp;= \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^n \\left(x_k^{(i)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n \\left( \\theta_k^{(j)} \\right)^2 \\end{aligned} $$\n  Goal:\n$$ \\min_{\\mathop{x^{(1)}, \\dots, x^{(n_m)}\\\\theta^{(1)}, \\dots, \\theta^{(n_u)}} }J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)}) $$\n    Understanding this optimization objective\n  The squared error term is the same as the squared error term in the two individual objectives above $\\sum_{(i, j):r(i, j)=1} \\left[\\left( \\theta^{(j)} \\right)^T x^{(i)} - y^{(i, j)} \\right]^2$\n So it\u0026rsquo;s summing over every movie rated by every users Note the \u0026ldquo;$:$\u0026rdquo; means, \u0026ldquo;for which\u0026rdquo;  Sum over all pairs $(i,j)$ for which $r(i,j)$ is equal to 1      The regularization terms\n Are simply added to the end from the original two optimization functions    This newly defined function has the property that\n If you held $x$ constant and only solved $\\theta$ then you solve the, \u0026ldquo;Given $x$, solve $\\theta$\u0026rdquo; objective above Similarly, if you held $\\theta$ constant you could solve $ x $    In order to come up with just one optimization function we treat this function as a function of both film features $x$ and user parameters $\\theta$\n Only difference between this in the back-and-forward approach is that we minimize with respect to both $x$ and $\\theta$ simultaneously    When we\u0026rsquo;re learning the features this way\n  Previously had a convention that we have an $x_0 = 1$ term\n  When we\u0026rsquo;re using this kind of approach we have no\n$x_0$,\n So now our vectors (both $x$ and $\\theta$) are $n$-dimensional (NOT $n+1$) i.e. $x \\in \\Bbb{R}^n, \\theta \\in \\Bbb{R}^n$    We do this because we are now learning all the features so if the system needs a feature always = 1 then the algorithm can learn one\n      Collaborative filtering algorithm\n   Initialize $\\theta^{(1)}, \\dots, \\theta^{(n_u)}$ and $x^{(1)}, \\dots, x^{(n_m)}$ to small random values   A bit like neural networks - initialize all parameters to small random numbers    Minimize cost function $J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)})$ using gradient descent  $$ \\text{for every $j=1, \\dots, n_u, i = 1, \\dots, n_m $ :} \\ x_k^{(i)}:= x_k^{(i)} - \\alpha \\left( \\sum_{j:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) \\theta^{(j)}k + \\lambda x_k^{(i)} \\right) \\ \\theta_k^{(j)}:= \\theta_k^{(j)} - \\alpha \\left( \\sum{i:r(i,j)=1} \\left( (\\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \\right) x_k^{(i)} + \\lambda \\theta^{(j)}_k \\right) $$\n  Having minimized the values, given a user (user $j$) with parameters $\\theta$ and movie (movie $i$) with learned features $x$, we predict a star rating of $\\theta^Tx$      33 Low Rank Matrix Factorization 33.1 Vectorization: Low Rank Matrix Factorization  low rank matrix factorization  $Y = \\begin{bmatrix} \\left(\\theta^{(1)}\\right)^{T}\\left(x^{(1)}\\right) \u0026amp; \\left(\\theta^{(2)}\\right)^{T}\\left(x^{(1)}\\right) \u0026amp; \\dots \u0026amp; \\left(\\theta^{\\left(n_{u}\\right)}\\right)^{T}\\left(x^{(1)}\\right) \\newline \\left(\\theta^{(1)}\\right)^{T}\\left(x^{(2)}\\right) \u0026amp; \\left(\\theta^{(2)}\\right)^{T}\\left(x^{(2)}\\right) \u0026amp; \\dots \u0026amp; \\left(\\theta^{\\left(n_{u}\\right)}\\right)^{T}\\left(x^{(2)}\\right) \\newline \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline \\left(\\theta^{(1)}\\right)^{T}\\left(x^{\\left(n_{m}\\right)}\\right) \u0026amp; \\left(\\theta^{(2)}\\right)^{T}\\left(x^{\\left(n_{m}\\right)}\\right) \u0026amp; \\dots \u0026amp; \\left(\\theta^{\\left(n_{u}\\right)}\\right)^{T}\\left(x^{\\left(n_{m}\\right)}\\right) \\end{bmatrix}$ $X = \\begin{bmatrix}-\\left( x^{(1)} \\right)^T- \\\\ \\cdots \\\\ -\\left( x^{(n_m)} \\right)^T- \\end{bmatrix}, \\Theta = \\begin{bmatrix}-\\left( \\theta^{(1)} \\right)^T- \\\\ \\cdots \\\\ -\\left( \\theta^{(n_u)} \\right)^T- \\end{bmatrix}$ $Y = X\\Theta^T$   Find related movies  For each product $i$, we learn a feature vector $x^{(i)} \\in \\Bbb{R}^n$ How to find movie $j$ related to movie $i$ ?  $\\text{small } |x^{(i)} - x^{(j)}| \\rightarrow \\text{movie $j$ and $i$ are \u0026ldquo;similar\u0026rdquo;}$      33.2 Implementational Detail: Mean Normalization Ex8: Anomaly Detection and Recommender Systemüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex8/ex8.ipynb\n ","permalink":"https://fang-lansheng.github.io/posts/2020-06-24-ml-ng-9/","summary":"Course Link ÔºöWeek9 - Anomaly Detection \u0026amp; Recommender Systems\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application\n Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?\n Imagine you\u0026rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you\u0026rsquo;re doing QA  Measure some features from engines (e.","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà9Ôºâ"},{"content":" Course Link ÔºöWeek8 - Unsupervised Learning \u0026amp; Dimensionality Reduction\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning \u0026amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook \u0026ldquo;smartlists\u0026rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24.2 K-Means Algorithm  Algorithm overview    Randomly allocate two points as the cluster centroids   Have as many cluster centroids as clusters you want to do ($K$ cluster centroids, in fact) In our example we just have two clusters, so there\u0026rsquo;re two cluster centroids (red and blue point)    Cluster assignment step   Go through each example and depending on if it\u0026rsquo;s closer the red or blue centroid assign each point to one of the two clusters To demonstrate this, we\u0026rsquo;ve gone through the data and \u0026ldquo;color\u0026rdquo; each point red or blue    Move centroid step   Take each centroid and move to the average of the correspondingly assigned data-points    Repeat 2) and 3) until convergence     More formal definition  Input:  $K$: number of clusters in the data Training set ${x^{(1)}, x^{(n)}, \\dots,x^{(m)}}$, $x^{(i)} \\in \\Bbb{R}^n$ (drop $x_0 = 1$ convention)   Algorithm:  Randomly initialize $K$ cluster centroids as ${\\mu_1, \\mu_2, \\dots, u_K \\in \\Bbb{R}^n }$      K-means for non-separated clusters  24.3 Optimization Objective   Notations:\n $c^{(i)}$ = index of cluster ($1, 2, \\dots, K$) to which example $x^{(i)}$ is currently assigned  $c^{(i)} = k \\in {1, 2, \\dots, K}$ $i \\in {1, 2, \\dots, m}$   $\\mu_k$ = cluster centroid k ($\\mu_k \\in \\Bbb{R}^n$) $\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned    Optimization objective\n  $$ J(c^{(1)}, \\dots, c^{(m)}, \\mu_1, \\dots, \\mu_K) = \\frac{1}{m} \\sum_{i=1}^m |x^{(i)} - \\mu_{c^{(i)}} |^2 $$\n  $$ \\mathop{\\mathop{\\text{min}}\\limits{c^{(1)}, \\dots, c^{(m)}}}\\limits{\\mu_1, \\dots, \\mu_K} J(c^{(1)}, \\dots, c^{(m)},\\mu_1, \\dots, \\mu_K) $$\n    If we consider the K-means algorithm\n The cluster assigned step is minimizing $J(\\cdot)$ with respect to $c^{(i)}$  i.e. find the centroid closet to each example Doesn\u0026rsquo;t change the centroids themselves   The move centroid step  This step is choosing the values of $\\mu$ which minimizes $J(\\cdot)$ with respect to $\\mu_k$   So, we\u0026rsquo;re partitioning the algorithm into two parts:  First part minimizes the $c$ variables Second part minimizes the $J$ variables      24.4 Random Initialization  Have number of centroids set to less than number of examples ($K \u0026lt; m$)  Randomly pick $K$ training examples Set $\\mu_1$ up to $\\mu_K$ to these examples values   K-means can converge to different solutions depending on the initialization setup  Risk of local optimum We can do multiple random initializations to see if we get the same result - many same result are likely to indicate a global optimum    24.5 Choosing the Number of Clusters  What is right value of $K$ ? Elbow method Vary $K$ and compute cost function at a range of $K$ values As $K$ increases, $J(\\cdot)$ minimum value should decrease (i.e. you decrease the granularity so centroids can better optimize) Plot $K$ vs. $J(\\cdot)$  Chose the \u0026ldquo;elbow\u0026rdquo; number of clusters (if you get a nice plot this is a reasonable way of choosing $K$) Risks: normally you don\u0026rsquo;t get a a nice line ‚Üí no clear elbow on curve   Another method for choosing $K$  Sometimes, you‚Äôre, running K-¬≠means to get clusters to use for some later/downstream purpose. Evaluate K¬≠‚Äêmeans based on a metric for how well it performs for that later purpose.    25 Motivation 25.1 Motivation I: Data Compression  A second type of unsupervised learning algorithm: dimensionality reduction Data compression Speeds up algorithms Reduces space used by data for them    25.2 Motivation II: Visualization  It\u0026rsquo;s hard to visualize highly dimensional data  Dimensionality reduction can improve how we display information in a tractable manner for human consumption Why do we care?  Often helps to develop algorithms if we can understand our data better Dimensionality reduction helps us do this, see data in a helpful manner Good for explaining something to someone if you can \u0026ldquo;show\u0026rdquo; it in the data      26 Principal Component Analysis 26.1 Principal Component Analysis Problem Formulation   For the problem of dimensionality reduction the most commonly used algorithm is PCA  Say we have a 2D data set which we wish to reduce to 1D\n  In other words, find a single line onto which to project this data. How do we determine this line?\n  The distance between each point and the projected version should be small (blue lines below are short)\n  PCA tries to find a lower dimensional surface so the sum of squares onto that surface is minimized\n  The blue lines are sometimes called the projection error\n PCA tries to find the surface (a straight line in this case) which has the minimum projection error    As an aside, you should normally do mean normalization and feature scaling on your data before PCA\n      A more formal description is\n For 2D-1D, we must find a vector $u^{(1)}$, which is of some dimensionality Onto which you can project the data so as to minimize the projection error  $u^{(1)}$ can be positive or negative ($-u^{(1)}$) which makes no difference  Each of the vectors define the same red line      In the more general case, To reduce from $n$-D to $k$-D we\n Find $k$ vectors ($u^{(1)} , u^{(2)}, \\dots, u^{(k)}$) onto which to project the data to minimize the projection error So lots of vectors onto which we project the data Find a set of vectors which we project the data onto the linear subspace spanned by that set of vectors  We can define a point in a plane with $k$ vectors   e.g. 3D ‚Üí 2D  Find pair of vectors which define a 2D plane (surface) onto which you\u0026rsquo;re going to project your data Much like the \u0026ldquo;shallow box\u0026rdquo; example in compression, we\u0026rsquo;re trying to create the shallowest box possible (by defining two of it\u0026rsquo;s three dimensions, so the box\u0026rsquo; depth is minimized)       PCA is NOT linear regression  For linear regression, fitting a straight line to minimize the straight line between a point and a squared line  Note: VERTICAL distance between point   For PCA minimizing the magnitude of the shortest orthogonal distance  Gives very different effects   More generally  With linear regression we\u0026rsquo;re trying to predict \u0026ldquo;$y$\u0026rdquo; With PCA there is no \u0026ldquo;$y$\u0026rdquo; - instead we have a list of features and all features are treated equally  If we have 3D dimensional data 3D-\u0026gt;2D  Have 3 features treated symmetrically          26.2 Principal Component Analysis Algorithm   Before applying PCA must do data preprocessing\n Mean normalization Feature scaling (depending on data)    With preprocessing done, PCA finds the lower dimensional sub-space which minimizes the sum of the square  Need to compute two things: $\\mu$ vectors \u0026amp; $z$ vectors    Algorithm description\n  Reducing data from $n$-dimensional to $k$-dimensional\n  Compute the covariance matrix: $\\Sigma = \\frac{1}{m}\\sum_{i=1}^n \\left(x^{(i)} \\right)\\left(x^{(i)} \\right)^T$\n $\\Sigma$: This is commonly denoted as Œ£ (Greek upper case sigma) - NOT summation symbol $\\Sigma = \\frac{1}{m} x^Tx$  $\\Sigma$ is an $n \\times n$ matrix $x^{(i)}$ is an $n \\times 1$ matrix      Compute eigenvectors of matrix $\\Sigma$: [U, S, V] = svd(sigma)\n svd = singular value decomposition    $U$, $S$ and $V$ are matrices\n  $U$ matrix is also an $n \\times n$ matrix\n  Turns out the columns of $U$ are the $u$ vectors we want\n  So to reduce a system from $n$-D to $k$-D: just take the first $k$ vectors from $U$: $$ U=\\left[\\begin{array}{cccc} | \u0026amp; | \u0026amp; \u0026amp; | \\ u^{(1)} \u0026amp; u^{(2)} \u0026amp; \\ldots \u0026amp; u^{(n)} \\ | \u0026amp; | \u0026amp; \u0026amp; | \\end{array}\\right] \\in \\mathbb{R}^{n \\times n} $$\n      Next we need to find some way to change $x$ (which is $n$ dimensional) to $z$ (which is $k$ dimensional) - reduce the dimensionality\n  Take first $k$ columns of the $u$ matrix and stack in columns: $$ U_{\\text{reduce}}=\\left[\\begin{array}{cccc} | \u0026amp; | \u0026amp; \u0026amp; | \\ u^{(1)} \u0026amp; u^{(2)} \u0026amp; \\ldots \u0026amp; u^{(k)} \\ | \u0026amp; | \u0026amp; \u0026amp; | \\end{array}\\right] \\in \\mathbb{R}^{n \\times k} $$\n  $z = U_{\\text{reduce}}^T * x=[k \\times n] * [n \\times 1] = [k \\times 1]$\n    Exactly the same as with supervised learning except we\u0026rsquo;re now doing it with unlabeled data\n  So in summary\n Preprocessing Calculate $\\Sigma$ (covariance matrix) Calculate eigenvectors with svd Take $k$ vectors from $U$, i.e. $U_{\\text{reduce}} = U(:,1:k)$ Calculate $z$ ($z =U_{\\text{reduce}}^T * x$)      27 Applying PCA 27.1 Reconstruction from compressed Representation   Earlier spoke about PCA as a compression algorithm\n If this is the case, is there a way to decompress the data from low dimensionality back to a higher dimensionality format?    Reconstruction\n  Say we have an example as follows:\n  We have our examples ($x^{(1)}$, $x^{(2)}$ etc.)\n  Project onto $z$-surface\n  Given a point $z^{(1)}$, how can we go back to the 2D space?\n    Considering\n $z = U_{\\text{reduce}}^T * x$    To go in the opposite direction we must do\n $x_{\\text{approx}} = U_{\\text{reduce}} * z,(=[n\\times k] * [k * 1] = [n \\times 1])$    So this creates the following representation\n  We lose some of the information (i.e. everything is now perfectly on that line) but it is now projected into 2D space\n  27.2 Choosing the Number of Principal Components   PCA tries to minimize average squared projection error: $$ \\frac{1}{m} \\sum_{i=1}^m | x^{(i)} - x^{(i)}_{\\text{approx}} |^2 $$\n  Total variationin data can be defined as the average over data saying how far are the training examples from the origin: $$ \\frac{1}{m} \\sum_{i=1}^m | x^{(i)} |^2 $$\n  Typically, choose $k$ to be smallest value so that $$ \\frac{\\frac{1}{m} \\sum_{i=1}^m | x^{(i)} - x^{(i)}{\\text{approx}} |^2}{\\frac{1}{m} \\sum{i=1}^m | x^{(i)} |^2} \\le 0.01 , (=1%) $$\n means \u0026ldquo;99% of variance is retained\u0026rdquo;    Algorithms:\n  Try PCA with $k = 1$\n  Compute $$ U_{\\text{reduce}}, z^{(1)}, \\dots,z^{(m)}, x^{(1)}{\\text{approx}}, \\dots,x^{(m)}{\\text{approx}} $$\n  check if the ratio mentioned above is less than (or equal to) 0.01?\n if not, try $k = 2, \\dots$      Easier way: [U, S, V] = svd(Sigma)\n  $S$ is an $n \\times n$ diagonal matrix: $$ S = \\begin{bmatrix} S_{11} \u0026amp; \u0026amp; \u0026amp; \\ \u0026amp; S_{22} \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \u0026amp; S_{nn} \\end{bmatrix} $$\n  And $$ \\frac{\\frac{1}{m} \\sum_{i=1}^m | x^{(i)} - x^{(i)}{\\text{approx}} |^2}{\\frac{1}{m} \\sum{i=1}^m | x^{(i)} |^2} = 1 - \\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} $$\n  Pick smallest value of $k$ which: $\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\ge 0.99$\n    27.3 Advice for Applying PCA  Speeding up supervised learning algorithm  Say you have a supervised learning problem  Input $x$ and $y$  $x$ is a 10,000 dimensional feature vector e.g. $100 \\times100$ images = 10,000 pixels Such a huge feature vector will make the algorithm slow   With PCA we can reduce the dimensionality and make it tractable How    Extract $x$   So we now have an unlabeled training set    Apply PCA to $x$ vectors   So we now have a reduced dimensional feature vector $z$    This gives you a new training set   Each vector can be re-associated with the label    Take the reduced dimensionality data set and feed to a learning algorithm   Use $y$ as labels and $z$ as feature vector    if you have a new example map from higher dimensionality vector to lower dimensionality vector, then feed into learning a algorithm       PCA maps one vector to a lower dimensionality vector  $x \\rightarrow z$ Defined by PCA only on the training set The mapping computes a set of parameters  Feature scaling values $U_{\\text{reduce}}$  Parameter learned by PCA Should be obtained only by determining PCA on your training set     So we use those learned parameters for our  Cross validation set Test set       Applications of PCA  Compression  Reduce memory/disk needed to store data Speed up learning algorithm   Visualization  Typically chose $k =2$ or $k = 3$ Because we can plot these values!   A bad use of PCA: Use it to prevent over-fitting  Reasoning  If we have $x^i$ we have $n$ features, $z^i$ has $k$ features which can be lower If we only have k features then maybe we\u0026rsquo;re less likely to over fit\u0026hellip;   This doesn\u0026rsquo;t work  BAD APPLICATION Might work OK, but not a good way to address over fitting Better to use regularization   PCA throws away some data without knowing what the values it\u0026rsquo;s losing  Probably OK if you\u0026rsquo;re keeping most of the data But if you\u0026rsquo;re throwing away some crucial data bad So you have to go to like 95-99% variance retained So here regularization will give you AT LEAST as good a way to solve over fitting     A second PCA myth  Used for compression or visualization - good Sometimes used to  Design ML system with PCA from the outset  But, what if you did the whole thing without PCA? (try it) See how a system performs without PCA When to use PCA: ONLY if you have a reason to believe PCA will help should you then add PCA PCA is easy enough to add on as a processing step  Try without first!            Ex7: K-Means Clustering and PCAüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex7/ex7.ipynb\n ","permalink":"https://fang-lansheng.github.io/posts/2020-06-22-ml-ng-8/","summary":"Course Link ÔºöWeek8 - Unsupervised Learning \u0026amp; Dimensionality Reduction\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning \u0026amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook \u0026ldquo;smartlists\u0026rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24.","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà8Ôºâ"},{"content":" Course Link ÔºöWeek7 - Support Vector Machines\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 21 Large Margin Classification 21.1 Optimization Objective  So far, we\u0026rsquo;ve seen a range of different algorithms  With supervised learning algorithms - performance is pretty similar  What matters often is:  The amount of training data Skill of applying algorithms       One final supervised learning algorithms that is widely used - Support Vector Machine (SVM)  Compared to both logistic regression and neural networks, a SVM sometimes gives a cleaner way of learning non-linear functions Later in the course we\u0026rsquo;ll do a survey of different supervised learning algorithms      Start with logistic regression, see how we can modify it to get the SVM\n  As before, the logistic regression hypothesis is as follows: $$ h_\\theta(x) \\frac{1}{1 + e^{-\\theta^Tx}} $$\n  And the sigmoid activation function looks like the one in the picture above.\n  In order to explain the math, we use $z$ as defined above\n    What do we want logistic regression to do?\n We have an example where $y = 1$  Then we hope $h_\\theta(x)$ is close to 1 With $h_\\theta(x)$ close to 1, $\\theta^Tx$ must be much largerthan 0   Similarly, when $y = 0$  Then we hope $h_\\theta(x)$ is close to 0 With $h_\\theta(x)$ close to 0, $\\theta^Tx$ must be much lessthan 0   This is our classic view of logistic regression, let\u0026rsquo;s consider another way of thinking about the problem     If you look at cost function, each example contributes a term (like the one in the picture) to the overall cost function If you then plug in the hypothesis definition $h_\\theta(x)$, you get an expanded cost function equation  So each training example contributes that term to the cost function for logistic regression   If $y = 1$ then only the first term in the objective matters. The plot of the function vs. $z$ shows that the cost contribution of an example when $y = 1$ given $z$  So if $z$ is big, the cost is low But if $z$ is 0 or negative the cost contribution is high This is why, when logistic regression sees a positive example, it tries to set $\\theta^Tx$ to be a very large term   If $y = 0$ then only the second term matters. We can again plot it and get a similar graph. Same deal,  If $z$ is small then the cost is low But if $z$ is large then the cost is massive   To build a SVM we must define our cost function  When $y = 1$  Take the $y = 1$ function and create a new cost function Instead of a curved line create two straight lines (magenta) which acts as an approximation to the logistic regression $y = 1$ function So this is the new $y=1$ cost function  Gives the SVM a computational advantage and an easier optimization problem We call this function $cost_1(z) $     Similarly, when $y = 0$  Do the equivalent with the $y=0$ function plot We call this function $cost_0(z)$     So here we define the two cost function terms for our SVM graphically  How do we implement this?      As a comparison/reminder we have logistic regression (as is shown in the picture above)\n  For the SVM we take our two logistic regression $ y=1$ and $y=0$ terms described previously and replace with $cost_1(\\theta^Tx)$ and $cost_0(\\theta^Tx)$\n  SVM notation is slightly different\n  21.2 Large Margin Intuition  Sometimes people refer to SVM as large margin classifiers  We\u0026rsquo;ll consider what that means and what an SVM hypothesis looks like The SVM cost function is as above, and we can draw out the cost terms: left is $cost_1$ and right is $cost_0$ What does it take to make terms small:  If $y=1$: $cost_1(z)=0$ only when $z \\ge 1$ if $y = 0$: $cost_0(z) = 0$ only when $z \\le -1$   Interesting property of SVM  In logistic regression, if you have a positive example, you only really need $z$ to be greater or equal to 0  If this is the case then you predict 1   SVM wants a bit more than that - doesn\u0026rsquo;t want to just get it right, but have the value be quite a bit bigger than zero  Throws in an extra safety margin factor       Logistic regression dose something similar   What are the consequences of this? Consider a case where we set C to be huge  $C = 100000$ So considering we\u0026rsquo;re minimizing $CA + B$  If $C$ is huge we\u0026rsquo;re going to pick an $A$ value so that $A$ is equal to zero What is the optimization problem here - how do we make $A = 0$?   Making $A = 0$  If $y = 1$: Then to make our \u0026ldquo;$A$\u0026rdquo; term 0 need to find a value of $\\theta$ so $(\\theta^T x)$ is greater than or equal to 1 Similarly, if $y = 0$: Then we want to make \u0026ldquo;$A$\u0026rdquo; term 0 then we need to find a value of $\\theta$ so $(\\theta^T x)$ is equal to or less than -1   So - if we think of our optimization problem a way to ensure that this first \u0026ldquo;$A$\u0026rdquo; term is equal to 0, we re-factor our optimization problem into just minimizing the \u0026ldquo;$B$\u0026rdquo; (regularization) term, because when $A = 0 \\rightarrow A*C = 0$ So we\u0026rsquo;re minimizing $B$, under the constraints shown above     Turns out when you solve this problem you get interesting decision boundaries  The green and magenta lines are functional decision boundaries which could be chosen by logistic regression  But they probably don\u0026rsquo;t generalize too well   The black line, by contrast is the the chosen by the SVM because of this safety net imposed by the optimization graph  More robust separator   Mathematically, that black line has a larger minimum distance (margin) from any of the training examples By separating with the largest margin you incorporate robustness into your decision making process     We looked at this at when $C$ is very large  SVM is more sophisticated than the large margin might look If you were just using large margin then SVM would be very sensitive to outliers You would risk making a ridiculous hugely impact your classification boundary  A single example might not represent a good reason to change an algorithm If $C$ is very large then we do use this quite na√Øve maximize the margin approach So we\u0026rsquo;d change the black to the magenta     What about non-linearly separable data?  Then SVM still does the right thing if you use a normal size $C$ So the idea of SVM being a large margin classifier is only really relevant when you have no outliers and you can easily linearly separable data    21.3 Mathematics Behind Large Margin Classification  Vector inner products   SVM Decision Boundary    The constraints we defined earlier:\n $\\theta^Tx \\ge 1 \\text{ if } y = 1$ $\\theta^Tx \\le -1 \\text{ if } y = 0$    Can be replaced/substituted with the constraints\n $p^i\\left|\\theta\\right| \\ge 1 \\text{ if } y = 1 $ $p^i\\left|\\theta\\right| \\le -1 \\text{ if } y = 0 $    Writing that into our optimization objective\n$$ \\begin{align*} \\min_\\theta \u0026amp; \\frac{1}{2}\\sum_{j=1}^n\\theta_j^2\\ \\mbox{s.t.} \u0026amp; |\\theta|\\cdot p^{(i)} \\geq 1\\quad \\mbox{if}\\ y^{(i)} = 1\\ \u0026amp; |\\theta|\\cdot p^{(i)} \\leq -1\\quad \\mbox{if}\\ y^{(i)} = 0 \\end{align*} $$\n  22 Kernels 22.1 Kernels I   What are kernels and how do we use them\n  We have a training set\n  We want to find a non-linear boundary\n  Come up with a complex set of polynomial features to fit the data\n Have $h_\\theta(x)$ which  Returns 1 if the combined weighted sum of vectors (weighted by the parameter vector) is less than or equal to 0 Else return 0   Another way of writing this (new notation) is  That a hypothesis computes a decision boundary by taking the sum of the parameter vector multiplied by a new feature vector $f$, which simply contains the various high order $x$ terms e.g., $h_\\theta(x) = \\theta_0 + \\theta_1f_1+\\theta_2f_2 + \\theta_3 f_3$, where $f_1 = x_1,f_2=x_1x_2,f_3=\\cdots$ (i.e. not specific values, but each of the terms from your complex polynomial function)   Is there a better choice of feature $f$ than the high order polynomials?  As we saw with computer imaging, high order polynomials become computationally expensive        New features\n  Define three features in this example (ignore $x_0$)\n  Have a graph of $x_1$ vs. $x_2$ (don\u0026rsquo;t plot the values, just define the space)\n  Pick three points in that space\n  These points $l^1$, $l^2$ and $l^3$, were chosen manually and are called landmarks\n Given $x$, define $f_1$ as the similarity between $(x, l^1)$: $f_1 = \\exp(-\\frac{|x-l^{(1)}|^2}{2\\sigma^2})$ If we remember our statistics, we know that: $\\sigma$ is the standard derivation, $\\sigma^2$ is commonly called the variance Remember, that as discussed: $|x-l^{(1)}|^2 = \\sum_{j =1}^n \\left( x_j - l_j^{(1)}\\right)^2$    So, $f_2$ is defined as:\n $f_2 = \\text{similarity}(x, l^2) = \\exp(-|x-l^{(2)}|^2/{2\\sigma^2})$    And similarly\n $f_3 = \\text{similarity}(x, l^3) = \\exp(-|x-l^{(3)}|^2/{2\\sigma^2})$    This similarity function is called a kernel\n The function is a Gaussian Kernel    So, instead of writing similarity between $x$ and $l$ we might write $f_1 = k(x, l^1)$\n    Driving deeper into the kernel\n So lets see what these kernels do and why the functions defined make sense  Say $x$ is close to a landmark,  Then the squared distance will be $\\sim0$ So $f_1 \\approx \\exp(-0^2/2\\sigma^2) \\approx e^{-0} \\approx 1$   Say $x$ is far from a landmark  Then the squared distance is big Gives $e^{\\text{- large number}} \\approx 0$   Each landmark defines a new features      22.2 Kernels II   Take the training data\n  For each example place a landmark at exactly the same location\n  So end up with $m$ landmarks\n One landmark per location per training example Means our features measure how close to a training set example something is    Given a new example, compute all the f values\n Gives you a feature vector $f$ ($f_0 \\cdots f_m$)  $f_0 = 1$ always      A more detailed look at generating the f vector\n  If we had a training example - features we compute would be using $(x_i, y_i)$\n  So we just cycle through each landmark, calculating how close to that landmark actually $x_i$ is\n $f_1^i, = k(x^i, l^1)$ $f_2^i, = k(x^i, l^2)$ $\u0026hellip;$ $f_m^i, = k(x^i, l^m)$    Somewhere in the list we compare $x$ to itself\u0026hellip; (i.e. when we\u0026rsquo;re at $f_i^i$)\n So because we\u0026rsquo;re using the Gaussian Kernel this evaluates to 1    Take these $m$ features $(f_1, f_2 \\cdots f_m)$ group them into an $[m +1 \\times 1]$ dimensional vector called $f$\n fi is the f feature vector for the $i^{\\text{th}}$ example And add a $0^{\\text{th}}$ term = 1      Given these kernels, how do we use a support vector machine\n    SVM hypothesis prediction with kernels\n Predict $y = 1$ if $\\theta^Tf \\ge 0$  Because $\\theta = [m + 1 \\times 1]$, $f = [m + 1 \\times 1]$   So, this is how you make a prediction assuming you already have $\\theta$  How do you get $\\theta$?      SVM training with kernels\n  Using the SVM learning algorithm $$ \\mathop{\\text{min}}\\limits{\\theta} C \\sum{i=1}^m y^{(i)}cost_1(\\theta^Tf^{(i)}) + (1 - y^{(i)})cost_0(\\theta^Tf^{(i)}) + \\frac{1}{2}\\sum_{j=1}^{n} \\theta^2_j $$\n Now, we minimizing using $f$ as the feature vector instead of $x$ By solving this minimization problem you get the parameters for your SVM    In this setup, $m = n$\n Because number of features is the number of training data examples we have    One final mathematic detail (not crucial to understand)\n If we ignore $\\theta_0$ then $\\sum_{j = 1}^n \\theta^2_j = \\theta^T\\theta$ is true What many implementations do is $\\theta^TM\\theta$  Where the matrix $M$ depends on the kernel you use Gives a slightly different minimization - means we determine a rescaled version of $\\theta$ Allows more efficient computation, and scale to much bigger training sets If you have a training set with 10,000 values, means you get 10,000 features  Solving for all these parameters can become expensive So by adding this in we avoid a for loop and use a matrix multiplication algorithm instead        You can apply kernels to other algorithms\n But they tend to be very computationally expensive But the SVM is far more efficient - so more practical    Lots of good off the shelf software to minimize this function, you don\u0026rsquo;t need to write your own\n    SVM parameters ($C$)\n Bias and variance trade off Must chose $C$:  $C$ plays a role similar to $1/\\lambda$ (where $\\lambda$ is the regularization parameter)   Large $C$ gives a hypothesis of low bias \u0026amp; high variance‚Üí overfitting Small $C$ gives a hypothesis of high bias \u0026amp; low variance‚Üí underfitting    SVM parameters ($\\sigma^2$)\n Parameters for calculating $f$ values Large $\\sigma^2$ ‚Üí $f$ features vary more smoothly ‚Üí higher bias, low variance Small $\\sigma^2$ ‚Üí $f$ features vary more abruptly ‚Üí lower bias, higher variance    23 SVMs in Practice 23.1 Using An SVM  Choosing a kernel  We\u0026rsquo;ve looked at the Gaussian kernel Need to define $\\sigma$ ($\\sigma^2$) When would you chose a Gaussian?  If $n$ is small and/or $m$ is large   If you\u0026rsquo;re using a Gaussian kernel then you may need to implement the kernel function Note: make sure you perform feature scaling before using a Gaussian kernel   Could use no kernel - linear kernel Predict $y = 1$ if $\\theta^Tx \\ge 0$  So no $f$ vector Get a standard linear classifier   Why do this?  If $n$ is large and $m$ is small then  Lots of features, few examples Not enough data - risk overfitting in a high dimensional feature-space       Other choice of kernel  Linear and Gaussian are most common Not all similarity functions are valid kernels  Must satisfy Mercer\u0026rsquo;s Theorem SVM use numerical optimization tricks  Mean certain optimization can be made, but they must follow the theorem     Polynomial Kernel: $(x^Tl + \\text{const})^{\\text{degree}}$  Usually performs worse than the Gaussian kernel Used when $x$ and $ l$ are both non-negative   String kernel Used if input is text strings Use for text classification   Chi-squared kernel Histogram intersection kernel     Multi-class classification for SVM  Many packages have built in multi-class classification packages Otherwise use one-vs all method   Logistic regression vs. SVM  When should you use SVM and when is logistic regression more applicable  If $n$ (features) is large vs. $m$ (training set) is small: Then use logistic regression or SVM with a linear kernel If $n$ is small and $m$ is intermediate: Gaussian kernel is good If $n$ is small and $m$ is large: SVM will be slow to run with Gaussian kernel  Manually create or add more features Use logistic regression of SVM with a linear kernel     Logistic regression and SVM with a linear kernel are pretty similar  Do similar things Get similar performance   A lot of SVM\u0026rsquo;s power is using different kernels to learn complex non-linear functions For all these regimes a well designed NN should work  But, for some of these problems a NN might be slower - SVM well implemented would be faster   SVM has a convex optimization problem - so you get a global minimum It\u0026rsquo;s not always clear how to chose an algorithm  Often more important to get enough data Designing new features Debugging the algorithm   SVM is widely perceived a very powerful learning algorithm    Ex6: Support Vector Machinesüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex6/ex6.ipynb\n ","permalink":"https://fang-lansheng.github.io/posts/2020-06-20-ml-ng-7/","summary":"Course Link ÔºöWeek7 - Support Vector Machines\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 21 Large Margin Classification 21.1 Optimization Objective  So far, we\u0026rsquo;ve seen a range of different algorithms  With supervised learning algorithms - performance is pretty similar  What matters often is:  The amount of training data Skill of applying algorithms       One final supervised learning algorithms that is widely used - Support Vector Machine (SVM)  Compared to both logistic regression and neural networks, a SVM sometimes gives a cleaner way of learning non-linear functions Later in the course we\u0026rsquo;ll do a survey of different supervised learning algorithms      Start with logistic regression, see how we can modify it to get the SVM","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà7Ôºâ"},{"content":" Course Link ÔºöWeek6 - Advice for Applying Machine Learning \u0026amp; Machine Learning System Design\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 16 Evaluating a Learning Algorithm 16.1 Deciding What to Try Next   Debugging a learning algorithm:\n  Suppose you have implemented regularized linear regression to predict housing prices.\n$$ J(\\theta) = \\frac{1}{2m}\\left[\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n\\theta_j^2 \\right] $$\n    However, when you test your hypothesis on a new set of houses, you find that it makes unacceptably large errors in its predictions. What should you try next?\n Get more training examples  Try smaller sets of features Try getting additional features Try adding polynomial features $(x_1^2,\\ x_2^2,\\ x_1x_2,\\ \\text{etc.})$ Try decreasing/increasing $\\lambda$      Machine learning diagnostic:\n Diagnostic: A test that you can run to gain insight what is/isn\u0026rsquo;t working with a learning algorithm, and gain guidance as to how best to improve its performance. Diagnostic can take time to implement, but doing so can be a very good use of your time.    16.2 Evaluating a Hypothesis   Once we have done some troubles shooting for errors in our predictions by:\n Getting more training examples Trying smaller sets of features Trying additional features Trying polynomial features Increasing or decreasing $\\lambda$  We can move on to evaluate our new hypothesis\n  A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70% of your data and the test set is the remaining 30%.\n  The new procedure using these two sets is then:\n Learn $\\Theta$ and minimize $J_{\\text{train}}(\\Theta)$ using the training set Compute the test set error $J_{\\text{test}}(\\Theta)$    The test set error\n  For linear regression:\n$$ J_{\\text{test}}(\\Theta) = \\frac{1}{2m_{\\text{test}}}\\sum_{i=1}^{m_\\text{test}}\\left[h_\\Theta(x_\\text{test}^{(i)}) - y_\\text{test}^{(i)}\\right]^2 $$\n  For classification - Misclassification error (aka 0/1 misclassification error):\n$$ \\text{err}\\left(h_\\Theta(x),\\ y\\right)=\\begin{cases} 1,\u0026amp;\\text{if $h_\\Theta(x)\\geq0.5$ \u0026amp; $y=0$ or $h_\\Theta(x)\u0026lt;0.5$ \u0026amp; $y=1$} \\ 0, \u0026amp; \\text{otherwise} \\end{cases} $$\nThis gives us a binary 0 or 1 error result based on a misclassification.\n  The average test error for the test set is:\n$$ \\text{Test Error} = \\frac{1}{m_\\text{test}}\\sum_{i=1}^{m_\\text{test}}\\text{err}\\left(h_\\Theta(x_\\text{test}^{(i)}),\\ y_\\text{test}^{(i)} \\right) $$\nThis gives us the proportion of the test data that was misclassified.\n    16.3 Model Selection and Train/Validation/Test Sets   Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor.\n  The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set.\n  Given many models with different polynomial degrees, we can use a systematic approach to identify the \u0026ldquo;best\u0026rdquo; function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.\n  One way to break down our dataset into the three sets is:\n Training set: 60% Cross validation set: 20% Test set: 20%    Train/validation/test error\n Training error:  $$ J_{\\text{train}}(\\theta)=\\frac{1}{2m}\\sum_{i=1}^m\\left[h_\\theta(x^{(i)})-y^{(i)} \\right]^2 $$\n Cross validation error:  $$ J_{\\text{cv}}(\\theta)=\\frac{1}{2m_\\text{cv}}\\sum_{i=1}^{m_\\text{cv}}\\left[h_\\theta(x^{(i)}\\text{cv})-y^{(i)}\\text{cv} \\right]^2 $$\n Test error:  $$ J_{\\text{test}}(\\theta)=\\frac{1}{2m_\\text{test}}\\sum_{i=1}^{m_\\text{test}}\\left[h_\\theta(x^{(i)}\\text{test})-y^{(i)}\\text{test} \\right]^2 $$\n  We can now calculate three separate error values for the three different sets using the following method:\n Optimize the parameters in $\\Theta$ using the training set for each polynomial degree. Find the polynomial degree $d$ with the least error using the cross validation set. Estimate the generalization error using the test set with $J_{\\text{test}}\\left(\\Theta^{(d)}\\right)$, $d$ = theta from polynomial with lower error.    This way, the degree of the polynomial $d$ has not been trained using the test set.\n  17 Bias vs. Variance 17.1 Diagnosing Bias vs. Variance   Bias/variance   In this section we examine the relationship between the degree of the polynomial $d$ and the underfitting or overfitting of our hypothesis.\n We need to distinguish whether bias or variance is the problem contributing to bad predictions. High bias is underfitting and hign variance is overfitting. Ideally, we need to find a golden mean between these two.    The training error will tend to decrease as we increase the degree $d$ of the polynomial.\n  At the same time, the cross validation error will tend to decrease as we increase $d$ up to a point, and then it will increase as $d$ is increased, forming a convex curve.\n High bias (underfitting): both $J_{\\text{train}}(\\Theta)$ and $J_{\\text{CV}}(\\Theta)$ will be high. Also, $J_{\\text{CV}}(\\Theta) \\approx J_{\\text{train}}(\\Theta)$. High variance (overfitting): $J_{\\text{train}}(\\Theta)$ will be low and $J_{\\text{CV}}(\\Theta)$ will be much greater than $J_{\\text{train}}(\\Theta)$.  This is summarized in the figure below:   17.2 Regularization and Bias/Variance In the figure above (The regularization term should be $\\frac{\\lambda}{2m} \\sum_{j=1}^n\\theta_j^2$), we see that as $\\lambda$ increases, our fit becomes more rigid. On the other hand, as $\\lambda$ approaches 0, we tend to overfit the data. So how do we choose our parameter $\\lambda$ to get it \u0026ldquo;just right\u0026rdquo;? In order to choose the model and the regularization term $\\lambda$, we need to:\n Create a list of lambdas (e.g. $\\lambda \\in \\lbrace 0,,0.01,,0.02,,0.04,,0.08,,\\dots,10.24 \\rbrace$). Create a set of models with different degrees or any other variants. Iterate through the $\\lambda $s and for each $\\lambda$ go through all the models to learn some $\\Theta$. Compute the cross validation error using the learned $\\Theta$ (computed with $ \\lambda $) on the $J_{\\text{CV}}(\\Theta)$ without regularization or $\\lambda = 0$. Select the best combo that produces the lowest error on the cross validation set. Using the best combo $\\Theta$ and $\\lambda$, apply it on $J_{\\text{test}}(\\Theta)$ to see if it has a good generalization of the problem.  17.3 Learning Curves   Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence:\n As the training set gets larger, the error for a quadratic function increases. The error value will plateau out after a certain $m$, or training set size.    Experiencing high bias:\n Low training set size: causes $J_{\\text{train}}(\\Theta)$ to be low and $J_{\\text{CV}}(\\Theta)$ to be high. Large training set size: causes both $J_{\\text{train}}(\\Theta)$ and $J_{\\text{CV}}(\\Theta)$ to be high with $J_{\\text{train}}(\\Theta)\\approx J_{\\text{CV}}(\\Theta)$. If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.    Experiencing high variance:\n Low training set size: $J_{\\text{train}}(\\Theta)$ will be low and $J_{\\text{CV}}(\\Theta)$ will be high. Large training set size: $J_{\\text{train}}(\\Theta)$ increases with training set size and $J_{\\text{CV}}(\\Theta)$ continues to decrease without leveling off. Also, $J_{\\text{train}}(\\Theta)\u0026lt; J_{\\text{CV}}(\\Theta)$ but the difference between them remains significant. If a learning algorithm is suffering from high variance, getting more training data is likely to help.    17.4 Deciding What to Do Next (Revisited)  Our decision process can be broken down as follows:  Getting more training examples: Fixes high variance Trying smaller sets of features: Fixes high variance Adding features: Fixes high bias Adding polynomial features: Fixes high bias Decreasing $\\lambda$: Fixes high bias Increasing $\\lambda$: Fixes high variance   Diagnosing Neural Networks  A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase $\\lambda$) to address the overfitting. Using a single hidden layer is a good starting default. You can train you neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best.   Model complexity effects  Lower-order polynomial (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently. Higher-order polynomial (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.    18 Building a Spam Classifier 18.1 Prioritizing What to Work On  Supervised learning.  $x = \\text{features of email}$. Choose 100 words indicative of spam/not spam  Note: In practice, take most frequently occurring $n$ words (10,000 to 50,000) in training set, rather that manually pick 100 words.   $y =1 \\text{ (spam), or } 0 \\text{ (not spam)} $   Building a spam classifier: How to spend your time to make it have low error?  Collect lots of data (for example \u0026ldquo;honeypot\u0026rdquo; project but doesn\u0026rsquo;t always work). Develop sophisticated features based on email routing information (from email header) Develop sophisticated features for message body, e.g. should \u0026ldquo;discount\u0026rdquo; and \u0026ldquo;discounts\u0026rdquo; be treated as the same word? How about \u0026ldquo;deal\u0026rdquo; and \u0026ldquo;Dealer\u0026rdquo;? Features about punctuation? Develop sophisticated algorithm to detect misspellings (e.g. m0rtage, med1cine, w4tches).    18.2 Error Analysis  Recommended approach  Start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation data. Plot learning curves to decide if more data, more features, etc. are likely to help. Error analysis: Manually examine the examples (in cross validation set) that your algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.   Error analysis The importance of numerical evaluation  19 Handling Skewed Data 19.1 Error Metrics for Skewed Classes   Cancer classification example\n Train logistic regression model $h_{\\theta}(x)$. $y = 1 \\text{ (if cancer)}$, $y=0 \\text{ (otherwise)}$ Find that you got 1% error on test set (99% correct diagnose)    Precision/Recall\n  $y = 1$ in presence of rare class that we want to detect\n  Precision: of all where we predicted $y = 1$, what fraction actually has cancer? $$ \\text{Precision} = \\frac{\\text{True positives}}{\\text{# predicted as positive}} = \\frac{\\text{True positives}}{\\text{True positives + False positives}} $$\n    Recall: of all patients that actually have cancer, what fraction did we correctly detect as having cancer? $$ \\text{Recall} = \\frac{\\text{True positives}}{\\text{# actual positives}} = \\frac{\\text{True positives}}{\\text{True positives + False negatives}} $$\n  19.2 Trading off Precision and Recall 20 Using Large Data Sets 20.1 Data For Machine Learning  Designing a high accuracy learning system  E.g. Classify between confusable words {to, two, too}, {then than}. Algorithms:  Perceptron (logistic regression) Winnow Memory-based Na√Øve Bayes     Large data rationale  Assume feature $x \\in \\Bbb{R}^{n + 1}$ has sufficient information to predict $y$ accurately. Use a learning algorithm with many parameters (e.g. logistic regression/linear regression with many features; neural network with many hidden units) ‚Üí low bias. Use a very large training set (unlikely to overfit) - low variance.    Ex5: Regularized Linear Regression and Bias v.s. Varianceüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex5/ex5.ipynb\n Instruction  Regularization Linear Regression: In the first half of the exercise, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir. In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias v.s. variance.  Visualizing the dataset Regularized linear regression cost function Regularized linear regression gradient Fitting linear regression   Bias-variance: An important concept in machine learning is the bias-variance tradeoff. Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data. In this part of the exercise, you will plot training and test errors on a learning curve to diagnose bias-variance problems.  Learning curves   Polynomial regression: The problem with our linear model was that it was too simple for the data and resulted in underfitting (high bias). In this part of the exercise, you will address this problem by adding more features.  Learning Polynomial Regression Adjusting the regularization parameter Selecting $\\lambda$ using a cross validation set Computing test set error Plotting learning curves with randomly selected examples    ","permalink":"https://fang-lansheng.github.io/posts/2020-03-22-ml-ng-6/","summary":"Course Link ÔºöWeek6 - Advice for Applying Machine Learning \u0026amp; Machine Learning System Design\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 16 Evaluating a Learning Algorithm 16.1 Deciding What to Try Next   Debugging a learning algorithm:\n  Suppose you have implemented regularized linear regression to predict housing prices.\n$$ J(\\theta) = \\frac{1}{2m}\\left[\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n\\theta_j^2 \\right] $$\n    However, when you test your hypothesis on a new set of houses, you find that it makes unacceptably large errors in its predictions.","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà6Ôºâ"},{"content":" Course Link ÔºöWeek 5 - Neural Networks: Learning\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 14 Cost Function and Backpropagation 14.1 Cost Function   Neural Network (Classification)\n Training Set: $\\lbrace \\left( x^{(1)}, y^{(1)} \\right), \\left( x^{(2)}, y^{(2)} \\right), \\dots, \\left( x^{(m)}, y^{(m)} \\right) \\rbrace$  $L$ = total number of layers in the network $s_l$ = number of units (not counting bias unit) in layer $l$ $K$ = number of output units/classes   Binary classification: $y = 0 \\text{ or } 1$ (1 output unit) Multi-class classification (K classes): $y \\in \\mathbb{R}^K$ (K output units)    Cost function\n  Logistic regression:\n$$ J(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^my^{(i)}\\log h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log (1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta^2_j $$\n    Neural network: ($h_\\Theta(x)\\in \\mathbb{R}^K, \\left( h_\\Theta\\left(x\\right) \\right)_k = k^{\\text{th}} \\text{ output}$)\n$$ \\begin{align} J(\\Theta) =\u0026amp; -\\frac{1}{m}\\left[ \\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}\\log\\left(h_\\Theta(x^{(i)})\\right)k + \\left( 1- y_k^{(i)} \\right)\\log\\left(1 - (h\\Theta(x^{(i)}))k\\right) \\right] \\ \u0026amp; +\\frac{\\lambda}{2m}\\sum{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}\\left( \\Theta_{ji}^{(l)} \\right)^2 \\end{align} $$\n We have added a few nested summations to account for our multiple output nodes.  In the first part of the equation, before the square brackets, we have an additional nested summations that loops through the number of output nodes. In the regularization part, after the square brackets, we must account for multiple matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.      Note:\n The double sum simply adds up the logistic regression costs calculated for each cell in the output layer The triple sum simply adds up the squares of all the individual $\\Theta$(s) in the entire network The $i$ in the triple sum does not refer to training example $i$    14.2 Backpropagation Algorithm   Backpropagationis a neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.\n  Our goal is to compute $\\mathop{\\text{min}}_\\limits{\\Theta}J(\\Theta)$\n  That is, we want to minimize our cost function $J$ using an optimal set of parameters in theta. In this section we\u0026rsquo;ll look at the equations we use to compute the partial derivative of $J(\\Theta)$: $\\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta)$\n  To do so, we use the following algorithm:   Backpropagation Algorithm\n  Given training set $\\lbrace \\left( x^{(1)}, y^{(1)} \\right), \\left( x^{(2)}, y^{(2)} \\right), \\dots, \\left( x^{(m)}, y^{(m)} \\right) \\rbrace$\n  Set $\\Delta_{i,j}^{(l)}:=0$ for all $(l,i,j)$ (hence you end up having a matrix full of zeros)\n  For training example $i=1\\text{ to } m$\n  Set $a^{(1)} := x^{(i)}$\n  Perform forward propagation to compute $a^{(l)}$ for $l = 2, 3, \\dots,L$   Using $y^{(i)}$, compute $\\delta^{(L)}=a^{(L)}-y^{(L)}$\n $L$ is our total number of layers $a^{(L)}$ is the vector of outputs of the activation units for the last layer $\\delta_j^{(l)}$ = \u0026ldquo;error\u0026rdquo; of node $j$ in layer $l$ So our \u0026ldquo;error values\u0026rdquo; for the last layer are simply the differences of our actual results in the last layer and the correct outputs in $y$.    To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left.\n  Compute $\\delta^{(L-1)}, \\delta^{(L-2)}, \\dots, \\delta^{(2)}$ using\n$$ \\delta^{(l)} = \\left(\\left( \\Theta^{(l)} \\right)^T\\delta^{(l+1)}\\right) * a^{(l)} * \\left( 1-a^{(l)} \\right) $$\n  The delta values of layer $l$ are calculated by multiplying the delta values in the next layer with the theta matrix of layer $l$\n  We then element-wise multiply that with a function called $g^\\prime$, which is the derivative of the activation function $g$ evaluated with the input values given by $z^{(l)}$\n  The $g^\\prime$ derivative terms can also be written out as:\n$$ g^\\prime\\left(z^{(l)}\\right) = a^{(l)}*\\left( 1 - a^{(l)} \\right) $$\n      $$\\Delta^{(l)}{i,j}:=\\Delta^{(l)}{i,j}+a_j^{(l)}\\delta_i^{(l+1)}$$ or with vectorization, $$\\Delta^{(l)}:=\\Delta^{(l)}+\\delta^{(l+1)}\\left(a^{(l)}\\right)^T$$\n  Hence we update our new $\\Delta$ matrix.\n$$ D^{(l)}{i,j}:=\\begin{cases} \\dfrac{1}{m}\\left(\\Delta^{(l)}{i,j} + \\lambda\\Theta^{(l)}{i,j}\\right), \u0026amp; \\text{if $j\\neq0$} \\ \\dfrac{1}{m}\\Delta^{(l)}{i,j}, \u0026amp; \\text{if $j=0$} \\end{cases} $$\nThe capital-delta matrix $D$ is used as an \u0026ldquo;accumulator\u0026rdquo; to add up our values as we go along and eventually compute our partial derivative. Thus we get $\\frac \\partial {\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{i,j}^{(l)}$\n    14.3 Backpropagation Intuition   Recall that the cost function for a neural network is:\n$$ \\begin{align} J(\\Theta) =\u0026amp; -\\frac{1}{m}\\left[ \\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}\\log\\left(h_\\Theta(x^{(i)})\\right)k + \\left( 1- y_k^{(i)} \\right)\\log\\left(1 - \\left(h\\Theta(x^{(i)})\\right)k\\right) \\right] \\ \u0026amp; +\\frac{\\lambda}{2m}\\sum{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}\\left( \\Theta_{j,i}^{(l)} \\right)^2 \\end{align} $$\n  If we consider simple non-multiclass classification ($k=1$) and disregard regularization, the cost is computed with:\n$$ \\text{cost}(i) =y^{(i)}\\log\\left(h_\\Theta(x^{(i)})\\right) +\\left( 1- y^{(i)} \\right)\\log\\left(1 - h_\\Theta(x^{(i)})\\right) $$\n  Intuitively, $\\delta_j^{(l)}$ is the \u0026ldquo;error\u0026rdquo; for $a_j^{(l)}$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function:\n$$ \\delta_j^{(l)} = \\frac{\\partial}{\\partial z_j^{(l)}}\\text{cost}(i) $$\n  Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some $\\delta_j^{(l)}$: In the image above, to calculate $\\delta_2^{(2)}$, we multiply the weights $\\Theta_{12}^{(2)}$ and $\\Theta_{22}^{(2)}$ by their respective $\\delta$ values found to the right of each edge. So we get $$\\delta_2^{(2)} = \\Theta_{12}^{(2)}\\delta_1^{(3)} + \\Theta_{22}^{(2)}\\delta_2^{(3)}$$. To calculate every single possible $$\\delta_j^{(l)}$$, we could start from the right of our diagram. We can think of our edges as our $$\\Theta_{ij}$$. Going from right to left, to calculate the value of $$\\delta_j^{(l)}$$, you can just take the over all sum of each weight times the $$\\delta$$ it is coming from. Hence, another example would be $$\\delta_2^{(3)} = \\Theta_{12}^{(3)}*\\delta_1^{(4)}$$.\n  15 Backprogation in Practice 15.1 Implementation Note: Unrolling Parameters   With neural networks, we are working with sets of matrices:\n$$ \\begin{align} \\Theta^{(1)},\\Theta^{(2)},\\Theta^{(3)},\\dots \\ D^{(1)},D^{(2)},D^{(3)},\\dots \\end{align} $$\n  In order to use optimizing functions such as fminunc(), we will want to \u0026ldquo;unroll\u0026rdquo; all the elements and put them into one vector.   15.2 Gradient Checking   Gradient Checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with:\n$$ \\dfrac{\\partial}{\\partial\\Theta}J(\\Theta) \\approx \\dfrac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2\\epsilon} $$\n  With multiple theta matrices, we can approximate the derivative with respect to $\\Theta_j$ as follows:\n$$ \\dfrac{\\partial}{\\partial\\Theta_j}J(\\Theta) \\approx \\dfrac{J(\\Theta_1, \\dots, \\Theta_j + \\epsilon, \\dots, \\Theta_n) - J(\\Theta_1, \\dots, \\Theta_j - \\epsilon, \\dots, \\Theta_n)}{2\\epsilon} $$\n  A small value for $\\epsilon$ such as $\\epsilon = 10^{-4}$, gurantees that the math works out properly. If the value for $\\epsilon$ is too small, we can end up with numerical problems.\n  Hence, we are only adding or subtracting epsilon to $\\Theta_j$ matrix.\nWe previously saw how to calculate the $\\text{deltaVector}$. So once we compute our $\\text{gradApprox}$ vector, we can check that $\\text{gradApprox $\\approx$ deltaVector}$\n  Implementation Note:\n Implement backprop to compute $\\text{DVec}$ (Unrolled $D^{(1)}, D^{(2)}, D^{(3)}$). Implement numerical gradient check to compute $\\text{gradApprox}$. Make sure they give similar values. Turn off gradient checking. Using backprop code for learning.    Important: Be sure to disable your gradient checking code before training your classifier. The code to compute $\\text{gradApprox}$ can be very slow.\n  15.3 Random Initialization  Initializing all theta weights to zero does not work with neural networks. When we backpropagete, all nodes will update to the same value repeatedly. Instead wo can randomly initialize our weights for our $\\Theta$ matrices using the following method:  Hence, we initialize each $\\Theta_{ij}^{(l)}$ to a random value between $[-\\epsilon, \\epsilon]$. Using the above formula gurantees that we get the desired bound. The same procedure applies to all the $\\Theta$\u0026rsquo;s. Note: the epsilon used above is unrelated to the epsilon from Gradient Checking.  15.4 Putting It Together  First, pick a network architecture. Choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.  Number of input units = dimension of features $x^{(i)}$ Number of output units = number of classes Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units) Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.   Training a neural network  Randomly initialize the weights. Implement forward propagation to get $h_\\Theta\\left(x^{(i)}\\right)$ for any $x^{(i)}$. Implement the cost function. Implement backpropagation to compute partial derivatives. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.   When we perform forward and back propagation, we loop on every training example:  The following image gives us an intuition of what is happening as we are implements our neural network: Ideally, you want $h_\\Theta\\left(x^{(i)}\\right) \\approx y^{(i)}$. This will minimize our cost function. However, keep in mind that $J(\\Theta)$ is not convex and thus we can end up in a local minimum instead.  Ex4: Neural Networks Learningüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex4/ex4.ipynb\n Ex4.1\u0026amp;4.2 Neural Networks\u0026amp;Backpropagation Instruction:\nIn the previous exercise, you implemented feedforward propagation for neural networks and used it to predict handwritten digits with the weights we provided. In this exercise, you will implement the backpropagation algorithm to learn the parameters for the neural network.\nCode:\nimport numpy as np import matplotlib.pyplot as plt import scipy.io as scio import scipy.optimize as opt \u0026#39;\u0026#39;\u0026#39; Part 0: Functions and Parameters \u0026#39;\u0026#39;\u0026#39; # Load training data def loadData(path): data = scio.loadmat(path) X, y = data[\u0026#39;X\u0026#39;], data[\u0026#39;y\u0026#39;] m = len(y) return X, y, m # Randomly select several data points to display def randomDisplay(data, num=100, cmap=\u0026#39;binary\u0026#39;, transpose=True): sample = data[np.random.choice(len(data), num)] size, size1 = int(np.sqrt(num)), int(np.sqrt(sample[0].shape[0])) fig0, ax0 = plt.subplots(nrows=size, ncols=size, sharex=True, sharey=True, figsize=(8, 8)) order = \u0026#39;F\u0026#39; if transpose else \u0026#39;C\u0026#39; for i in range(size): for j in range(size): ax0[i, j].imshow(sample[size * i + j].reshape([size1, size1], order=order), cmap=cmap) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.subplots_adjust(wspace=0, hspace=0) plt.suptitle(str(num)+\u0026#39; examples from the dataset\u0026#39;, fontsize=24) # Implements the neural network cost function for a two layer neural network # which performs classification def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg_param): # Reshape nn_params back into the parameters Theta1 and Theta2, # the weight matrices for our 2 layer neural network Theta1 = nn_params[:hidden_layer_size * (input_layer_size + 1)].\\ reshape([hidden_layer_size, (input_layer_size + 1)], order=\u0026#39;F\u0026#39;) Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].\\ reshape([num_labels, (hidden_layer_size + 1)], order=\u0026#39;F\u0026#39;) # Setup some useful variables m = X.shape[0] # Feedforward Propagation a1 = np.c_[np.ones([X.shape[0], 1]), X] z2 = np.dot(a1, Theta1.T) a2 = np.c_[np.ones([z2.shape[0], 1]), sigmoid(z2)] z3 = np.dot(a2, Theta2.T) a3 = sigmoid(z3) y = np.eye(num_labels)[y.reshape(-1) - 1] # Add regularization term cost1 = - np.sum((np.log(a3) * y) + np.log(1 - a3) * (1 - y)) / m cost2 = 0.5 * reg_param * (np.sum(np.square(Theta1[:, 1:])) + np.sum(np.square(Theta2[:, 1:]))) / m cost = cost1 + cost2 # Backpropagation delta3 = a3 - y delta2 = np.dot(delta3, Theta2)[:, 1:] * sigmoidGradient(z2) Delta1, Delta2 = np.dot(delta2.T, a1), np.dot(delta3.T, a2) # Add regularization to gradient Theta1_grad = Delta1 / m Theta1_grad[:, 1:] += reg_param * Theta1[:, 1:] / m Theta2_grad = Delta2 / m Theta2_grad[:, 1:] += reg_param * Theta2[:, 1:] / m # Unroll gradients grad = np.r_[Theta1_grad.ravel(order=\u0026#39;F\u0026#39;), Theta2_grad.ravel(order=\u0026#39;F\u0026#39;)] return cost, grad # Sigmoid function def sigmoid(z): return 1 / (1 + np.exp(-z)) # sigmoidGradient() returns the gradient of the sigmoid function evaluated at z def sigmoidGradient(z): return sigmoid(z) * (1 - sigmoid(z)) # Randomly initialize the weights of a layer with L_in incoming connections and L_out outgoing connections def randInitWeights(L_in, L_out, epsilon_init=0.12): # epsilon_init = np.sqrt(6) / np.sqrt(L_in + L_out) W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init return W # Initialize the weights of a layer with fan_in incoming connections and fan_out # outgoing connections using a fixed strategy, this will help you later in debugging def debugInitWeights(fan_in, fan_out): # Initialize W using \u0026#34;sin\u0026#34;, this ensures that W is always of the same values # and will be useful for debugging W = np.sin(np.arange(1, 1 + (1 + fan_in) * fan_out)) / 10.0 W = W.reshape(fan_out, 1 + fan_in, order=\u0026#39;F\u0026#39;) return W # Computes the gradient using \u0026#34;finite differences\u0026#34; nd gives us # a numerical estimate of the gradient. def computeNumericalGradient(costFunc, nn_params, e=1e-4): numgrad = np.zeros(nn_params.shape) perturb = np.diag(e * np.ones(nn_params.shape)) for i in range(nn_params.size): loss1, _ = costFunc(nn_params - perturb[:, i]) loss2, _ = costFunc(nn_params + perturb[:, i]) numgrad[i] = (loss2 - loss1) / (2 * e) return numgrad # Creates a small neural network to check the backpropagation gradients def checkNNGradients(nnCostFunction, reg_param=0, visually_examine=False): input_layer_size = 3 hidden_layer_size = 5 num_labels = 3 m = 5 # We generate some \u0026#39;random\u0026#39; test data Theta1 = debugInitWeights(input_layer_size, hidden_layer_size) Theta2 = debugInitWeights(hidden_layer_size, num_labels) # Reusing debugInitWeights to generate X X = debugInitWeights(input_layer_size - 1, m) y = np.arange(1, 1 + m) % num_labels # Unroll parameters nn_params = np.r_[Theta1.ravel(order=\u0026#39;F\u0026#39;), Theta2.ravel(order=\u0026#39;F\u0026#39;)] # Short hand for cost function costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, reg_param) cost, grad = costFunc(nn_params) numgrad = computeNumericalGradient(costFunc, nn_params) # Visually examine the two gradient computations. # The two columns you get should be very similar if (visually_examine): print(np.stack([numgrad, grad], axis=1)) print(\u0026#39; - The above two columns you get should be very similar.\u0026#39;) print(\u0026#39; - (Left: Your Numerical Gradient; Right: Analytical Gradient)\u0026#39;) # Evaluate the norm of the difference between two solutions. # If you have a correct implementation, and assuming that you used EPSILON = 1e-4 # in \u0026#39;computeNumericalGradient()\u0026#39;, then diff below should be less that 1e-9 diff = np.linalg.norm(numgrad - grad) / np.linalg.norm(numgrad + grad) print(\u0026#39; - If your backpropagation implementation is correct, \\n\u0026#39; \u0026#39; - then the relative difference will be small (less than 1e-9).\\n\u0026#39; \u0026#39; - Relative Difference:\u0026#39;, diff) # Predict the label of an input given a trained neural network def predict(theta1, theta2, X): a1 = np.c_[np.ones([X.shape[0], 1]), X] z2 = np.dot(a1, theta1.T) a2 = np.c_[np.ones([z2.shape[0], 1]), sigmoid(z2)] z3 = np.dot(a2, theta2.T) a3 = sigmoid(z3) pred = np.argmax(a3, axis=1) return pred + 1 # Visualize what the representations captured by the hidden units def plotHiddenUnits(X, figsize=(8, 8)): if X.ndim == 2: m, n = X.shape elif X.ndim == 1: m, n = 1, X.size X = X[None] # Promote to a 2-D array else: raise IndexError(\u0026#39;Input X should be 1 or 2 dimensional.\u0026#39;) size = int(np.sqrt(n)) rows = cols = int(np.sqrt(m)) fig1, ax1 = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=figsize) ax1 = [ax1] if m == 1 else ax1.ravel() for i, ax in enumerate(ax1): ax.imshow(X[i].reshape([size, size], order=\u0026#39;F\u0026#39;), cmap=\u0026#39;Greys\u0026#39;) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.subplots_adjust(wspace=0, hspace=0) plt.suptitle(\u0026#39;Visualization of Hidden Units\u0026#39;, fontsize=24) # Setup the parameters input_layer_size = 400 # 20x20 Input Images of Digits hidden_layer_size = 25 # 25 hidden units num_labels = 10 # 10 labels, form 1 to 10 (mapping \u0026#39;0\u0026#39; to label \u0026#39;10\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 1: Loading and Visualizing Data \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;1. Loading and Visualizing Data ... \\n\u0026#39;) # Load Training Data X, y, m = loadData(\u0026#39;ex4data1.mat\u0026#39;) # Randomly select 100 data points to display randomDisplay(X, 100, transpose=True) \u0026#39;\u0026#39;\u0026#39; Part 2: Loading Parameters \u0026#39;\u0026#39;\u0026#39; # Load the weights into variables Theta1 and Theta2 print(\u0026#39;2. Loading Saved Neural Network Parameters ...\\n\u0026#39;) weights = scio.loadmat(\u0026#39;ex4weights.mat\u0026#39;) Theta1, Theta2 = weights[\u0026#39;Theta1\u0026#39;], weights[\u0026#39;Theta2\u0026#39;] # Unroll parameters nn_params = np.r_[Theta1.ravel(order=\u0026#39;F\u0026#39;), Theta2.ravel(order=\u0026#39;F\u0026#39;)] \u0026#39;\u0026#39;\u0026#39; Part 3: Compute Cost (Feedforward) \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;3. Feedforward Using Neural Network ...\u0026#39;) # Set weight regularization parameter to 0 (i.e. no regularization) cost, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg_param=0) print(\u0026#39; - Cost at parameters (loaded form ex4weights.mat): \u0026#39;, cost) print(\u0026#39; - (This value should be about 0.287629.)\\n\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 4: Implement Regularization \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;4. Checking Cost Function (w/ Regularization) ...\u0026#39;) # Weight regularization parameter (we set this to 1 here) cost, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg_param=1) print(\u0026#39; - Cost at parameters (loaded form ex4weights.mat): \u0026#39;, cost) print(\u0026#39; - (This value should be about 0.383770.)\\n\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 5: Sigmoid Gradient \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;5. Evaluating sigmoid gradient ...\u0026#39;) g = sigmoidGradient(np.array([-1, -0.5, 0, 0.5, 1])) print(\u0026#39; - Sigmoid gradient evaluated at [-1, -0.5, 0, 0.5, 1]:\\n-\u0026#39;, g) \u0026#39;\u0026#39;\u0026#39; Part 6: Initializing Parameters \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;\\n6. Initializing Neural Network Parameters ...\u0026#39;) init_Theta1 = randInitWeights(input_layer_size, hidden_layer_size) init_Theta2 = randInitWeights(hidden_layer_size, num_labels) # Unroll parameters init_nn_params = np.r_[init_Theta1.ravel(order=\u0026#39;F\u0026#39;), init_Theta2.ravel(order=\u0026#39;F\u0026#39;)] \u0026#39;\u0026#39;\u0026#39; Part 7: Implement Backpropagation \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;\\n7. Checking Backpropagation ...\u0026#39;) # Check gradients by running checkNNGradients checkNNGradients(nnCostFunction, reg_param=0, visually_examine=True) \u0026#39;\u0026#39;\u0026#39; Part 8: Implement Regularization \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;\\n8. Checking Backpropagation (w/ Regularization) ...\u0026#39;) debug_lambda = 3.0 checkNNGradients(nnCostFunction, reg_param=debug_lambda, visually_examine=False) debug_cost, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, debug_lambda) print(\u0026#39; - Cost at (fixed) debugging parameters (w/ lambda = \u0026#39;, debug_lambda, \u0026#39;):\u0026#39;, debug_cost, \u0026#39;\\n- (for lambda = 3, this value should be about 0.576051)\\n\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 9: Training Neural Network \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;9. Training Neural Network ...\u0026#39;) # After you have completed the assignment, change the MaxIter # to a larger value to see how training helps options = {\u0026#39;maxiter\u0026#39;: 400} # You should also try different values of lambda train_lambda = 3.0 # Create \u0026#34;short hand\u0026#34; for the cost function to be minimized costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, train_lambda) # Now, \u0026#39;costFunc\u0026#39; is a function that takes in only one argument # (the neural network parameters) res = opt.minimize(costFunc, init_nn_params, jac=True, method=\u0026#39;TNC\u0026#39;, options=options) nn_params = res.x # Obtain Theta1 and Theta2 back from nn_params Theta1 = nn_params[:hidden_layer_size * (input_layer_size + 1)].\\ reshape([hidden_layer_size, input_layer_size + 1], order=\u0026#39;F\u0026#39;) Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].\\ reshape([num_labels, hidden_layer_size + 1], order=\u0026#39;F\u0026#39;) # Implement predict y_predict = predict(Theta1, Theta2, X) print(\u0026#39; - When lambda = {:}, MaxIter = {:}, Training set accuracy = {:.2f}%\\n\u0026#39;. format(train_lambda, options[\u0026#39;maxiter\u0026#39;], np.mean(y_predict == y.flatten()) * 100)) \u0026#39;\u0026#39;\u0026#39; Part 10: Visualize Weights \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;10. Visualizing Neural Network ...\\n\u0026#39;) plotHiddenUnits(Theta1[:, 1:]) plt.show() Outputs:\n Console  Randomly select 100 data points to display  Visualization of hidden units   ","permalink":"https://fang-lansheng.github.io/posts/2020-03-14-ml-ng-5/","summary":"Course Link ÔºöWeek 5 - Neural Networks: Learning\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 14 Cost Function and Backpropagation 14.1 Cost Function   Neural Network (Classification)\n Training Set: $\\lbrace \\left( x^{(1)}, y^{(1)} \\right), \\left( x^{(2)}, y^{(2)} \\right), \\dots, \\left( x^{(m)}, y^{(m)} \\right) \\rbrace$  $L$ = total number of layers in the network $s_l$ = number of units (not counting bias unit) in layer $l$ $K$ = number of output units/classes   Binary classification: $y = 0 \\text{ or } 1$ (1 output unit) Multi-class classification (K classes): $y \\in \\mathbb{R}^K$ (K output units)    Cost function","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà5Ôºâ"},{"content":" Course Link ÔºöWeek 4 - Neural Networks: Representation\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 11 Motivations 11.1 Non-linear Hypotheses Neural Networks is a much better way to learn complex non-linear hypotheses even when your input feature space $(n)$ is large.\n11.2 Neurons and the Brain  Origins: Algorithms that try to mimin the brain. Was very widely used in 80s and early 90s; popularity diminished in late 90s. Recent resurgence: State-of-the-art technique for many applications.  12 Neural Networks 12.1 Model Representation I   Neuron in the brain  Neurons are basically computational units that take inpus (dendrites) as electrical inputs (called \u0026ldquo;spikes\u0026rdquo;) that are channeled to outputs (axons).    Neuron model: Logistic unit   In our model, our dendrites are like the input features $x_1 \\cdots x_n$, and the output is the result of our hypothesis function.\n  In this model our $x_0$ input node is sometimes called the \u0026ldquo;bias unit\u0026rdquo;, it is always equal to 1.\n  In neural networks, we use the same logistic function as in classification, $\\frac{1}{1+e^{-\\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) activation function.\n  In this situation, our \u0026ldquo;$\\theta$\u0026rdquo; parametrs are sometimes called \u0026ldquo;weights\u0026rdquo;.\n  Visually, a simplistic representation looks like:\n$$ \\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\end{bmatrix} \\rightarrow \\begin{bmatrix}\\quad \\end{bmatrix} \\rightarrow h_\\theta(x) $$\n    Neural Network   Our input nodes (layer 1), also known as the \u0026ldquo;input layer\u0026rdquo;, go into another node (layer 2), which finally outputs the hypothesis function, known as the \u0026ldquo;output layer\u0026rdquo;.\n  We can have intermediate layers of nodes between the input and output layers called the \u0026ldquo;hidden layers\u0026rdquo;.\n  In this example, we label these intermediate or \u0026ldquo;hidden\u0026rdquo; layer nodes $a_0^2 \\cdots a_n^2$ and call them \u0026ldquo;activation units\u0026rdquo;.\n$$ \\begin{align} a_i^{(j)} = \u0026amp;\\text{\u0026ldquo;activation\u0026rdquo; of unit $i$ in layer $j$} \\newline \\Theta^{(j)} =\u0026amp; \\text{matrix of weights controlling function mapping from }\\newline \u0026amp;\\text{layer $j$ to layer $j+1$}\\end{align} $$\n  If we had one hidden layer, it would look like:\n$$\\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3 \\end{bmatrix} \\rightarrow \\begin{bmatrix}a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \\end{bmatrix} \\rightarrow h_\\theta(x)$$\n  The values for each of the \u0026ldquo;activation\u0026rdquo; nodes is obtained as follows:\n$$ \\begin{align*} a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\newline a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\newline a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\newline h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) \\newline \\end{align*} $$\n  This is saying that we compute our activation nodes by using a $3\\times4$ matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix $\\Theta^{(2)}$ containing the weights for our second layer of nodes.\n  Each layer gets its own matrix of weights, $\\Theta^{(j)}$.\n  The dimensions of these matrices of weights is determined as follows:\n$$ \\begin{align*} \u0026amp;\\text{If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$,} \\newline \u0026amp;\\text{then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$.} \\end{align*} $$\nThe $+1$ comes from the addition in $\\Theta^{(j)}$ of the \u0026ldquo;bias nodes\u0026rdquo;, $x_0$ and $\\Theta_0^{(j)}$. In other words, the output nodes will not include the bias nodes while the inputs will.\n    12.2 Model Representation II   Forward propation: Vectorized implementation   To re-iterate, the following is an example of a neural network:$$ \\begin{align*} a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\newline a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\newline a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\newline h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) \\newline \\end{align*} $$\n  In this section we\u0026rsquo;ll do a vectorized implementation of the above functions. We\u0026rsquo;re going to define a new variable $z_k^{(j)}$ that encompasses the parameters inside our $g$ function.\n  In our previous example, if we replaced by the variable $z$ for all the parameters we would get:\n$$ \\begin{align}a_1^{(2)} = g(z_1^{(2)}) \\newline a_2^{(2)} = g(z_2^{(2)}) \\newline a_3^{(2)} = g(z_3^{(2)}) \\newline \\end{align} $$\n    In other words, for layer $j=2$ and node $k$, the variable $z$ will be:\n$$ z_k^{(2)}=\\Theta_{k,0}^{(1)}x_0 + \\Theta_{k,1}^{(1)}x_1+\\cdots+\\Theta_{k,n}^{(1)}x_n $$\n  The vector represation of $x$ and $z^{(j)}$ is:\n$$ \\begin{align*}x = \\begin{bmatrix}x_0 \\newline x_1 \\newline\\cdots \\newline x_n\\end{bmatrix} \u0026amp;\\qquad z^{(j)} = \\begin{bmatrix}z_1^{(j)} \\newline z_2^{(j)} \\newline\\cdots \\newline z_n^{(j)}\\end{bmatrix}\\end{align*} $$\n  Setting $x=a^{(1)}$, we can rewrite the equation as: $z^{(j)}=\\Theta^{(j-1)}a^{(j-1)}$\n    We are multiplying our matrix $\\Theta^{(j-1)}$ with dimensions $s_j \\times (n+1)$ (where $s_j$ is the number of our activation nodes) by our vector $a^{(j-1)}$ with height $(n+1)$. This gives us our vector $z^{(j)}$ with height $s_j$.\n  Now we can get a vector of our activation nodes for layer $j$ as follows: $a^{(j)} = g(z^{(j)})$. Where our function $g$ can be applied element-wise to our vector $z^{(j)}$.\n  We can then add a bias unit (equal to 1) to layer $j$ after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to 1.\n  To compute our final hypothesis, let\u0026rsquo;s first compute another $z$ vector:\n  $$ z^{(j+1)} = \\Theta^{(j)}a^{(j)} $$\n  We get this final $z$ vector by multiplying the next theta matrix after $\\Theta^{(j-1)}$ with the values of all the activation nodes we just got.\n  This last theta matrix $\\Theta^{(j)}$ will have only one row which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with:\n  $$ h_\\Theta(x) = a^{(j+1)}=g\\left(z^{(j+1)}\\right) $$\n Notice that in this last step, between layer $j$ and layer $j+1$, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypothesis.    Neural Network learning its own features   Other network architectures   13 Applications 13.1 Examples and Intuitions I   Non-linear classification examples: XOR/XNOR   Simple example: AND   A simple example of applying neural networks is predicting $x_1 \\text{AND } x_2$ , which is the logical \u0026lsquo;and\u0026rsquo; operator and is only true if both $x_1$ and $x_2$ are 1.\n  The graph of our function will look like:\n$$ \\begin{align*}\\begin{bmatrix}x_0 \\newline x_1 \\newline x_2\\end{bmatrix} \\rightarrow\\begin{bmatrix}g(z^{(2)})\\end{bmatrix} \\rightarrow h_\\Theta(x)\\end{align*} $$\nRemember that $x_0$ is our bias variable and is always 1.\n  Let\u0026rsquo;s set our first theta matrix as: $\\Theta^{(1)} = \\begin{bmatrix} -30 \u0026amp; 20 \u0026amp; 20 \\end{bmatrix}$.\n  This will cause the output of our hypothesis to only be positive if both $x_1$ and $x_2$ are 1. In other words:\n$$ \\begin{align*}\u0026amp; h_\\Theta(x) = g(-30 + 20x_1 + 20x_2) \\newline \\newline \u0026amp; x_1 = 0 \\ \\ and \\ \\ x_2 = 0 \\ \\ then \\ \\ g(-30) \\approx 0 \\newline \u0026amp; x_1 = 0 \\ \\ and \\ \\ x_2 = 1 \\ \\ then \\ \\ g(-10) \\approx 0 \\newline \u0026amp; x_1 = 1 \\ \\ and \\ \\ x_2 = 0 \\ \\ then \\ \\ g(-10) \\approx 0 \\newline \u0026amp; x_1 = 1 \\ \\ and \\ \\ x_2 = 1 \\ \\ then \\ \\ g(10) \\approx 1\\end{align*} $$\n    So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate.\n  Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator \u0026lsquo;OR\u0026rsquo;, meaning either $x_1$ is true or $x_2$ is ture, or both. Where $g(z)$ is the following:  13.2 Examples and Intuitions II   Negation ($\\text{NOT } x_1$):   Putting it together: $x_1 \\text{ XNOR } x_2$\n  The $\\Theta^{(1)}$ matrices for AND, NOR, and OR are:\n$$ \\begin{align*}\\text{AND}:\\newline\\Theta^{(1)} \u0026amp;=\\begin{bmatrix}-30 \u0026amp; 20 \u0026amp; 20\\end{bmatrix} \\newline \\text{NOR}:\\newline\\Theta^{(1)} \u0026amp;= \\begin{bmatrix}10 \u0026amp; -20 \u0026amp; -20\\end{bmatrix} \\newline \\text{OR}:\\newline\\Theta^{(1)} \u0026amp;= \\begin{bmatrix}-10 \u0026amp; 20 \u0026amp; 20\\end{bmatrix} \\newline\\end{align*} $$\n  We can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1).\n$$ \\begin{align*}\\begin{bmatrix}x_0 \\newline x_1 \\newline x_2\\end{bmatrix} \\rightarrow\\begin{bmatrix}a_1^{(2)} \\newline a_2^{(2)} \\end{bmatrix} \\rightarrow\\begin{bmatrix}a^{(3)}\\end{bmatrix} \\rightarrow h_\\Theta(x)\\end{align*} $$\n  For the transition between the first and second layer, we\u0026rsquo;ll use a $\\Theta^{(1)}$ matrix that combines the values for AND and NOR:\n$$ \\Theta^{(1)} =\\begin{bmatrix}-30 \u0026amp; 20 \u0026amp; 20 \\newline 10 \u0026amp; -20 \u0026amp; -20\\end{bmatrix} $$\n  For the transition between the second and third layer, we\u0026rsquo;ll use a $\\Theta^{(2)}$ matrix that uses the value for OR:\n$$ \\Theta^{(2)} =\\begin{bmatrix}-10 \u0026amp; 20 \u0026amp; 20\\end{bmatrix} $$\n  Let\u0026rsquo;s write out the values for all our nodes:\n$$ \\begin{align*}\u0026amp; a^{(2)} = g(\\Theta^{(1)} \\cdot x) \\newline\u0026amp; a^{(3)} = g(\\Theta^{(2)} \\cdot a^{(2)}) \\newline\u0026amp; h_\\Theta(x) = a^{(3)}\\end{align*} $$\n  And there we have the XNOR operator using a hidden layer with two nodes.\n    Neural Network intuition   Examples:   13.3 Multiclass Classification   Multiple output units: One-vs-all\n To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly.    We can define our set of resulting classes as $y$:\n$$ y^{(i)} = \\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\end{bmatrix} $$\n    Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provides us with some new information which leads to our final hypothesis function.\n  The setup looks like:\n$$ \\begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ \\cdots \\ x_n \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(2)} \\ a_1^{(2)} \\ a_2^{(2)} \\ \\cdots \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(3)} \\ a_1^{(3)} \\ a_2^{(3)} \\ \\cdots\\end{bmatrix} \\rightarrow \\cdots \\rightarrow \\begin{bmatrix} h_\\Theta(x)1 \\ h\\Theta(x)2 \\ h\\Theta(x)3 \\ h\\Theta(x)_4 \\end{bmatrix} $$\n  Our resulting hypothesis for one set of inputs may look like:\n$$ h_\\Theta(x) =\\begin{bmatrix}0 \\newline 0 \\newline 1 \\newline 0 \\newline\\end{bmatrix} $$\nIn which case our resulting class is the third one down, or $h_\\Theta(x)_3$, which represents the motorcycle.\n    Ex3: Multi-class Classification and Neural Networksüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex3/ex3.ipynb\n Ex3.1 Multi-class Classification Instruction: For this exercise, you will use logistic regression and neural networks to recognize handwritten digits (from 0 to 9). Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. This exercise will show you how the methods you‚Äôve learned can be used for this classification task.\nCode:\nimport numpy as np import matplotlib.pyplot as plt import scipy.io as scio import scipy.optimize as opt \u0026#39;\u0026#39;\u0026#39; Part 0: Functions and Parameters \u0026#39;\u0026#39;\u0026#39; # Load training data def loadData(path): data = scio.loadmat(path) X, y = data[\u0026#39;X\u0026#39;], data[\u0026#39;y\u0026#39;] m = len(y) return X, y, m # Randomly select several data points to display def randomDisplay(data, num=100, cmap=\u0026#39;binary\u0026#39;, transpose=True): sample = data[np.random.choice(len(data), num)] size, size1 = int(np.sqrt(num)), int(np.sqrt(sample[0].shape[0])) fig0, ax0 = plt.subplots(nrows=size, ncols=size, sharex=True, sharey=True, figsize=(8, 8)) order = \u0026#39;F\u0026#39; if transpose else \u0026#39;C\u0026#39; for i in range(size): for j in range(size): ax0[i, j].imshow(sample[size * i + j].reshape([size1, size1], order=order), cmap=cmap) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.subplots_adjust(wspace=0, hspace=0) plt.suptitle(str(num)+\u0026#39; examples from the dataset\u0026#39;, fontsize=24) # Sigmoid function def sigmoid(z): return 1 / (1 + np.exp(-z)) # Compute cost and gradient for logistic regression with regularization def lrCostFunction(theta, X, y, lam): m = len(y) cost1 = - np.sum(y * np.log(sigmoid(np.dot(X, theta))) + (1 - y) * np.log(1 - sigmoid(np.dot(X, theta)))) / m cost2 = 0.5 * lam * np.dot(theta[1:].T, theta[1:]) / m cost = cost1 + cost2 grad = np.dot(X.T, sigmoid(np.dot(X, theta)) - y) / m grad[1:] += (lam * theta / m)[1:] return cost, grad def costFunc(param, *args): X, y, lam = args m, n = X.shape theta = param.reshape([n, 1]) cost1 = - np.sum(y * np.log(sigmoid(np.dot(X, theta))) + (1 - y) * np.log(1 - sigmoid(np.dot(X, theta)))) / m cost2 = 0.5 * lam * np.dot(theta[1:].T, theta[1:]) / m return cost1 + cost2 def gradFunc(param, *args): X, y, lam = args m, n = X.shape theta = param.reshape(-1, 1) grad = np.dot(X.T, sigmoid(np.dot(X, theta)) - y) / m grad[1:] += (lam * theta / m)[1:] return grad.ravel() # oneVsAll() trains multiple logistic regression classifiers and returns all # the classifiers in a matrix all_theta, where the i-th row of all_theta # corresponds to the classifier for label i def oneVsAll(X, y, num_labels, lam): m, n = X.shape all_theta = np.zeros([num_labels, n + 1]) X = np.c_[np.ones([m, 1]), X] for i in range(1, num_labels + 1): params = np.zeros(n + 1) args = (X, y == i, lam) res = opt.minimize(fun=costFunc, x0=params, args=args, method=\u0026#39;TNC\u0026#39;, jac=gradFunc) all_theta[i - 1, :] = res.x return all_theta # Predict the label for a trained one-vs-all classifier def predictOneVsAll(X, all_theta): m, n = X.shape X = np.c_[np.ones([m, 1]), X] h_argmax = np.argmax(sigmoid(np.dot(X, all_theta.T)), axis=1) return h_argmax + 1 # Setup the parameters input_layer_size = 400 # 20x20 Input Images of Digits num_labels = 10 # 10 labels, form 1 to 10 (note that we have mapped \u0026#39;0\u0026#39; to label \u0026#39;10\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 1: Loading and Visualizing Data \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;Loading and Visualizing Data ... \u0026#39;) # Load Training Data X, y, m = loadData(\u0026#39;ex3data1.mat\u0026#39;) # Randomly select 100 data points to display randomDisplay(X, 100) plt.show() \u0026#39;\u0026#39;\u0026#39; Part 2.1: Vectorize Logistic Regression \u0026#39;\u0026#39;\u0026#39; # Test case for lrCostFunction print(\u0026#39;\\nTesting lrCostFunction() with regularization\u0026#39;) theta_test = np.array(([-2], [-1], [1], [2])) X_test = np.c_[np.ones([5, 1]), np.linspace(1, 15, 15).reshape([5, 3], order=\u0026#39;F\u0026#39;) / 10] y_test = np.array(([1], [0], [1], [0], [1])) lam_test = 3 cost, grad = lrCostFunction(theta_test, X_test, y_test, lam_test) print(\u0026#39;Cost: \u0026#39;, cost.flatten()) print(\u0026#39;Expected cost: 2.534819\u0026#39;) print(\u0026#39;Gradients: \u0026#39;, grad.flatten()) print(\u0026#39;Expected gradients: 0.146561\\t-0.548558\\t0.724722\\t1.398003\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 2.2: One-vs-All Training \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;\\nTraining One-vs-All Logistic Regression...\u0026#39;) theta = oneVsAll(X, y, num_labels=num_labels, lam=1.0) \u0026#39;\u0026#39;\u0026#39; Part 3: Predict for One-Vs-All \u0026#39;\u0026#39;\u0026#39; y_predict = predictOneVsAll(X, all_theta=theta) print(\u0026#39;Training Set Accuracy: \u0026#39;, np.mean(y.ravel() == y_predict) * 100, \u0026#39;%\u0026#39;) Output:\n Console:  Randomly select several data points to display:   Ex3.2 Neural Networks Instruction:\nIn this part of the exercise, you will implement a neural network to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses. Your goal is to implement the feedforward propagation algorithm to use our weights for prediction.\nOur neural network is shown below. It has 3 layers ‚Äì an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size $20√ó20$, this gives us 400 input layer units (excluding the extra bias unit which always outputs +1). As before, the training data will be loaded into the variables $X$ and $y$. Code:\nimport numpy as np import matplotlib.pyplot as plt import scipy.io as scio \u0026#39;\u0026#39;\u0026#39; Part 0: Functions and Parameters \u0026#39;\u0026#39;\u0026#39; # Load training data def loadData(path): data = scio.loadmat(path) X, y = data[\u0026#39;X\u0026#39;], data[\u0026#39;y\u0026#39;] m = len(y) return X, y, m # Randomly select several data points to display def randomDisplay(data, num=100, cmap=\u0026#39;binary\u0026#39;, transpose=True): sample = data[np.random.choice(len(data), num)] size, size1 = int(np.sqrt(num)), int(np.sqrt(sample[0].shape[0])) fig0, ax0 = plt.subplots(nrows=size, ncols=size, sharex=True, sharey=True, figsize=(8, 8)) order = \u0026#39;F\u0026#39; if transpose else \u0026#39;C\u0026#39; for i in range(size): for j in range(size): ax0[i, j].imshow(sample[size * i + j].reshape([size1, size1], order=order), cmap=cmap) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.subplots_adjust(wspace=0, hspace=0) plt.suptitle(str(num)+\u0026#39; examples from the dataset\u0026#39;, fontsize=24) # Sigmoid function def sigmoid(z): return 1 / (1 + np.exp(-z)) # Predict the label of an input given a trained neural network def predict(theta1, theta2, X): a1 = np.c_[np.ones([X.shape[0], 1]), X] z2 = np.dot(a1, theta1.T) a2 = np.c_[np.ones([z2.shape[0], 1]), sigmoid(z2)] z3 = np.dot(a2, theta2.T) a3 = sigmoid(z3) pred = np.argmax(a3, axis=1) return pred + 1 # Setup the parameters input_layer_size = 400 # 20x20 Input Images of Digits hidden_layer_size = 25 # 25 hidden units num_labels = 10 # 10 labels, form 1 to 10 (note that we have mapped \u0026#39;0\u0026#39; to label \u0026#39;10\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 1: Loading and Visualizing Data \u0026#39;\u0026#39;\u0026#39; print(\u0026#39;Loading and Visualizing Data ... \u0026#39;) # Load Training Data X, y, m = loadData(\u0026#39;ex3data1.mat\u0026#39;) # Randomly select 100 data points to display randomDisplay(X, 100, transpose=True) plt.show() \u0026#39;\u0026#39;\u0026#39; Part 2: Loading Parameters \u0026#39;\u0026#39;\u0026#39; weights = scio.loadmat(\u0026#39;ex3weights.mat\u0026#39;) theta1, theta2 = weights[\u0026#39;Theta1\u0026#39;], weights[\u0026#39;Theta2\u0026#39;] \u0026#39;\u0026#39;\u0026#39; Part 3: Implement Predict \u0026#39;\u0026#39;\u0026#39; y_predict = predict(theta1, theta2, X) print(\u0026#39;\\nTraining Set Accuracy: \u0026#39;, np.mean(y_predict == y.flatten()) * 100, \u0026#39;%\u0026#39;) # Randomly select examples random_index = np.random.choice(m, size=10) print(\u0026#39;Indexes of examples :\u0026#39;, random_index) for i, idx in enumerate(random_index): # print(\u0026#39;Example[%d]\\t is number %d,\\t we predict it as %d\u0026#39; % (i + 1, y[idx], y_predict[idx ])) print(\u0026#39;Example[{:0\u0026gt;2}] is number {:^2}, we predict it as {:^2}\u0026#39;. format(str(i + 1), str(int(y[idx])), str(y_predict[idx]))) Output:\n Console:   ","permalink":"https://fang-lansheng.github.io/posts/2020-03-10-ml-ng-4/","summary":"Course Link ÔºöWeek 4 - Neural Networks: Representation\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 11 Motivations 11.1 Non-linear Hypotheses Neural Networks is a much better way to learn complex non-linear hypotheses even when your input feature space $(n)$ is large.\n11.2 Neurons and the Brain  Origins: Algorithms that try to mimin the brain. Was very widely used in 80s and early 90s; popularity diminished in late 90s. Recent resurgence: State-of-the-art technique for many applications.","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà4Ôºâ"},{"content":" Course Link ÔºöWeek 3 - Logistic Regression\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n  For now, we will focus on the binary classification problem.\n  $y \\in {0,1}$ - the variable that we\u0026rsquo;re trying to predict\n $0: \\text{\u0026ldquo;Negative Class\u0026rdquo;}$ (e.g., benign tumor) $1: \\text{\u0026ldquo;Positive Class\u0026rdquo;}$ (e.g., malignant tumor)    Applying linear regression to a classification problem often isn\u0026rsquo;t a great idea.\n  Logistic Regression: $0\\leq h_\\theta(x) \\leq 1$ (BTW, this is actually a classification algorithm)\n  7.2 Hypothesis Representation  Logistic Regression Model: want $0 \\leq h_\\theta(x) \\leq 1$ $h_\\theta(x) = g(\\theta^Tx)$  Sigmoid function (or logistic function): $g(z) = \\frac{1}{1 + e^{-z}}$. The following image shows us what the sigmoid function looks like:  $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$   Interpretation of hypothesis output:  $h_\\theta(x) = \\text{estimated probability that } y=1 \\text{ on input }x$ $h_\\theta(x)= P(y=1 \\vert x;\\theta)$ : probability that $y = 1$, given $x$, parameterized by $\\theta$ $P(y=0 \\vert x;\\theta) + P(y=1 \\vert x;\\theta) = 1 \\\\ P(y=0 \\vert x;\\theta) = 1 - P(y=1 \\vert x;\\theta)$    7.3 Decision Boundary  Logistic regression: $h_\\theta(x) = g(\\theta^Tx), \\quad g(z)=\\frac{1}{1+e^{-z}}$  Suppose predict $y=\\begin{cases} 0, \u0026amp; \\text {if $h_\\theta(x) \\geq$ 0.5} \\newline 1, \u0026amp; \\text{if $h_\\theta(x) \u0026lt; $ 0.5} \\end{cases}$   Decision boundary: the decision boundary is the line that separates the area where $y=0$ and where $y=1$.  It is created by our hypothesis function The input to the sigmoid function of $g(z)$ doesn\u0026rsquo;t need to be linear, and could be a function that describes a circle or any shape to fit the data    8 Logistic Regression Model 8.1 Cost Function  Logistic regression cost function:  $$ J(\\theta) = \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}(h_\\theta(x^{(i)}, y^{(i)} ) $$\n $\\text{Cost}(h_\\theta(x), y)=\\begin{cases} -\\log(h_\\theta(x)), \u0026amp; \\text {if } y =1 \\newline -\\log(1-h_\\theta(x)), \u0026amp; \\text{if } y=0 \\end{cases}$  $\\text{Cost}(h_\\theta(x), y) = 0, \\text{ if } h_\\theta(x) = y$ $\\text{Cost}(h_\\theta(x), y) \\rightarrow \\infty, \\text{ if $y=0$ and $h_\\theta(x) \\rightarrow 1$} $ $\\text{Cost}(h_\\theta(x), y) \\rightarrow \\infty, \\text{ if $y=1$ and $h_\\theta(x) \\rightarrow 0$} $   When $y=1$, we get the following plot for $J(\\theta) \\text{ vs } h_\\theta(x)$  Similarly, when $y=0$, we get the following plot for $J(\\theta) \\text{ vs } h_\\theta(x)$   8.2 Simplified Cost Function and Gradient Descent   Logistic regression cost function\n $J(\\theta) = \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}(h_\\theta(x^{(i)}, y^{(i)} )$ $\\text{Cost}(h_\\theta(x), y)=\\begin{cases} -\\log(h_\\theta(x)), \u0026amp; \\text {if } y =1 \\newline -\\log(1-h_\\theta(x)), \u0026amp; \\text{if } y=0 \\end{cases}$ $\\text{Note: $y=0$ or $ 1$ always}$    Then, $\\text{Cost}(h_\\theta(x), y)=-y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))$\n  $$ \\begin{align} J(\\theta) \u0026amp;= \\frac{1}{m}\\sum_{i =1}^{m}\\text{Cost}\\left(h_\\theta(x^{(i)}, y^{(i)}\\right) \\ \u0026amp;= -\\frac{1}{m}\\sum_{i =1}^{m}\\left[ y^{(i)}\\log h_\\theta(x^{(i)})+(1-y^{(i)})\\log \\left(1-h_\\theta(x^{(i)})\\right) \\right] \\end{align} $$\n  A vectorized implementation is:\n $h = g(X\\theta)$ $J(\\theta) = \\frac{1}{m}\\cdot \\left(-y^T\\log(h)-(1-y)^T\\log(1-h)\\right)$    To fit parameters $\\theta : \\mathop{\\text{min}}\\limits_\\theta J(\\theta)$\n  To make a prediction given new $x$ : output $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n  Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\newline \\qquad (\\text{simultaneously update all $\\theta_j$) } \\newline } \\end{array} \\right. $$\n(Algorithm looks identical to linear regression!)\n  A vectorized implementation is: $\\theta := \\theta - \\frac{\\alpha}{m}X^T\\left( g(X\\theta)- \\vec{y} \\right)$\n  8.3 Advanced Optimization   Cost function $J(\\theta)$. Want $\\mathop{\\text{min}}\\limits_\\theta J(\\theta)$.\n  Given $\\theta$, we have code that can compute\n $J(\\theta)$ $\\frac{\\partial}{\\partial \\theta_j}J(\\theta), \\ \\text{(for $j = 0, 1, \\dots,n$)} $    Optimization algorithms:\n (In this class) Gradient descent Conjugate gradient, BFGS, L-BFGS  Advantages:  No need to manually pick $\\alpha$ Often faster that gradient descent   Disadvantages:  More sophisticated        9 Multiclass classification 9.1 Multiclass Classification: One-vs-all   $y = \\lbrace 0, 1, \\dots, n \\rbrace$\n$$ h_\\theta^{(0)}(x) = P(y=0 \\vert x;\\theta) \\ h_\\theta^{(1)}(x) = P(y=1 \\vert x;\\theta) \\ \\cdots \\ h_\\theta^{(n)}(x) = P(y=n \\vert x;\\theta) \\ \\text{prediction} = \\mathop{\\text{max}}\\limits_i h_\\theta^{(i)}(x) $$\n  The following image shows how one could classify 3 classes:   One-vs-all (one-vs-rest):\n $h_\\theta^{(i)} = P(y=i \\vert x;\\theta)\\quad (i=1,2,\\dots,n)$ Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y = i$. On a new input $x$, to make a prediction, pick the class i that maximizes $h_\\theta(x)$ : $\\mathop{\\text{max}}\\limits_i h_\\theta^{(i)}(x)$    10 Solving the Problem of Overfitting 10.1 The Problem of Overfitting  Overfitting: If we have too many features, the learned hypothesis may fit the training set well ($J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left( h_\\theta(x^{(i)}-y^{(i)}) \\right)^2\\approx 0$), but fail to generalize to new new examples (predict prices on new examples).  Options of addressing overfitting  Reduce number of features  Manually select which features to keep Model selection algorithm   Regularization  Keep all the features, but reduce magnitude/values of parameters $\\theta_j$ Regularization works well when we have a lot of slightly useful features, each of which contributes a bit to predicting $y$      10.2 Cost Function   Regularization: small values for parameters $\\theta_0, \\theta_1, \\dots, \\theta_n$\n \u0026ldquo;Simpler\u0026rdquo; hypothesis Less prone to overfitting    Cost function after regularization:\n$$ J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m\\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda\\sum_{j=1}^n \\theta_j^2 \\right] $$\n $\\lambda: \\text{regularization parameter}$    10.3 Regularized Linear Regression   Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_0^{(i)} \\newline \\qquad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\left[ \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)} + \\lambda\\theta_j \\right] \\quad j\\in\\lbrace 1,2,\\dots,n \\rbrace \\newline } \\end{array} \\right. $$\n When $j=1, 2, \\dots, n$, you can also write $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}$ $1-\\alpha\\frac{\\lambda}{m} \u0026lt; 1$    Normal equaiton\n$$ X=\\begin{bmatrix} (x^{(1)})^T \\newline \\vdots \\newline (x^{(m)})^T \\end{bmatrix}{m\\times(n+1)} \\quad y=\\begin{bmatrix} y^{(1)} \\newline \\vdots \\newline x^{(m)} \\end{bmatrix}\\in\\mathbb{R^{m}} \\\\ \\text{To } \\mathop{\\text{min}}\\limits\\theta J(\\theta), \\ \\theta=\\left(X^TX+\\lambda \\begin{bmatrix} 0 \u0026amp; \u0026amp; \u0026amp; \\ \u0026amp; 1 \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \\ \u0026amp; \u0026amp; \u0026amp; 1 \\end{bmatrix} \\right)^{-1}X^Ty $$\n  Non-invertibility\n Suppose $\\mathop{m}\\limits_{\\text{#examples}} \\leq \\mathop{n}\\limits_{\\text{#features}} $, then $X^TX$ will be non-invertible/singluar. $\\text{If } \\lambda \u0026gt; 0, \\ \\theta=\\left(\\underbrace{X^TX+\\lambda \\begin{bmatrix} 0 \u0026amp; \\newline \u0026amp; I_n \\end{bmatrix} }_{\\text{invertible}} \\right)^{-1}X^Ty$    10.4 Regularized Logistic Regression   We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pick line, is less likely to overfit than the non-regularized function represented by the blue line:   Regularized cost function:\n$$ J(\\theta)= -\\frac{1}{m}\\sum_{i =1}^{m}\\left[ y^{(i)}\\log h_\\theta(x^{(i)})+(1-y^{(i)})\\log \\left(1-h_\\theta(x^{(i)})\\right) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2 $$\n $\\sum_{j=1}^{n}\\theta_j^2$ means to explicitly exclude the bias term, $\\theta_0$.    Gradient descent:\n$$ \\left. \\begin{array}{l} \\text{repeat } {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_0^{(i)} \\newline \\qquad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\left[ \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)} + \\lambda\\theta_j \\right] \\quad j\\in\\lbrace 1,2,\\dots,n \\rbrace \\newline } \\end{array} \\right. $$\n  Ex2: Logistic Regressionüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex2/ex2.ipynb\n Ex2.1 Logistic Regression Instruction: In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\nCode:\nimport numpy as np import matplotlib.pyplot as plt import scipy.optimize as op \u0026#39;\u0026#39;\u0026#39; Part 0: Loading Data and Defining Functions \u0026#39;\u0026#39;\u0026#39; # Hypothesis function def h(X, theta): return 1 / (1 + np.exp(-np.dot(X, theta))) # Logistic regression cost function def costFunction(theta, X, y): m = len(y) cost = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m grad = np.dot(X.T, h(X, theta) - y) / m return cost, grad # The objective function to be minimized def costFunc(params, *args): X, y = args [m, n] = X.shape theta = params.reshape([n, 1]) cost = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m return cost # Method for computing the gradient vector def gradFunc(params, *args): X, y = args [m, n] = X.shape theta = params.reshape([n, 1]) grad = np.dot(X.T, h(X, theta) - y) / m return grad.flatten() # The first two columns contains the exam scores and the third column contains the label print(\u0026#39;Loading Data ... \u0026#39;) data = np.loadtxt(\u0026#39;ex2data1.txt\u0026#39;, dtype=float, delimiter=\u0026#39;,\u0026#39;) X, y = data[:, 0:2], data[:, 2:3] \u0026#39;\u0026#39;\u0026#39; Part 1: Plotting \u0026#39;\u0026#39;\u0026#39; fig0, ax0 = plt.subplots() label1, label0 = np.where(y.ravel() == 1), np.where(y.ravel() == 0) ax0.scatter(X[label1, 0], X[label1, 1], marker=\u0026#39;+\u0026#39;, color=\u0026#39;g\u0026#39;, label=\u0026#39;Admitted\u0026#39;) ax0.scatter(X[label0, 0], X[label0, 1], marker=\u0026#39;x\u0026#39;, color=\u0026#39;r\u0026#39;, label=\u0026#39;Not admitted\u0026#39;) ax0.legend(loc=\u0026#39;upper right\u0026#39;) ax0.set_xlabel(\u0026#39;Exam 1 Score\u0026#39;) ax0.set_ylabel(\u0026#39;Exam 2 Score\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 2: Compute Cost and Gradient \u0026#39;\u0026#39;\u0026#39; [m, n] = X.shape X = np.c_[np.ones([m, 1]), X] # Add intercept term to x and X_test initial_theta = np.zeros([n + 1, 1]) # Initialize fitting parameters # Compute and display initial cost and gradient cost, grad = costFunction(initial_theta, X, y) print(\u0026#39;\\nCost at initial theta : \u0026#39;, cost.ravel()) print(\u0026#39;Expected cost (approx): 0.693\u0026#39;) print(\u0026#39;Gradient at initial theta :\u0026#39;, grad.ravel()) print(\u0026#39;Expected gradients (approx): \\t-0.1000\\t-12.0092\\t-11.2628\u0026#39;) # Compute and display cost and gradient with non-zero theta test_theta = np.array(([-24], [0.2], [0.2])) cost, grad = costFunction(test_theta, X, y) print(\u0026#39;\\nCost at test theta : \u0026#39;, cost.ravel()) print(\u0026#39;Expected cost (approx): 0.218\u0026#39;) print(\u0026#39;Gradient at test theta :\u0026#39;, grad.ravel()) print(\u0026#39;Expected gradients (approx): \\t0.043\\t2.5662\\t2.647\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 3: Optimizing using fminunc \u0026#39;\u0026#39;\u0026#39; params = np.zeros([n + 1, 1]) args = (X, y) # uUse Newton Conjugate Gradient algorithm to obtain the optimal theta res = op.minimize(fun=costFunc, x0=params, args=args, method=\u0026#39;TNC\u0026#39;, jac=gradFunc) cost, theta = res.fun, res.x print(\u0026#39;\\nCost at theta found by fminunc: \u0026#39;, cost) print(\u0026#39;Expected cost (approx): 0.203\u0026#39;) print(\u0026#39;theta: \u0026#39;, theta) print(\u0026#39;Expected theta (approx): \\t-25.161\\t0.206\\t0.201\u0026#39;) # Plot boundary x1 = np.arange(min(X[:, 1]), max(X[:, 1]), 1) x2 = (-theta[0] - theta[1] * x1) / theta[2] plt.plot(x1, x2, color=\u0026#39;blue\u0026#39;) plt.show() \u0026#39;\u0026#39;\u0026#39; Part 4: Predict and Accuracies \u0026#39;\u0026#39;\u0026#39; prob = h(np.array(([1, 45, 85])), theta) print(\u0026#39;\\nFor a student with scores 45 and 85, we predict an adimission probability of \u0026#39;, prob) print(\u0026#39;Expected value: 0.775 +/- 0.002\u0026#39;) p = np.where(h(X, theta) \u0026gt; 0.5, 1.0, 0.0) print(\u0026#39;Train accuracy: \u0026#39;, np.mean(p == y.flatten()) * 100, \u0026#39;%\u0026#39;) print(\u0026#39;Expected accuracy (approx): 89.0 %\\n\u0026#39;) Output:\n Console  Training data with decision boundary   Ex2.2 Regularized Logistic Regression Instruction: In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nCode:\nimport numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures \u0026#39;\u0026#39;\u0026#39; Part 0: Loading Data and Defining Functions \u0026#39;\u0026#39;\u0026#39; # Feature mapping function to polynomial features def mapFeature(X1, X2): degree = 6 X = np.ones([len(X1), 1]) for i in np.arange(1, degree + 1, 1): for j in range(i + 1): X = np.c_[X, X1**(i-j) * X2**(j)] return X # Hypothesis function def h(X, theta): return 1 / (1 + np.exp(-np.dot(X, theta))) # Compute cost and gradient for logistic regression with regularization def costFunctionReg(theta, X, y, reg_param): m = len(y) cost1 = - np.sum(y * np.log(h(X, theta)) + (1 - y) * np.log(1 - h(X, theta))) / m cost2 = 0.5 * reg_param * np.dot(theta[1:].T, theta[1:]) / m # Don\u0026#39;t penalize theta_0 cost = cost1 + cost2 grad = np.dot(X.T, h(X, theta) - y) / m grad[1:] += (reg_param * theta / m)[1:] return cost, grad # Use Batch Gradient Descent algorithm to minimize cost def batchGradientDescent(X, y, theta, alpha=0.1, iters = 2000, reg=1): J_history = np.zeros(iters) for i in range(iters): cost, grad = costFunctionReg(theta, X, y, reg) theta = theta - alpha * grad J_history[i] = cost return theta, J_history # The first two columns contains the exam scores and the third column contains the label print(\u0026#39;Loading Data ... \u0026#39;) data = np.loadtxt(\u0026#39;ex2data2.txt\u0026#39;, dtype=float, delimiter=\u0026#39;,\u0026#39;) X, y = data[:, 0:2], data[:, 2:3] # Plot data fig0, ax0 = plt.subplots() label1, label0 = np.where(y.ravel() == 1), np.where(y.ravel() == 0) ax0.scatter(X[label1, 0], X[label1, 1], marker=\u0026#39;+\u0026#39;, color=\u0026#39;k\u0026#39;, label=\u0026#39;y = 1\u0026#39;) ax0.scatter(X[label0, 0], X[label0, 1], marker=\u0026#39;x\u0026#39;, color=\u0026#39;y\u0026#39;, label=\u0026#39;y = 0\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 1: Regularized Logistic Regression \u0026#39;\u0026#39;\u0026#39; m = len(y) X = mapFeature(X[:, 0], X[:, 1]) # Initialize fitting parameters initial_theta = np.zeros([X.shape[1], 1]) # Set regularization parameter lambda to 1 reg_param = 1 # Compute and display inital cost gradient for regularized logistic regression cost0, grad0 = costFunctionReg(initial_theta, X, y, reg_param) print(\u0026#39;\\nCost at initial theta (zeros): \u0026#39;, cost0.flatten()) print(\u0026#39;Expected cost (approx): 0.693\u0026#39;) print(\u0026#39;Gradient at initial theta (zeros) - first five values only: \u0026#39; , np.around(grad0[0:5], 4).flatten()) print(\u0026#39;Expected gradients (approx) - first five values only: \u0026#39;, \u0026#39;0.0085 0.0188 0.0001 0.0503, 0.0115\u0026#39;) # Compute and display cost and gradient with all-ones theta and lambda = 10 test_theta = np.ones([X.shape[1], 1]) cost1, grad1 = costFunctionReg(test_theta, X, y, reg_param=10) print(\u0026#39;\\nCost at test theta (with lambda = 10): \u0026#39;, cost1.flatten()) print(\u0026#39;Expected cost (approx): 3.16\u0026#39;) print(\u0026#39;Gradient at test theta - first five values only: \u0026#39; , np.around(grad1[0:5], 4).flatten()) print(\u0026#39;Expected gradients (approx) - first five values only: \u0026#39;, \u0026#39;0.3460 0.1614 0.1948 0.2269, 0.0922\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Part 2: Regularization and Accuracies \u0026#39;\u0026#39;\u0026#39; # Optimize theta, J_history = batchGradientDescent(X, y, initial_theta, reg=reg_param) # fig1, ax1 = plt.subplots() # ax1.plot(np.arange(2000), J_history, \u0026#39;c\u0026#39;) # Plot boundary poly = PolynomialFeatures(6) x1min, x1max, x2min, x2max = X[:, 1].min(), X[:, 1].max(), X[:, 2].min(), X[:, 2].max() xx1, xx2 = np.meshgrid(np.linspace(x1min, x1max), np.linspace(x2min, x2max)) bd = 1 / (1 + np.exp(-poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(theta))) bd = bd.reshape(xx2.shape) CS = ax0.contour(xx1, xx2, bd, [0.5], colors=\u0026#39;c\u0026#39;) CS.collections[0].set_label(\u0026#39;Decision\\nBoundary\u0026#39;) ax0.set_title(r\u0026#39;$\\lambda$ = \u0026#39;+str(reg_param)) ax0.legend(loc=\u0026#39;upper right\u0026#39;) ax0.set_xlabel(\u0026#39;Microchip Test 1\u0026#39;) ax0.set_ylabel(\u0026#39;Microchip Test 2\u0026#39;) plt.show() # Compute accuracy on our training set p = np.where(h(X, theta) \u0026gt;= 0.5, 1.0, 0.0) print(\u0026#39;\\nTrain Accuracy: \u0026#39;, np.mean(p == y) * 100, \u0026#39;%\u0026#39;) print(\u0026#39;Expected accuracy (with lambda = 1): 83.1 % (approx)\u0026#39;) Output:\n Console ((Œª = 1)  Training data with decision boundary (Œª = 1)  Too much regularization (Underfitting) (Œª = 100)   ","permalink":"https://fang-lansheng.github.io/posts/2020-03-04-ml-ng-3/","summary":"Course Link ÔºöWeek 3 - Logistic Regression\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n  For now, we will focus on the binary classification problem.\n  $y \\in {0,1}$ - the variable that we\u0026rsquo;re trying to predict","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà3Ôºâ"},{"content":" Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \\cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \\cdots, n$).   Hypothesis: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$  Previously: $h_\\theta(x) = \\theta_0 + \\theta_1x$ Now: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4$ For convenience of notation, define $x_0 = 1 \\ (x_0^{(i)}=1,\\ i=1, 2, \\cdots, m)$. $x = [ \\begin{matrix}x_0 \u0026amp; x_1 \u0026amp; x_2 \u0026amp; \\cdots \u0026amp; x_n \\end{matrix}]^T \\in \\mathbb{R^{n+1}}$ $\\theta = [ \\begin{matrix}\\theta_0 \u0026amp; \\theta_1 \u0026amp; \\theta_2 \u0026amp; \\cdots \u0026amp; \\theta_n \\end{matrix}]^T \\in \\mathbb{R^{n+1}}$   Multivariate Linear Regression  $$\\begin{align} h_\\theta(x)\u0026amp;=\\theta^Tx= \\begin{bmatrix}\\theta_0 \\ \\theta_1 \\ \\theta_2 \\ \\cdots \\ \\theta_n \\end{bmatrix} \\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ \\cdots \\ x_n \\end{bmatrix} = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\end{align}$$\n5.2 Gradient Descent for Multiple Variables   Hypothesis: $\\begin{align} h_\\theta(x)\u0026amp;=\\theta^Tx = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\end{align}$\n  Parameters: $\\theta_0, \\theta_1, \\dots, \\theta_n$ ($\\theta = \\begin{bmatrix} \\theta_0 \u0026amp; \\theta_1 \u0026amp; \\cdots \u0026amp; \\theta_n \\end{bmatrix}^T$)\n  Cost function: $J(\\theta_0, \\theta_1, \\dots, \\theta_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 $\n  Gradient descent:\n$$\\left.\\begin{array}{l} \\text{repeat until convergence}\\ { \\ \\qquad \\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} \\ \\qquad (\\text{simultaneously update for every } \\theta_j \\text{ for } j := 0\\dots n) \\ } \\end{array}\\right.$$\n  The following image compares gradient descent with one variable to gradient descent with multiple variables:   5.3 Gradient Descent in Practice I - Feature Scaling  Feature scaling Idea: make sure features are on similar scale. Get every feature into approximately a $-1 \\leq x_i \\leq 1$ range.   Mean normalization: replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean.  $x_i:= \\frac{x_i-u_i}{s_i}$: $u_i$ is the average of all the values for the feature $(i)$ and $s_i$ is the range of the values $(\\text{max - min})$, or $s_i$ is the standard deviation.    5.4 Gradient Descent in Practice II - Learning Rate  Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(\\theta)$, over the number of iterations of gradient descent. if $J(\\theta)$ ever increases, then you probably need to decrease $\\alpha$. Automatic convergence test. Declare convergence if $J(\\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it\u0026rsquo;s difficult to choose this threshold value. Make sure gradient descent is working correctly.  For sufficient small $\\alpha$, $J(\\theta)$ should decrease on every iteration. If $\\alpha$ is too large: may not decrease on every iteration and thus may not converge. if $\\alpha$ is too small: slow convergence.    5.5 Features and Polynomial Regression  [Example] Housing prices prediction: $h_\\theta(x)=\\theta_0 + \\theta_1 \\times frontage + \\theta_2 \\times depth$ Polynomial regression: We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).  6. Computing Parameters Analytically 6.1 Normal Equation   Normal equation: Method to solve for $\\theta$ analytically.\n $\\theta \\in \\mathbb{R^{n+1}}\\qquad J(\\theta_0,\\theta_1,\\dots,\\theta_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$ $\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\cdots=0 \\qquad (\\text{for every }j)$ Solve for $\\theta_0, \\theta_1, \\dots,\\theta_n$    $m$ examples $(x^{(1)},y^{(1)}), \\dots,(x^{(m)}, y^{(m)})$; $n$ features.\n$$x^{(i)}=\\begin{bmatrix} x_0^{(i)} \\ x_1^{(i)} \\ x_2^{(i)} \\ \\vdots \\ x_n^{(i)} \\end{bmatrix} \\in \\mathbb{R^{n+1}}, \\qquad \\mathop{x}\\limits_{\\text{(design matrix)}} = \\begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \\vdots \\ (x^{(m)})^T \\end{bmatrix}_{m\\times(n+1)}$$\n  $\\theta=(X^TX)^{-1}X^Ty$\n  The following is a comparison of gradient descent and the normal equation:\n     Gradient Descent Normal Equation     Need to choose $\\alpha$. No need to choose $\\alpha$.   Needs many iterations. Don\u0026rsquo;t need to iterate.   $\\text{O}(kn^2)$ $\\text{O}(n^3)$, Need to compute $(X^TX)^{-1}$   Works well even when $n$ is large. Slow if $n$ is very large.    6.2 Normal Equation and Noninvertibility  Normal equation: $\\theta=(X^TX)^{-1}X^Ty$. But what if $X^TX$ is non-invertible (singular/degenerate)? If $X^TX$ is noninvertible, the common causes might be having:  Redundant features, where two features are very closely related (i.e. they are linearly dependent). Too many features (e.g. $m \\leq n$). In this case, delete some features, or use regularization.    Ex1: Linear Regressionüë®‚Äçüíª  See this exercise on Coursera-MachineLearning-Python/ex1/ex1.ipynb\n Ex1.1 Linear regression with one variable Instruction: In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\nCode:\nimport numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D \u0026#34;\u0026#34;\u0026#34; Part 1: Basic Function \u0026#34;\u0026#34;\u0026#34; def computeCost(X, y, theta): \u0026#39;\u0026#39;\u0026#39; Compute cost for linear regression :param X: input variables :param y: output variables :param theta: parameters :return: Cost function \u0026#39;\u0026#39;\u0026#39; m = len(y) J = 0 for xi, yi in zip(X, y): J = J + np.square(float(np.dot(xi, theta)) - yi) return J / (2 * m) def gradientDescent(X, y, theta, alpha, num_iters): \u0026#39;\u0026#39;\u0026#39; updates theta by taking num_iters gradient steps with learning rate alpha :param X: input variables :param y: output variables :param theta: parameters :param alpha: learning rate :param num_iters: times of iteration :return: [theta, J_history] \u0026#39;\u0026#39;\u0026#39; # Initialize some useful values m = len(y) J_history = np.zeros(num_iters) for i in range(num_iters): sum0, sum1 = 0, 0 for j in range(m): sum0 = sum0 + (np.dot(X[j], theta) - y[j]) * X[j][0] sum1 = sum1 + (np.dot(X[j], theta) - y[j]) * X[j][1] theta[0] = theta[0] - alpha * sum0 / m theta[1] = theta[1] - alpha * sum1 / m # Save the cost J in every iteration J_history[i] = computeCost(X, y, theta) return theta, J_history \u0026#34;\u0026#34;\u0026#34; Part 2: Plotting \u0026#34;\u0026#34;\u0026#34; print(\u0026#39;Plotting Data ...\u0026#39;) data = np.loadtxt(\u0026#34;ex1data1.txt\u0026#34;, dtype=float, delimiter=\u0026#39;,\u0026#39;) X = data[:, 0] y = data[:, 1] m = len(y) # number of training examples # Plot Data fig0, ax0 = plt.subplots() ax0.scatter(X, y, marker=\u0026#39;x\u0026#39;, label=\u0026#39;Training data\u0026#39;) ax0.set_xlabel(\u0026#39;Population of City in 10,000s\u0026#39;) ax0.set_ylabel(\u0026#39;Profit in $10,000s\u0026#39;) \u0026#34;\u0026#34;\u0026#34; Part 3: Cost and Gradient Descent \u0026#34;\u0026#34;\u0026#34; X = np.c_[np.ones((m, 1)), data[:,0]] # Add a column of ones to X theta = np.zeros((2, 1)) # initialize fitting parameters # Some gradient descent settings iterations = 1500 alpha = 0.01 print(\u0026#39;\\nTesting the cost function ... \\n\u0026#39;) # compute and display initial cost J = computeCost(X, y, theta) print(\u0026#39;With theta = [0 ; 0], cost computed = \u0026#39;, J) print(\u0026#39;Expected cost value (approx) 32.07\\n\u0026#39;) # further testing of the cost function J = computeCost(X, y, np.array([[-1], [2]])) print(\u0026#39;With theta = [-1 ; 2], cost computed = \u0026#39;, J) print(\u0026#39;Expected cost value (approx) 54.24\\n\u0026#39;) print(\u0026#39;Running Gradient Descent ...\\n\u0026#39;) # run gradient descent theta, J_history = gradientDescent(X, y, theta, alpha, iterations) # print theta to screen print(\u0026#39;Theta found by gradient descent: Œ∏0 =\u0026#39;, theta[0], \u0026#39;, Œ∏1 =\u0026#39;, theta[1]) print(\u0026#39;Expected theta values (approx): -3.6303 1.6664\\n\u0026#39;) # Plot the linear fit ax0.plot(X[:, 1], np.dot(X, theta), color=\u0026#39;r\u0026#39;, label=\u0026#39;Linear regression\u0026#39;) ax0.legend(loc=\u0026#39;lower right\u0026#39;) ax0.set_title(\u0026#39;Linear regression with one variable\u0026#39;) # Predict values for population sizes of 35,000 and 70,000 predict1 = np.dot([1, 3.5], theta) predict2 = np.dot([1, 7], theta) print(\u0026#39;For population = 35,000, we predict a profit of \u0026#39;, predict1 * 10000) print(\u0026#39;For population = 70,000, we predict a profit of \u0026#39;, predict2 * 10000) \u0026#34;\u0026#34;\u0026#34; Part 4: Visualizing J(theta_0, theta_1) \u0026#34;\u0026#34;\u0026#34; print(\u0026#39;\\nVisualizing J(theta_0, theta_1) ... \\n\u0026#39;) # Plot the reducing of cost function during iteration fig1, ax1 = plt.subplots() ax1.plot(np.arange(iterations), J_history, \u0026#39;b\u0026#39;) ax1.set_xlabel(\u0026#39;Iterations\u0026#39;) ax1.set_ylabel(r\u0026#39;$J(\\theta_0, \\theta_1)$\u0026#39;) ax1.set_title(r\u0026#39;Reducing of $J(\\theta_0, \\theta_1)$ during iteration\u0026#39;) # Grid over which we will calculate J theta0_vals = np.linspace(-10, 10, 100) theta1_vals = np.linspace(-1, 4, 100) # Initialize J_vals to a matrix of 0\u0026#39;s J_vals = np.zeros([len(theta0_vals), len(theta1_vals)]) # Fill out J_vals for i in range(len(theta0_vals)): for j in range(len(theta1_vals)): t = [theta0_vals[i], theta1_vals[j]] J_vals[i][j] = computeCost(X, y, t) x_contour, y_contour = theta0_vals, theta1_vals theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals) # Produce surface and contour plots of J(Œ∏) fig2 = plt.figure() ax2 = Axes3D(fig2) ax2.plot_surface(theta0_vals, theta1_vals, J_vals.T, rstride=1, cstride=1, cmap=cm.rainbow) ax2.set_xlabel(r\u0026#39;$\\theta_0$\u0026#39;) ax2.set_ylabel(r\u0026#39;$\\theta_1$\u0026#39;) ax2.set_title(\u0026#39;Surface\u0026#39;) fig3, ax3 = plt.subplots() CS = ax3.contour(theta0_vals, theta1_vals, J_vals.T, levels=50) ax3.plot(theta[0], theta[1], \u0026#39;bx\u0026#39;) ax3.set_xlabel(r\u0026#39;$\\theta_0$\u0026#39;) ax3.set_ylabel(r\u0026#39;$\\theta_1$\u0026#39;) ax3.set_title(\u0026#39;Contour\u0026#39;) plt.show() Output:\n Console   Training data with linear regression fit.   Reducing of $J(\\theta_0, \\theta_1)$ during iteration   Produce surface and contour plots of $J(Œ∏)$  Ex1.2 Linear regression with multiple variables Instruction: In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\nCode:\nimport numpy as np import matplotlib.pyplot as plt \u0026#34;\u0026#34;\u0026#34; Part 0: Basic Function \u0026#34;\u0026#34;\u0026#34; def computeCost(X, y, theta): \u0026#39;\u0026#39;\u0026#39; Compute cost for linear regression :param X: input variables :param y: output variables :param theta: parameters :return: Cost function \u0026#39;\u0026#39;\u0026#39; m = len(y) J = np.sum(np.square(X.dot(theta) - y)) / (2 * m) return J def gradientDescentMulti(X, y, theta, alpha, num_iters): \u0026#39;\u0026#39;\u0026#39; updates theta by taking num_iters gradient steps with learning rate alpha :param X: input variables :param y: output variables :param theta: parameters :param alpha: learning rate :param num_iters: times of iteration :return: [theta, J_history] \u0026#39;\u0026#39;\u0026#39; # Initialize some useful values m, n = len(y), len(theta) J_history = np.zeros(num_iters) for i in range(num_iters): temp = np.dot((np.dot(X, theta) - y.reshape(m, 1)).T, X) theta = theta - alpha * temp.T / m J_history[i] = computeCost(X, y, theta) return theta, J_history def featureNormalize(X): \u0026#39;\u0026#39;\u0026#39; Normalize the features in X :param X: features :return: X_norm: normalized X; mean: mean value; sigma: standard deviation \u0026#39;\u0026#39;\u0026#39; mean = np.mean(X, 0) sigma = np.std(X, 0) X_norm = (X - mean) / sigma return X_norm, mean, sigma def normalEqn(X, y): \u0026#39;\u0026#39;\u0026#39; Computes the closed-form solution to linear regression :param X: input variables :param y: output variables :return: parameters \u0026#39;\u0026#39;\u0026#39; theta = np.linalg.inv(X.T@X)@X.T@y return theta \u0026#34;\u0026#34;\u0026#34; Part 1: Feature Normalization \u0026#34;\u0026#34;\u0026#34; # Load data data = np.loadtxt(\u0026#39;ex1data2.txt\u0026#39;, dtype=float, delimiter=\u0026#39;,\u0026#39;) X = data[:, 0:2] y = data[:, 2:3] m = len(y) # Scale features and set them to zero mean X_norm, mean, sigma = featureNormalize(X) y_norm, _, _ = featureNormalize(y) # Add intercept term to X X_norm = np.c_[np.ones([m, 1]), X_norm] \u0026#34;\u0026#34;\u0026#34; Part 2: Gradient Descent \u0026#34;\u0026#34;\u0026#34; print(\u0026#39;Running gradient descent ... \u0026#39;) # Choose some alpha value alpha = 0.01 num_iters = 1000 # Init theta and run gradient descent theta = np.zeros([3, 1]) theta, J_history = gradientDescentMulti(X_norm, y, theta, alpha, num_iters) # Plot the convergence graph fig1, ax1 = plt.subplots() ax1.plot(np.arange(num_iters), J_history, \u0026#39;b\u0026#39;) ax1.set_xlabel(\u0026#39;Iterations\u0026#39;) ax1.set_ylabel(r\u0026#39;$J(\\theta)$\u0026#39;) ax1.set_title(r\u0026#39;Convergence of $J(\\theta)$\u0026#39;) # Display gradient descent\u0026#39;s result print(\u0026#39;Theta computed from gradient descent: \\n\u0026#39;, theta) \u0026#34;\u0026#34;\u0026#34; Part 3: Normal Equations \u0026#34;\u0026#34;\u0026#34; print(\u0026#39;\\nSolving with normal equations ... \u0026#39;) # Add intercept term to X X = np.c_[np.ones([m, 1]), X] # Calculate the parameters from the normal equation theta1 = normalEqn(X, y) print(\u0026#39;Theta computed from normal equations: \\n\u0026#39;, theta1) plt.show() Output:\n Console (There is some differences between the two results)   The convergence graph  ","permalink":"https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-2/","summary":"Course Link ÔºöWeek 2 - Linear Regression with Multiple Variables\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \\cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \\cdots, n$).","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ"},{"content":" Course Link ÔºöWeek 1 - Introduction\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 0. ÂÜôÂú®ËØæÂâç  ‰Ωú‰∏∫Êú∫Âô®Â≠¶‰π†ÂÖ•Èó®ËØæÁ®ã‰∏≠ÁöÑÁªèÂÖ∏ÔºåÂê¥ÊÅ©ËææÊïôÊéàÁöÑËØæÁ®ãÂÖ∑ÊúâÂ¶Ç‰∏ãÁâπÁÇπÔºö  Êï∞Â≠¶Êé®ÂØºËøáÁ®ãÂ∞ëÔºåÂÅèÈáçÂü∫Á°ÄÔºåÈó®ÊßõËæÉ‰Ωé ËØæÁ®ãÁü≠Â∞èÁ≤æÊÇçÔºåËÆ≤Ëß£Ê∏ÖÊô∞ÔºåË¶ÜÁõñÈù¢Âπø ‰∏ªË¶ÅÁî® Matlab ÂÆåÊàêËØæÂêéÁ®ãÂ∫è‰Ωú‰∏ö   ËØæÁ®ãÂ≠¶‰π†ÁõÆÊ†áÔºö  ÁêÜËß£Êú∫Âô®Â≠¶‰π†ÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÂíåÊñπÊ≥ï Áî® Python ÂÆûÁé∞ËØæÁ®ãË¶ÅÊ±ÇÊéåÊè°ÁöÑÁÆóÊ≥ï    1. Introduction 1.1 Welcome   Machine Learning\n Grew out of work in AI New capability for computers    Examples:\n  Database mining\n  Large datesets from growth of automation/web\nE.g., Web click data, medical records, biology, engineering\n    Applications can\u0026rsquo;t program by hand.\nE.g., Autonomous helicopter, handwriting recognition, most of Natural Language Processing(NLP), Computer Vision.\n  Self-customizing programs\nE.g., Amazon, Netflix product recommendations\n  Understanding human learning (brain, real AI).\n    1.2 What is Machine Learning?  Machine Learning definition  Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998). Well-posed Learing Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, inproves with experience E.   Machie Learning algorithms  Supervised learing(ÁõëÁù£Â≠¶‰π†) Unsupervised learning(Êó†ÁõëÁù£Â≠¶‰π†) Others: Reinforcement learing(Âº∫ÂåñÂ≠¶‰π†), recommender systems(Êé®ËçêÁ≥ªÁªü) Also talk about: Practical advice for applying learnng algorithms. \u0026ldquo;You have all these tools but the more important thing is to learn how to use these tools properly.\u0026rdquo;    1.3 Supervised Learning  Supervised Learning: \u0026ldquo;right answers\u0026rdquo; given  In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into \u0026ldquo;regression\u0026rdquo; and \u0026ldquo;classification\u0026rdquo; problems.   Regression: predict continuous valued output  In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function.   Classification: discrete valued output (like, 0 or 1)  In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.    1.4 Unsupervised Learning  Unsupervised Learning Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don\u0026rsquo;t necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.   Application  Organize computing clusters Social network analysis Market segmentation Astrononical data analysis   Cocktail party problem  2. Model and Cost Function 2.1 Model Representation   Training set:   Notation:\n $m$ = Number of training examples $\\boldsymbol{x}\u0026rsquo;s$ = \u0026ldquo;input\u0026rdquo; variables / features $\\boldsymbol{y}\u0026rsquo;s$ = \u0026ldquo;output\u0026rdquo; variables / \u0026ldquo;target\u0026rdquo; variables $(x,\\ y)$ = a single training example, $(x^{(i)},\\ y^{(i)})$ = a specific training example, $i = 1, \\dots,m$ $X$ = the space of input values $Y$ = the space of output values $h$ = hypothesis, is a function that maps from $\\boldsymbol{x}\u0026rsquo;s$ to $\\boldsymbol{y}\u0026rsquo;s$    Our goal is, given a training set, to learn a function $h:X\\rightarrow Y$, so that $h(x)$ is a ‚Äúgood‚Äù predictor for the corresponding value of $y$. The process is therefore like this:  Linear regression with one variable Linear function: $h_{\\theta}(x) = \\theta_0+\\theta_1*x$  $\\theta_{i}$: parameters      2.2 Cost Function   Idea: Choose $\\theta_0$, $\\theta_1$ so that $h_\\theta(x)$ is close to $y$ for our training examples $(x,y)$\n  Cost function(also called the Squared error functionor Mean squared error, often used for regression problems):\n$$J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y}i-y_i)^2 = \\frac{1}{2m}\\sum{i=1}^m[h_\\theta(x^{(i)})-y^{(i)}]^2$$\n  $h_\\theta(x^{(i)}) = \\theta_0+\\theta_1 x^{(i)} = \\hat{y}_i$\n  Goal: to minimize the value of this function\n  2.3 Cost Function - Intuition I  Review  Hypothesis: $h_\\theta(x)=\\theta_0+\\theta_1x$ Parameters: $\\theta_0$, $\\theta_1$ Cost Function: $J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}]^2$ Goal: $\\mathop{\\text{minimize}}\\limits_{\\theta_0, \\theta_1} J(\\theta_0, \\theta_1)$   Simplified  $h_\\theta(x)=\\theta_1x$ (i.e. $\\theta_0$ = 0, there is only one parameter: $\\theta _1$) $\\mathop{\\text{minimize}}\\limits_{\\theta_0}J(\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}]^2$    2.4 Cost Function - Intuition II  This section refers to Cost Function Intuition II - Reading material\n A contour plot is a graph that contains many contour lines. A contour line of two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below. Taking any color and going along the \u0026lsquo;circle\u0026rsquo;, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J(\\theta_0, \\theta_1)$ and as a result, they are found along the same line. The circled $\\times$ displays the value of the cost function for the graph on the left when $\\theta_0$ = 800 and $\\theta_1$ = -0.15. Taking another $h(x)$ and plotting its contour plot, one gets the following graphs: When $\\theta_0$ = 360 and $\\theta_1$ = 0, the value of $J(\\theta_0, \\theta_1)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data. The graph above minimizes the cost function as much as possible and consequently, the result of $\\theta_1$ and $\\theta_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most \u0026lsquo;circle\u0026rsquo;.\n3. Parameter Learning 3.1 Gradient Descent   Now:\n Have some function $J(\\theta_0, \\theta_1)$ Want $\\mathop{\\text{min}}\\limits_{\\theta_0, \\theta_1} J(\\theta_0, \\theta_1)$    Outline:\n Start with some $\\theta_0$, $\\theta_1$ (for example, set $\\theta_0$ = 0, $\\theta_1$ = 0) Keep changing $\\theta_0$, $\\theta_1$ to reduce $J(\\theta_0, \\theta_1)$ until we hopefully end up at a minimum    Gredient descent algorithm:\n$$\\left. \\begin{array}{l} \\text{repeat until convergence}\\ {\\ \\qquad \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)\\quad(\\text{for}\\ j=0\\ \\text{and}\\ j=1) \\ } \\end{array} \\right.$$\n  $:=$, assianment operator (ËµãÂÄºËøêÁÆóÁ¨¶)\n  $\\alpha$, learing rate (a number)\n  Simultaneous update $\\theta_0$ and $\\theta_1$:\n$$\\left. \\begin{array}{l} \\color{blue}{\\text{temp0}} := \\theta_0 - \\alpha\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)\\ \\color{blue}{\\text{temp1}} := \\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)\\ \\theta_0:=\\color{blue}{\\text{temp0}}\\ \\theta_1:=\\color{blue}{\\text{temp1}} \\end{array} \\right.$$\n    3.2 Gradient Descent Intuition  This section refers to Gradient Descent Intuition - Reading material\n We explored the scenario where we used one parameter $\\theta_1$ and plotted its cost function to implement a gradient descent. Our formula for a single parameter was:\n$$\\left. \\begin{array}{l} \\text{repeat until convergence}\\ {\\ \\qquad \\theta_1 := \\theta_1 - \\alpha\\frac{d}{ d\\theta_1}J(\\theta_1) \\ } \\end{array} \\right.$$\nRegardless of the slope\u0026rsquo;s sign for $\\frac{d}{ d\\theta_1}J(\\theta_1)$, $\\theta_1$ eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of $\\theta_1$ increases and when it is positive, the value of $\\theta_1$ decreases. On a side note, we should adjust our parameter $\\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong. How does gradient descent converge with a fixed step size $\\alpha$?\nThe intuition behind the convergence is that $\\frac{d}{ d\\theta_1}J(\\theta_1)$ approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get: $\\theta_1 := \\theta_1-\\alpha*0$. 3.3 Gradient Descent For Linear Regression   Put together gradient descentwith cost function: apply gradient descent to minimize our squared error cost function.\n  Gradient descent algorithm\n$$\\left. \\begin{array}{l} \\text{repeat until convergence}\\ {\\ \\qquad \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)\\qquad(\\text{for}\\ j=0\\ \\text{and}\\ j=1) \\ } \\end{array} \\right.$$\n  Linear Regression Model\n$$\\begin{align}h_{\\theta}(x)\u0026amp;=\\theta_0+\\theta_1x \u0026amp;\\text{linear hypothesis} \\ J(\\theta_0, \\theta_1)\u0026amp;=\\frac{1}{2m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}]^2 \u0026amp;\\text{squared error cost function} \\end{align}$$\n  Partial derivative:\n$$\\begin{align} \\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)\u0026amp;= \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}]^2 \\ \u0026amp;= \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum_{i=1}^{m}[\\theta_0 + \\theta_1 x^{(i)}-y^{(i)}]^2 \\end{align} $$\n$$\\begin{align} j=0\u0026amp;:\\quad \\frac{\\partial}{\\partial\\theta_0}J(\\theta_0, \\theta_1) =\\frac{1}{m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}] \\ j=1\u0026amp;:\\quad\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0, \\theta_1) =\\frac{1}{m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}]\\cdot x^{(i)}\\end{align} $$\n  \u0026ldquo;Batch\u0026rdquo; Gradient Descent \u0026ldquo;Batch\u0026rdquo;: Each step of gradient descent uses all the traning examples.     The following section refers to Gradient Descent For Linear Regression - Reading material\n We specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to:\n$$ \\left. \\begin{array}{l} \\text{repeat until convergence}\\ {\\ \\qquad \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}] \\ \\qquad \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}[h_\\theta(x^{(i)})-y^{(i)}]\\cdot x^{(i)} \\ } \\end{array} \\right. $$\nWhere $m$ is the size of training set, $\\theta_0$ a constant that will be changing simultaneously with $\\theta_1$ and $x_i$, $y_i$ are values of the given training set (data).\nNote that we have separated out the two cases for $\\theta_j$ into separate equations for $\\theta_0$ and $\\theta_1$; and that for $\\theta_1$ we are multiplying $x_i$ at the end due to the derivative. The following is a derivative of $\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$ for a single example:\n$$ \\begin{align} \\frac{\\partial}{\\partial\\theta_j}J(\\theta) \u0026amp;= \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2}(h_\\theta(x)-y)^2 \\ \u0026amp;= 2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y) \\ \u0026amp;= (h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(\\sum_{i=0}^n\\theta_ix_i-y) \\ \u0026amp;= (h_\\theta(x) - y)\\cdot x_i \\end{align} $$\nThe point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.\nSo, this is simply gradient descent on the original cost function $J$. This method looks at every example in the entire training set on every step, and is called batch gradient descent. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning $\\alpha$ is not too large) to the global minimum. Indeed, $J$ is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function. The ellipse shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at $(48,\\ 30)$. The $x\u0026rsquo;s$ in the figure (joined by straight lines) mark the successive values of $\\theta$ that gradient descent went through as it converged to its minimum.\n4. Linear Algebra Review 4.1 Matrices and Vectors  Matrix: Rectangular array of numbers.  Dimension of matrix: number of rows $\\times$ number of columns Matrix elements (entries of matrix): $A_{ij} = \\ \u0026ldquo;i, j\\text{ entry}\u0026rdquo; \\text{ in the } i^{th} \\text{ row},\\ j^{th}\\text{ column.}$   Vector: An $n\\times1$ matrix  Vector elements: $y_i=i^{th} \\text{ element}$ Generally, we use 1-indexed vectors in this course.   Notation and terms:  $A_{ij}$ refers to the element in the $i^{th}$ row and $j^{th}$ column of matrix $A$. A vector with \u0026lsquo;$n$\u0026rsquo; rows is referred to as an \u0026lsquo;$n$\u0026rsquo;-dimensional vector. $v_i$ refers to the element in the $i^{th}$ row of the vector. Scalarmeans that an object is a single value, not a vector or matrix. $\\mathbb{R}$ refers to the set of scalar real numbers. $\\mathbb{R^n}$ refers to the set of $n$-dimensional vectors of real numbers.    4.2 Addition and Scalar Multiplication  Matrix addition Scalar multiplication Combination of operands  4.3 Matrix-Vector Multiplicaiton We map the column of the vector onto each row of the matrix, multiplying each element and summing the result.\n$$ \\begin{bmatrix} a \u0026amp; b \\newline c \u0026amp; d \\newline e \u0026amp; f \\end{bmatrix} \\begin{bmatrix} x \\newline y \\newline \\end{bmatrix} =\\begin{bmatrix} ax + by \\newline cx + dy \\newline ex + f*y\\end{bmatrix} $$\nThe result is a vector. The number of columns of the matrix must equal the number of rows of the vector. An $m\\times n$ matrix multiplied by an $n \\times 1$ vector results in an $m \\times 1$ vector.\n4.4 Matrix-Matrix Multiplication We multiply two matrices by breaking it into several vector muliplicaitons and concatenating the result.\n$$ \\begin{bmatrix} a \u0026amp; b \\newline c \u0026amp; d \\newline e \u0026amp; f \\end{bmatrix} \\begin{bmatrix} w \u0026amp; x \\newline y \u0026amp; z \\newline \\end{bmatrix} =\\begin{bmatrix} aw + by \u0026amp; ax + bz \\newline cw + dy \u0026amp; cx + dz \\newline ew + fy \u0026amp; ex + f*z\\end{bmatrix} $$\nAn $m\\times n$ matrix multiplied by an $n\\times o$ matrix results in an $m \\times o$ matrix. To multiply two matrices, the number of columns of the first matrix must equal the number of rows of the second matrix.\n4.5 Matrix Multiplication Properties  Let $A$ and $B$ be matrices. Then in general, $A\\cdot B \\neq B \\cdot A$ (not commutative). $A \\cdot B \\cdot C = (A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$. Identity Matrix: denoted $I$ (or $I_{n\\times n}$). For any matrix $A_{m\\times n}$, $A_{m\\times n} \\cdot I_{n\\times n} = I_{m \\times m} \\cdot A_{m \\times n} = A_{m \\times n}$.  4.6 Inverse and Transpose  Matrix inverse: if $A$ is an $m\\times m$ matrix (squre matrix), and if it has an inverse, then $AA^{-1}=A^{-1}A=I$.  Matrices that don\u0026rsquo;t have an inverse are \u0026ldquo;singular\u0026rdquo; or \u0026ldquo;degenerate\u0026rdquo;.   Matrix Transpose: Let $A$ be an $m \\times n$ matrix, and let $B = A^T$. Then $B$ is an $n \\times m$ matrix, and $B_{ij}=A_{ji}$.  ","permalink":"https://fang-lansheng.github.io/posts/2020-02-26-ml-ng-1/","summary":"Course Link ÔºöWeek 1 - Introduction\nCode : Fang-Lansheng/Coursera-MachineLearning-Python\n 0. ÂÜôÂú®ËØæÂâç  ‰Ωú‰∏∫Êú∫Âô®Â≠¶‰π†ÂÖ•Èó®ËØæÁ®ã‰∏≠ÁöÑÁªèÂÖ∏ÔºåÂê¥ÊÅ©ËææÊïôÊéàÁöÑËØæÁ®ãÂÖ∑ÊúâÂ¶Ç‰∏ãÁâπÁÇπÔºö  Êï∞Â≠¶Êé®ÂØºËøáÁ®ãÂ∞ëÔºåÂÅèÈáçÂü∫Á°ÄÔºåÈó®ÊßõËæÉ‰Ωé ËØæÁ®ãÁü≠Â∞èÁ≤æÊÇçÔºåËÆ≤Ëß£Ê∏ÖÊô∞ÔºåË¶ÜÁõñÈù¢Âπø ‰∏ªË¶ÅÁî® Matlab ÂÆåÊàêËØæÂêéÁ®ãÂ∫è‰Ωú‰∏ö   ËØæÁ®ãÂ≠¶‰π†ÁõÆÊ†áÔºö  ÁêÜËß£Êú∫Âô®Â≠¶‰π†ÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÂíåÊñπÊ≥ï Áî® Python ÂÆûÁé∞ËØæÁ®ãË¶ÅÊ±ÇÊéåÊè°ÁöÑÁÆóÊ≥ï    1. Introduction 1.1 Welcome   Machine Learning\n Grew out of work in AI New capability for computers    Examples:\n  Database mining\n  Large datesets from growth of automation/web\nE.g., Web click data, medical records, biology, engineering","title":"Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà1Ôºâ"},{"content":" Êú¨ÊñáÂèäÊé•‰∏ãÊù•Âá†ÁØáÂêåÁ≥ªÂàóÊñáÁ´†ÊòØÂ≠¶‰π† SIFT ÁÆóÊ≥ïÂíå OpenCV SIFT Ê∫êÁ†ÅÊó∂ÁöÑÂ≠¶‰π†Á¨îËÆ∞ÔºåÊï¥ÂêàËá™ÂèÇËÄÉÊñáÁåÆÂèäÂçöÂÆ¢„ÄÇÂº∫ÁÉàÂª∫ËÆÆÈòÖËØª ËÆ∫ÊñáÂéüÊñá„ÄÅGitHub‰∏äÁöÑÊ∫êÁ†Å ‰ª•Âèä @ËµµÊò•Ê±ü ÁöÑ opencv 2.4.9 Ê∫êÁ†ÅÂàÜÊûê„ÄÇ\n Lowe Âú® 2004 Âπ¥ÊèêÂá∫‰∫ÜÂ∞∫Â∫¶‰∏çÂèòÁâπÂæÅÂèòÊç¢ (Scale Invariant Feature Transform, SIFT) ÁÆóÊ≥ï„ÄÇ SIFT ‰∏ªË¶ÅÁî±ÂÖ≥ÈîÆÁÇπÊé¢ÊµãÂô® (detector) ÂíåÊèèËø∞Á¨¶ (descriptor) ÁªÑÊàêÔºåÂÆÉÁöÑÂÆûÁé∞ÂàÜ‰∏∫‰ª•‰∏ãÂõõÊ≠•:\n Â∞∫Â∫¶Á©∫Èó¥ÊûÅÂÄºÊé¢Êµã (scale-space extrema detection)ÔºöÈÄöËøáÈ´òÊñØÂ∑ÆÂàÜÂáΩÊï∞ÊêúÁ¥¢ÊâÄÊúâÂ∞∫Â∫¶ÂíåÂõæÂÉè‰ΩçÁΩÆÔºå‰ª•ËØÜÂà´ÂØπ‰∫éÂ∞∫Â∫¶ÂíåÊñπÂêë‰∏çÂèòÁöÑÊΩúÂú®ÂÖ¥Ë∂£ÁÇπ„ÄÇ ÂÖ≥ÈîÆÁÇπÁ≤æÁ°ÆÂÆö‰Ωç (keypoint localization)ÔºöÁ≤æÁ°ÆÁ°ÆÂÆöÊØè‰∏™ÂÄôÈÄâÁÇπÁöÑÂ∞∫Â∫¶‰∏é‰∫öÂÉèÁ¥†Á∫ß‰ΩçÁΩÆÔºåÊ†πÊçÆÂÖ∂Á®≥ÂÆöÊÄßÈòàÂÄºÈÄâÊã©ÂÖ≥ÈîÆÁÇπ„ÄÇ ÊñπÂêëÂàÜÈÖç (orientation assignment)ÔºöÂü∫‰∫éÂõæÂÉèÁöÑÂ±ÄÈÉ®Ê¢ØÂ∫¶ÊñπÂêëÔºå‰∏∫ÊØè‰∏™ÁâπÊÄßÁÇπÂàÜÈÖç‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÊñπÂêëËßíÂ∫¶„ÄÇÊâÄÊúâÂêéÁª≠ÁöÑÊìç‰ΩúÈÉΩÊòØÁõ∏ÂØπ‰∫éÊâÄÁ°ÆÂÆö‰∏ãÊù•ÁöÑÁâπÂæÅÁÇπÁöÑËßíÂ∫¶„ÄÅÂ∞∫Â∫¶Âíå‰ΩçÁΩÆÁöÑÂü∫Á°Ä‰∏äËøõË°åÁöÑÔºåÂõ†Ê≠§ÁâπÂæÅÁÇπÂÖ∑ÊúâËßíÂ∫¶„ÄÅÂ∞∫Â∫¶Âíå‰ΩçÁΩÆÁöÑ‰∏çÂèòÊÄß„ÄÇ ÂÖ≥ÈîÆÁÇπÊèèËø∞Á¨¶ (keypoint descriptor)ÔºöÂú®ÊâÄÈÄâÂÆöÁöÑÂ∞∫Â∫¶Á©∫Èó¥ÂÜÖÔºåÊµãÈáèÁâπÂæÅÁÇπÈÇªÂüüÂå∫ÂüüÁöÑÂ±ÄÈÉ®ÂõæÂÉèÊ¢ØÂ∫¶ÔºåÂ∞ÜËøô‰∫õÊ¢ØÂ∫¶ËΩ¨Êç¢Êàê‰∏ÄÁßçÂÖÅËÆ∏Â±ÄÈÉ®ËæÉÂ§ßÁ®ãÂ∫¶ÁöÑÂΩ¢Áä∂ÂèòÂΩ¢Âíå‰∫ÆÂ∫¶ÂèòÂåñÁöÑÊèèËø∞Á¨¶ÂΩ¢Âºè„ÄÇ  ‰∏ãÈù¢Â∞ÜÂØπÂÖ∂ËøõË°åËØ¶ÁªÜÈòêÈáäÔºö\n1. Â∞∫Â∫¶Á©∫Èó¥ÊûÅÂÄºÊé¢Êµã ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÁöÑÁ¨¨‰∏ÄÈò∂ÊÆµÊòØËØÜÂà´ÂèØ‰ª•Âú®Âêå‰∏ÄÂØπË±°ÁöÑ‰∏çÂêåËßÜÂõæ‰∏ãÈáçÂ§çÂàÜÈÖçÁöÑ‰ΩçÁΩÆÂíåÂ∞∫Â∫¶„ÄÇÈÄöËøáÂú®Á¥†ÊúâÂèØËÉΩÁöÑÂ∞∫Â∫¶ËøõË°åÊêúÁ¥¢ÔºåÂèØ‰ª•Ê£ÄÊµãÂá∫ÂØπÂõæÂÉèÂ∞∫Â∫¶‰∏çÂèòÁöÑÁ®≥ÂÆöÁâπÂæÅ„ÄÇËøô‰∏ÄËøáÁ®ã‰∏≠‰ΩøÁî®Âà∞ÁöÑÊòØË¢´Áß∞‰∏∫Â∞∫Â∫¶Á©∫Èó¥ (scale space) ÁöÑÂ∞∫Â∫¶ËøûÁª≠ÂáΩÊï∞^[1]„ÄÇ\nKoenderink (The structure of images, 1984) Âíå Lindeberg (Detecting salient blob-like image structures and their scales with a scale-space primal sketch: A method for focus-of-attention, 1993) Â∑≤ÁªèËØÅÊòéÔºåÂîØ‰∏ÄÂèØËÉΩÁöÑÂ∞∫Â∫¶Á©∫Èó¥Ê†∏ÊòØÈ´òÊñØÂáΩÊï∞„ÄÇÂõ†Ê≠§ÔºåÂõæÂÉèÁöÑÂ∞∫Â∫¶Á©∫Èó¥Ë¢´ÂÆö‰πâ‰∏∫ÂáΩÊï∞ $L(x, y, \\sigma)$ÔºåÂÆÉÊòØÁî±ÂèØÂèòÂ∞∫Â∫¶È´òÊñØÂáΩÊï∞ $G(x, y, \\sigma)$ ‰∏éËæìÂÖ•ÂõæÂÉè $I(x,y)$ Âç∑ÁßØÂæóÂà∞ÁöÑÔºö\n$$ \\begin{equation*} L(x, y, \\sigma) = G(x, y, \\sigma) \\bigotimes I(x, y) \\label{eq:scale-space} \\end{equation*} $$\nÂÖ∂‰∏≠Ôºå$\\bigotimes$ Ë°®Á§∫Âú® $(x, y)$ Â§ÑÁöÑÂç∑ÁßØËøêÁÆó„ÄÇ‰∏îÊúâÔºö\n$$ \\begin{equation*} G(x, y, \\sigma) = \\frac{1}{2\\pi \\sigma^2}e^{-(x^2+y^2)/2\\sigma^2} \\label{eq:gaussian} \\end{equation*} $$\n‰∏∫‰∫ÜÊúâÊïàÂú∞Ê£ÄÊµãÂ∞∫Â∫¶Á©∫Èó¥‰∏≠Á®≥ÂÆöÂÖ≥ÈîÆÁÇπÁöÑ‰ΩçÁΩÆÔºåLowe (Object recognition from local scale-invariant features, 1999) ÊèêÂá∫Âà©Áî®È´òÊñØÂ∑ÆÂàÜÂáΩÊï∞(difference-of-Gaussian) ‰∏éÂõæÂÉèÁöÑÂç∑ÁßØÊù•Ê±ÇÂæóÂ∞∫Â∫¶Á©∫Èó¥ÊûÅÂÄº $D(x, y, \\sigma)$„ÄÇÂÆÉÂèØ‰ª•ÈÄöËøáÈó¥ÈöîÂ∏∏Êï∞ $k$ ÁöÑÁõ∏ÈÇªÂ∞∫Â∫¶ÁöÑÂ∑ÆÂàÜÊù•ËÆ°ÁÆóÔºö\n$$ \\begin{equation} \\begin{aligned} D(x,y,\\sigma) \u0026amp;= (G(x, y, k\\sigma) - G(x, y, \\sigma)) \\bigotimes I(x, y) \\ \u0026amp;= L(x, y, k\\sigma) - L(x, y, \\sigma) \\end{aligned} \\label{eq:dog} \\end{equation} \\tag{1} $$\nËøôÊ†∑ÂÅöÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜÂáèÂ∞ëËÆ°ÁÆóÈáèÔºåÂõ†‰∏∫È´òÊñØÂ∑ÆÂàÜÂèØ‰ª•ÈÄöËøáÂõæÂÉèÈó¥ÁÆÄÂçïÁöÑÁõ∏ÂáèÂæóÂà∞„ÄÇÊ≠§Â§ñÔºåLindeberg ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈ´òÊñØÂ∑ÆÂàÜÂáΩÊï∞ $DoG$ ‰∏éÂ∞∫Â∫¶ÂΩí‰∏ÄÂåñÁöÑÈ´òÊñØÊãâÊôÆÊãâÊñØÁÆóÂ≠ê (Laplacian of Gaussian)Ôºå$\\sigma^2\\nabla^2G$ÔºåÊòØËøë‰ººÁõ∏Á≠âÁöÑ„ÄÇLowe Âú®ËÆ∫Êñá‰∏≠ÂêëÊàë‰ª¨ËØÅÊòé‰∫ÜÂ¶Ç‰∏ãÁªìËÆ∫Ôºö\n$$ \\begin{equation*} \\sigma\\nabla^2G = \\frac{\\partial G}{\\partial \\sigma} \\approx \\frac{G(x, y, k\\sigma) - G(x, y, \\sigma)}{k\\sigma - \\sigma} \\end{equation*} $$\nÂõ†Ê≠§, ÂàôÊúâ:\n$$ \\begin{equation*} G(x, y, k\\sigma) - G(x, y, \\sigma) \\approx (k - 1) \\sigma^2 \\nabla^2 G. \\end{equation*} $$\nÂºè‰∏≠Âõ†Â≠ê $(k-1)$ Âú®‰ªª‰ΩïÂ∞∫Â∫¶‰∏äÈÉΩÊòØÂ∏∏Êï∞ÔºåÂõ†Ê≠§‰∏ç‰ºöÂΩ±ÂÉèÊûÅÂÄº‰ΩçÁΩÆ„ÄÇ‰∏î Lowe ÂèëÁé∞Ëøë‰ººËØØÂ∑ÆÂØπ‰∫éÊûÅÂÄºÊé¢ÊµãÂíåÂÆö‰ΩçÁöÑÁ®≥ÂÆöÊÄßÂá†‰πéÊ≤°ÊúâÂΩ±Âìç„ÄÇ\nÈ´òÊñØÂ∑ÆÂàÜÂõæÂÉè $D(x, y, \\sigma)$ ÈáëÂ≠óÂ°îÁöÑÊûÑÈÄ†ÊñπÊ≥ïÂ¶ÇÂõæ 1 ÊâÄÁ§∫„ÄÇÈ´òÊñØÈáëÂ≠óÂ°îÂÖ±ÂàÜ O ÁªÑ (Octave), ÊØèÁªÑÂèàÂàÜ S Â±Ç (Layer)„ÄÇÁªÑÂÜÖÂêÑÂ±ÇÂõæÂÉèÁöÑÂàÜËæ®ÁéáÊòØÁõ∏ÂêåÁöÑÔºåÂç≥ÈïøÂíåÂÆΩÁõ∏ÂêåÔºå‰ΩÜÂ∞∫Â∫¶ÈÄêÊ∏êÂ¢ûÂä†ÔºåÂç≥Ë∂äÊé•ËøëÈ°∂Á´ØÂõæÂÉèË∂äÊ®°Á≥ä„ÄÇÊØè‰∏ÄÂ±ÇÁöÑÂàùÂßãÂõæÂÉè‰∏éÈ´òÊñØÈÄêÊ≠•Âç∑ÁßØÔºå‰∫ßÁîüÁî±Â∞∫Â∫¶Á©∫Èó¥‰∏≠ÁöÑÂ∏∏Êï∞Âõ†Â≠ê $k$ ÂàÜÈöîÁöÑÂõæÂÉèÔºåÂ¶ÇÂ∑¶ÂàóÊâÄÁ§∫„ÄÇÊØè‰∏ÄÂ±ÇÁöÑÈ´òÊñØÂõæÂÉèÈáëÂ≠óÂ°îÂÆåÊàê‰πãÂêéÔºåÊàë‰ª¨ÈÄâÂèñËØ•Â±ÇÁöÑÁ¨¨‰∫åÂº†ÂõæÂÉèËøõË°åÈöîÁÇπÈôçÈááÊ†∑ (ÂõæÂÉèÈïøÂíåÂÆΩÂáèÂ∞è‰∏∫ÂéüÊù•ÁöÑ $1/2$)Ôºå‰Ωú‰∏∫‰∏ã‰∏ÄÂ±ÇÁöÑÂàùÂßãÂΩ±ÂÉè (Âõ†Ê≠§ÂÖ∂Â∞∫Â∫¶Âõ†Â≠ê $\\sigma$ ‰∏∫‰∏äÂ±ÇÂõæÂÉèÁöÑ‰∏§ÂÄç)„ÄÇSIFT Â∞ÜÊØèÂ±ÇÂ∞∫Â∫¶Á©∫Èó¥ÂàíÂàÜ‰∏∫Êï¥Êï∞ $s$ ‰∏™Â≠êÂ±Ç, Âõ†Ê≠§ $k=2^{1/s}$„ÄÇÊâÄ‰ª•‰∏∫‰∫ÜË¶ÜÁõñÂÖ®ÈÉ®ÁöÑ $s$ Â∞∫Â∫¶, È´òÊñØÈáëÂ≠óÂ°î‰∏≠ÊØèÂ±ÇËá≥Â∞ëË¶ÅÊúâ $s+3$ Âº†ÂõæÂÉè„ÄÇÈ´òÊñØÈáëÂ≠óÂ°îÂ±Ç‰∏≠Áõ∏ÈÇªÂõæÂÉè‰πãÂ∑ÆÊûÑÊàê‰∫ÜÂè≥‰æßÁöÑÈ´òÊñØÂ∑ÆÂàÜÂõæÂÉèÈáëÂ≠óÂ°î„ÄÇ\nÊûÅÂÄºÁÇπÁöÑÊêúÁ¥¢ÊòØÂú®È´òÊñØÂ∑ÆÂàÜÈáëÂ≠óÂ°î‰∏≠ËøõË°åÁöÑ, Ëøô‰∫õÊûÅÂÄºÁÇπÂ∞±ÊòØÂÄôÈÄâÁöÑÁâπÂæÅÁÇπ„ÄÇ‰∏∫‰∫ÜÊ£ÄÊµã $D(x, y, \\sigma)$ ‰∏≠ÁöÑÂ±ÄÈÉ®ÊúÄÂ§ßÂÄºÂíåÊúÄÂ∞èÂÄºÔºåSIFT Â∞ÜÊØè‰∏™ÈááÊ†∑ÁÇπ‰∏éÂΩìÂâçÂõæÂÉè‰∏≠ÂÖ´‰∏™Áõ∏ÈÇªÂÉèÁ¥†ÁöÑÂÄº‰ª•Âèä‰∏ä‰∏ãÂ±ÇÂ∞∫Â∫¶‰∏≠ÁöÑ‰πù‰∏™Áõ∏ÈÇªÂÉèÁ¥†ÂÄºËøõË°åÂØπÊØî (Â¶ÇÂõæ 2)„ÄÇËøôÈ°πÊêúÁ¥¢Â∑•‰ΩúÁöÑËÆ°ÁÆóÊàêÊú¨ÈùûÂ∏∏‰Ωé, Âõ†‰∏∫ÂæàÂ§öÈááÊ†∑ÁÇπÂú®ÈÇªÂüüÂÉèÁ¥†ÂÄºÊØîËæÉÁöÑËøáÁ®ã‰∏≠Ë¢´ÊéíÈô§‰∫Ü„ÄÇ\n2. ÂÖ≥ÈîÆÁÇπÁöÑÁ≤æÁ°ÆÂÆö‰Ωç Áî±‰∫éÊûÅÂÄºÁÇπÁöÑÊêúÁ¥¢ÊòØÂú®Á¶ªÊï£Á©∫Èó¥‰∏≠ËøõË°åÁöÑÔºåÂπ∂‰∏îËøô‰∫õÁ¶ªÊï£Á©∫Èó¥ËøòÊòØÁªèËøá‰∏çÊñ≠ÈááÊ†∑ÂæóÂà∞ÁöÑ„ÄÇÈÄöËøáÂ±ÄÈÉ®ÊûÅÂÄºÊé¢ÊµãÁ°ÆÂÆöÂÄôÈÄâÁÇπÁöÑ‰ΩçÁΩÆÂíåÂ∞∫Â∫¶‰πãÂêé, Êàë‰ª¨ÈúÄË¶ÅÈÄöËøá‰∏âÁª¥‰∫åÊ¨°ÂáΩÊï∞ÊãüÂêàÂæóÂà∞ÂÖ≥ÈîÆÁÇπÁöÑÁ≤æÁ°Æ‰ΩçÁΩÆÔºå‰ª•ËææÂà∞‰∫öÂÉèÁ¥†Á∫ßÁöÑÁ≤æÂ∫¶„ÄÇ\nÊ†πÊçÆ Invariant Features from Interest Point Groups (2002) ÔºåÂ∞∫Â∫¶Á©∫Èó¥ÂáΩÊï∞ $D(x, y, \\sigma)$ Ê≥∞ÂãíÂ±ïÂºÄÂà∞‰∫åÊ¨°È°πÁöÑÂΩ¢Âºè‰∏∫Ôºö\n$$ \\begin{equation} D(\\mathbf{x})=D+\\frac{\\partial D}{\\partial \\mathbf{x}}^{T} \\mathbf{x}+\\frac{1}{2} \\mathbf{x}^{\\mathbf{T}} \\frac{\\partial^{2} D}{\\partial \\mathbf{x}^{2}} \\mathbf{x} \\label{eq:dog-taylor} \\end{equation} \\tag{2} $$\nÂÖ∂‰∏≠ $D$ ‰∏∫ $D(x, y, \\sigma)$ Âú®ÂÖ≥ÈîÆÁÇπÂ§ÑÁöÑÂÄºÔºå$\\mathbf{x}=(x, y, \\sigma)^T$ ÊòØÂÖ≥ÈîÆÁÇπÁöÑÂÅèÁßªÈáè„ÄÇ‰ª§\n$$ \\begin{equation} \\frac{\\partial D(\\mathbf{x})}{\\partial\\mathbf{x}} = 0 \\end{equation} $$\nÂç≥ÂèØÂæóÂà∞ $\\mathbf{x}$ ÁöÑÊûÅÂÄº $\\mathbf{\\hat{x}}$Ôºö\n$$ \\begin{equation} \\hat{\\mathbf{x}}=-\\frac{\\partial^{2} D^{-1}}{\\partial \\mathbf{x}^{2}} \\frac{\\partial D}{\\partial \\mathbf{x}} \\label{eq:dog-3} \\end{equation} \\tag{3} $$\nÂ¶ÇÊûú $\\hat{\\mathbf{x}}$ Âú®‰ªªÊÑèÊñπÂêë $(x, y, \\sigma)$ ‰∏äÂ§ß‰∫é 0.5ÔºåÂ∞±ÊÑèÂë≥ÁùÄËØ•ÂÖ≥ÈîÆÁÇπ‰∏éÂè¶‰∏ÄÈááÊ†∑ÈùûÂ∏∏Êé•ËøëÔºåËøôÊó∂Â∞±Áî®ÊèíÂÄºÊù•‰ª£ÊõøÂÖ≥ÈîÆÁÇπÁöÑ‰ΩçÁΩÆ„ÄÇÂÖ≥ÈîÆÁÇπÂÅáËÆæÂÅèÁßªÈáè $\\hat{\\mathbf{x}})$ Âç≥‰∏∫ÂÖ≥ÈîÆÁÇπÁöÑÁ°Æ‰ΩçÁΩÆ„ÄÇ‰∏∫‰∫Ü‰øùËØÅÁªìÊûúÁöÑÂáÜÁ°ÆÊÄßÔºåÊàë‰ª¨ÂæÄÂæÄ‰ΩøÁî®Ëø≠‰ª£ÁöÑÊñπÊ≥ïËøõË°åËøô‰∏ÄÊèíÂÄºËøáÁ®ã„ÄÇ\nÂÆö‰ΩçÂà∞ÂÖ≥ÈîÆÁÇπÁöÑÁ≤æÁ°Æ‰ΩçÁΩÆÂêéÔºå‰∏∫ÊèêÈ´òÂåπÈÖçÁöÑÁ®≥ÂÆöÊÄßÔºåÊàë‰ª¨ÈúÄË¶ÅÂà†Èô§‰ΩéÂØπÊØîÂ∫¶ÁöÑÁÇπ„ÄÇÂ∞ÜÂºè (3) ‰ª£ÂÖ• (2) ÂæóÔºö\n$$ \\begin{equation} D(\\hat{\\mathbf{x}})=D+\\frac{1}{2} \\frac{\\partial D}{\\partial \\mathbf{x}} \\hat{\\mathbf{x}} \\end{equation} $$\nÂºè‰∏≠ $D(\\hat{\\mathbf{x}})$ ÂèØ‰ª•Áî®Êù•Ë°°ÈáèÁâπÂæÅÁÇπÁöÑÂØπÊØîÂ∫¶ÔºåÂú® Lowe ÁöÑËÆ∫Êñá‰∏≠ÔºåÂØπÊØîÂ∫¶ $|D(\\hat{\\mathbf{x}})|$ Â∞è‰∫é 0.03 ÁöÑÊûÅÂÄºÁÇπ‰ºöË¢´ËàçÂºÉ„ÄÇ\nËÄå‰∏∫‰∫Ü‰øùËØÅÂÖ≥ÈîÆÁÇπÁöÑÁ®≥ÂÆöÊÄßÔºå‰ªÖ‰ªÖËàçÂºÉ‰ΩéÂØπÊØîÂ∫¶ÁöÑÂÄôÈÄâÁÇπÊòØ‰∏çÂ§üÁöÑ„ÄÇÈ´òÊñØÂ∑ÆÂàÜÂáΩÊï∞Âú®‰ºö‰∫ßÁîüÂæàÂº∫ÁöÑËæπÁºòÊïàÂ∫îÔºåÂõ†Ê≠§ÂæàÂÆπÊòìÂèóÂà∞Âô™Â£∞ÁöÑÂπ≤Êâ∞„ÄÇÊâÄ‰ª•Êàë‰ª¨‰πüÈúÄË¶ÅÂâîÈô§ÊéâËøô‰∫õ‰∏çÁ®≥ÂÆöÁöÑËæπÁºòÁÇπ„ÄÇ\nÈ´òÊñØÂ∑ÆÂàÜÂáΩÊï∞ÁöÑÁõ∏Â∫îÂ≥∞ÂÄºÂæÄÂæÄÂú®Ê®™Ë∑®ËæπÁºòÁöÑÂú∞ÊñπÊúâËæÉÂ§ßÁöÑÁöÑ‰∏ªÊõ≤ÁéáÔºåËÄåÂú®ÂûÇÁõ¥ËæπÁºòÁöÑÂú∞ÊñπÊúâËæÉÂ∞èÁöÑ‰∏ªÊõ≤Áéá„ÄÇ ‰∏ªÊõ≤ÁéáÂèØ‰ª•ÈÄöËøá $2 \\times 2$ ÁöÑ Hessian Áü©Èòµ $\\mathbf{H}$ Êù•ËÆ°ÁÆóÔºö\n$$ \\begin{equation} \\mathbf{H}=\\left[ \\begin{array}{cc}{D_{x x}} \u0026amp; {D_{x y}} \\ {D_{x y}} \u0026amp; {D_{y y}}\\end{array}\\right] \\label{eq:sift-hessian} \\end{equation} \\tag{4} $$\nÂÖ∂‰∏≠ÔºåÂØºÊï∞ÂèØ‰ª•ÈÄöËøáÁõ∏ÈÇªÊ†∑Êú¨ÁÇπÁöÑÂ∑ÆÂàÜÊù•ËÆ°ÁÆó„ÄÇ\n$\\mathbf{H}$ ÁöÑÁâπÂæÅÂÄº‰∏é $D$ ÁöÑ‰∏ªÊõ≤ÁéáÊàêÊ≠£ÊØî„ÄÇËÆæ $\\alpha$ ÊòØÊúÄÂ§ßÁöÑÁâπÂæÅÂÄºÔºå$\\beta$ ÊòØÊúÄÂ∞èÁöÑÁâπÂæÅÂÄº„ÄÇÁâπÂæÅÂÄºÁöÑÊÄªÂíå‰∏é‰πòÁßØÂèØ‰ª•ÂàÜÂà´ÈÄöËøá $\\mathbf{H}$ ÁöÑËøπ‰∏éË°åÂàóÂºèÊù•ËÆ°ÁÆóÔºö\n$$ \\begin{align} \\text{Tr}(\\mathbf{H}) \u0026amp;= D_{xx} + D_{yy} = \\alpha + \\beta \\ \\text{Det}(\\mathbf{H}) \u0026amp;= D_{xx}D_{yy} - (D_{xy})^2 = \\alpha\\beta \\end{align} $$\nÂ¶ÇÊûúË°åÂàóÂºè‰∏∫Ë¥üÔºåÂàôËØ•ÂÄôÈÄâÁÇπÂ∞ÜË¢´ËàçÂºÉ„ÄÇ‰ª§ $r$ ‰∏∫ÊúÄÂ§ßÁâπÂæÅÂÄº‰∏éÊúÄÂ∞èÁâπÂæÅÂÄºÁöÑÊØîÂÄºÔºåÂç≥ $r = \\alpha / \\beta$ÔºåÂàôÔºö\n$$ \\begin{equation} \\frac{\\operatorname{Tr}(\\mathbf{H})^{2}}{\\operatorname{Det}(\\mathbf{H})}=\\frac{(\\alpha+\\beta)^{2}}{\\alpha \\beta}=\\frac{(r \\beta+\\beta)^{2}}{r \\beta^{2}}=\\frac{(r+1)^{2}}{r} \\end{equation} $$\n$\\frac{(r + 1)^2}{r}$ ÁöÑÂÄºÂú®‰∏§‰∏™ÁâπÂæÅÂÄºÁõ∏Á≠âÊó∂ÊúÄÂ∞èÔºåÂπ∂‰∏îÈöèÁùÄ $r$ ÁöÑÂ¢ûÂ§ßËÄåÂ¢ûÂ§ß„ÄÇÂõ†Ê≠§Ôºå‰∏∫‰∫ÜÊ£ÄÊü•‰∏ªÊõ≤ÁéáÁöÑÊØîÂÄºÊòØÂê¶‰Ωé‰∫éÊüê‰∏™ÈòàÂÄº $r$ÔºåÂè™ÈúÄË¶ÅÂà§Êñ≠Ôºö\n$$ \\begin{equation} \\frac{\\operatorname{Tr}(\\mathbf{H})^{2}}{\\operatorname{Det}(\\mathbf{H})}\u0026lt;\\frac{(r+1)^{2}}{r} \\end{equation} $$\nËøôÊ†∑ËÉΩÂ§üÊòæËëóÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇÂêåÊó∂Êàë‰ª¨ÂèñÁªèÈ™åÂÄº $r = 10$ÔºåÂç≥ÊéíÈô§‰∏ªÊõ≤Áéá‰πãÊØîÂ§ß‰∫é 10 ÁöÑÂÄôÈÄâÁÇπ„ÄÇ\n3. ÊñπÂêëÂàÜÈÖç ÁªèËøá‰∏äËø∞‰∏§‰∏™Ê≠•È™§ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÆåÂÖ®ÊâæÂá∫‰∏ÄÂπÖÂõæÂÉè‰∏≠ÁöÑÁâπÂæÅÁÇπÔºå‰∏îÂÆÉ‰ª¨ÂØπ‰∫éÂ∞∫Â∫¶ÂÖ∑Êúâ‰∏çÂèòÊÄß„ÄÇËÄåÊ†πÊçÆÂ±ÄÈÉ®ÂõæÂÉèÂ±ûÊÄß‰∏∫ÊØè‰∏™ÂÖ≥ÈîÆÁÇπÊåáÂÆöÊüê‰∏™ÊñπÂêëÔºåÂàôÂÖ≥ÈîÆÁÇπÊèèËø∞Á¨¶ÂèØ‰ª•ÈÄöËøáËØ•ÊñπÂêëÊù•Ë°®Á§∫Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊóãËΩ¨‰∏çÂèòÊÄß„ÄÇÊàë‰ª¨Ê†πÊçÆÂÖ≥ÈîÆÁÇπÁöÑÂ∞∫Â∫¶ÈÄâÊã©‰∏é‰πãÊúÄÊé•ËøëÁöÑÈ´òÊñØÂπ≥ÊªëÂõæÂÉè $L$Ôºå‰ª•‰ΩøÂæóÊâÄÊúâËÆ°ÁÆóÊª°Ë∂≥‰∫ÜÂ∞∫Â∫¶‰∏çÂèòÊÄß„ÄÇÂØπËØ•Â∞∫Â∫¶‰∏ãÁöÑÊØè‰∏Ä‰∏™ÂõæÂÉèÈááÊ†∑ÁÇπ $L(x, y)$ÔºåÊàë‰ª¨Ê†πÊçÆÂÉèÁ¥†ÂÄºÂ∑ÆÂàÜÊù•ËÆ°ÁÆóÂÖ∂Ê¢ØÂ∫¶ÂπÖÂÄº $m(x, y)$ ÂíåÊñπÂêë $\\theta(x, y)$Ôºö\n$$ \\begin{equation} \\begin{array}{c} {m(x, y)=\\sqrt{[(L(x+1, y)-L(x-1, y)]^{2}+[(L(x, y+1)-L(x, y-1)]^{2}}} \\ {\\theta(x, y)=\\tan ^{-1}[(L(x, y+1)-L(x, y-1)] /[(L(x+1, y)-L(x-1, y)]} \\end{array} \\end{equation} $$\nSIFT Ê†πÊçÆÂÖ≥ÈîÆÁÇπÈÇªÂüüÂÜÖÊ†∑Êú¨ÁÇπÁöÑÊ¢ØÂ∫¶ÊñπÂêëÊù•ÁîüÊàêÊñπÂêëÁõ¥ÊñπÂõæ„ÄÇËØ•Áõ¥ÊñπÂõæ‰∏ÄÂÖ±Êúâ 36 Êü± (bin)Ôºå‰∏ÄÊü± $10^{\\circ}$ÔºåË¶ÜÁõñÊï¥‰∏™ $0^{\\circ} \\sim 360^{\\circ}$ ÁöÑËåÉÂõ¥„ÄÇÊ∑ªÂä†Âà∞Áõ¥ÊñπÂõæ‰∏≠ÁöÑÊØè‰∏™Ê†∑Êú¨ÁÇπÊ¢ØÂ∫¶ÊñπÂêëÈÉΩ‰ºöÊ†πÊçÆÂÖ∂Ê¢ØÂ∫¶ÂπÖÂÄº‰ª•ÂèäÂúÜÂΩ¢È´òÊñØÂä†ÊùÉÁ™óÂè£ (ÂÖ∂ $\\sigma$ ‰∏∫ÂÖ≥ÈîÆÁÇπÂ∞∫Â∫¶ÁöÑ 1.5 ÂÄç) ËøõË°åÂä†ÊùÉ„ÄÇ\nÊñπÂêëÁõ¥ÊñπÂõæÁöÑÂ≥∞ÂÄºÂØπÂ∫î‰∫éÂÖ≥ÈîÆÁÇπÂ±ÄÈÉ®Ê¢ØÂ∫¶ÁöÑ‰∏ªÊñπÂêë„ÄÇÂêåÊó∂ÔºåÂ¶ÇÊûúÁõ¥ÊñπÂõæ‰∏≠ÁöÑÊüê‰∏ÄÊü±ÁöÑÂ≥∞ÂÄºÈ´ò‰∫éÂÖ∂ÂâçÂêé‰∏§Êü±Ôºå‰∏îÂ§ß‰∫éÂ§ß‰∫é‰∏ªÂ≥∞ÂÄºÁöÑ 80%ÔºåÂàôÊàë‰ª¨Âú®ËØ•‰ΩçÁΩÆÂ§Ñ‰πüÂàõÂª∫ÂÖ∑ÊúâËØ•Êü±ÊâÄ‰ª£Ë°®ÁöÑÊñπÂêë (ÂèØËßÜ‰∏∫ËæÖÊñπÂêë) ÁöÑÂÖ≥ÈîÆÁÇπ„ÄÇÂõ†Ê≠§ÔºåÂ¶ÇÊûú‰∏Ä‰∏™ÊñπÂêëÁõ¥ÊñπÂõæÊúâÂæàÂ§öÂπÖÂÄºÁõ∏ËøëÁöÑÂ≥∞ÂÄºÔºåÈÇ£‰πàÂú®ÂÖ∂Áõ∏ÂêåÂ∞∫Â∫¶Âíå‰ΩçÁΩÆÂ§Ñ‰ºöÊúâÂæàÂ§öÂÖ≥ÈîÆÁÇπÔºå‰ΩÜÂÆÉ‰ª¨ÁöÑÊñπÂêëÊúâÊâÄ‰∏çÂêå„ÄÇÊ†πÊçÆ Lowe ÁöÑÁªìËÆ∫ÔºåÂ§ßÊ¶ÇÂè™Êúâ 15% ÁöÑÁÇπË¢´ÂàÜÈÖç‰∫ÜÂ§ö‰∏™ÊñπÂêëÔºå‰ΩÜËøô‰∫õÊñπÂêëËÉΩÂ§üÊòæËëóÊèêÈ´òÂåπÈÖçÁöÑÁ®≥ÂÆöÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊØè‰∏™Áõ¥ÊñπÂõæÂ≥∞ÂÄºÂØπÂ∫îÊñπÂêëÊòØÈÄöËøáÂØπÂÖ∂ÊúÄÊé•ËøëÁöÑ‰∏â‰∏™Êü±ËøõË°åÊäõÁâ©Á∫øÊãüÂêà„ÄÅÁÑ∂ÂêéÂÜçÊèíÂÄºÂæóÂà∞ÁöÑ„ÄÇ\n4. ÂÖ≥ÈîÆÁÇπÊèèËø∞Á¨¶ ‰πãÂâçÁöÑÊ≠•È™§Â∑≤Áªè‰∏∫ÊØè‰∏™ÂÖ≥ÈîÆÁÇπÂàÜÈÖç‰∫ÜÂõæÂÉè‰ΩçÁΩÆ„ÄÅÂ∞∫Â∫¶ÂíåÊñπÂêë„ÄÇÂú®ÂõæÂÉèÂ±ÄÈÉ®Âå∫ÂüüÂÜÖÔºåÂú®ÂõæÂÉèÂ±ÄÈÉ®Âå∫ÂüüÂÜÖÔºåËøô‰∫õÂèÇÊï∞ÂèØ‰ª•ÈáçÂ§çÂú∞Áî®‰ª•ÊèèËø∞Â±ÄÈÉ®‰∫åÁª¥ÂùêÊ†áÁ≥ªÁªüÔºåÂõ†‰∏∫Ëøô‰∫õÂèÇÊï∞ÂÖ∑Êúâ‰∏çÂèòÊÄß„ÄÇÊúÄÂêé‰∏ÄÊ≠•ÂàôÊòØËÆ°ÁÆóÂ±ÄÈÉ®ÂõæÂÉèÂå∫ÂüüÁöÑÊèèËø∞Á¨¶ (local descriptor)ÔºåËØ•ÊèèËø∞Á¨¶ÂÖ∑ÊúâÈ´òÂ∫¶ÁöÑÁã¨ÁâπÊÄßÔºåÂêåÊó∂ÂØπ‰∫éÂÖâÁÖßÊàñ 3D ËßÜÁÇπÁöÑÂèòÂåñÂÖ∑ÊúâÂæàÈ´òÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\nÂõæ 3 Â±ïÁé∞‰∫ÜÊèèËø∞Á¨¶ÁöÑËÆ°ÁÆóÊñπÊ≥ï„ÄÇÈ¶ñÂÖàÔºåÊ†πÊçÆÂÖ≥ÈîÆÁÇπÁöÑÂ∞∫Â∫¶ÈÄâÊã©Áõ∏ÂêåÊ®°Á≥äÁ®ãÂ∫¶ÁöÑÈ´òÊñØÈáëÂ≠óÂ°îÂΩ±ÂÉèÔºåÂØπÂÖ≥ÈîÆÁÇπÈÇªÂüüÂÜÖÂÉèÁ¥†ËøõË°åÈááÊ†∑‰ª•Ê±ÇÂæóÂÖ∂ÂõæÂÉèÊ¢ØÂ∫¶ÂíåÊñπÂêë„ÄÇ‰∏∫‰∫Ü‰øùËØÅÁâπÂæÅÁü¢ÈáèÂÖ∑ÊúâÊóãËΩ¨‰∏çÂèòÊÄßÔºå‰ª•ÂÖ≥ÈîÆÁÇπ‰∏∫‰∏≠ÂøÉÔºåÂú®ÂÖ∂ÈÇªÂüüÂÜÖÂ∞ÜÊèèËø∞Á¨¶ÁöÑÂùêÊ†áËΩ¥ÂíåÊ¢ØÂ∫¶ÊñπÂêëÊóãËΩ¨Ëá≥ÂÖ≥ÈîÆÁÇπÁöÑ‰∏ªÊñπÂêë„ÄÇÂõæ 3 Â∑¶‰æßÂõæÂÉè‰∏≠ÁöÑÊØè‰∏™Â∞èÁÆ≠Â§¥‰ª£Ë°®ËØ•ÈááÊ†∑ÁÇπÁöÑÊ¢ØÂ∫¶ÊñπÂêëÂíåÂ§ßÂ∞è„ÄÇ‰ΩøÁî®È´òÊñØÂä†ÊùÉÂáΩÊï∞ ($\\sigma$ Á≠â‰∫éÊèèËø∞Á¨¶Á™óÂè£ÂÆΩÂ∫¶ 1/2) Êù•‰∏∫ÊØè‰∏™ÈááÊ†∑ÁÇπÁöÑÊ¢ØÂ∫¶ÂπÖÂÄºÂàÜÈÖçÊùÉÈáçÔºåÂõæ‰∏≠ÂúÜÂúà‰ª£Ë°®Á™óÂè£ËåÉÂõ¥„ÄÇ\nÁÑ∂ÂêéÂú®ÂÖ≥ÈîÆÁÇπ $4\\times 4$ ÁöÑÈÇªÂüüËåÉÂõ¥ÂÜÖÂàõÂª∫ÊñπÂêëÁõ¥ÊñπÂõæ„ÄÇÂÖ≥ÈîÆÁÇπÊèèËø∞Á¨¶Â¶ÇÂõæ 3 ‰∏≠Âè≥‰æßÂõæÂÉèÊâÄÁ§∫„ÄÇÊØè‰∏™Áõ¥ÊñπÂõæÊúâÂÖ´‰∏™ÊñπÂêëÔºåÁÆ≠Â§¥ÈïøÂ∫¶ÂØπÂ∫î‰∏éËØ•Áõ¥ÊñπÂõæÂπÖÂÄºÁöÑÂ§ßÂ∞è„ÄÇËØ•Âõæ‰∏≠ÊòæÁ§∫ÁöÑÊòØ $2 \\times 2$ ÁöÑÊñπÂêëÁõ¥ÊñπÂõæÈòµÂàóÔºåÊ†πÊçÆ Lowe ÁöÑËÆ∫ÊñáÁªìÊûúÔºå‰ΩøÁî® $4 \\times 4$ ÁöÑÊñπÂêëÁõ¥ÊñπÂõæÈòµÂàóÔºåÊØè‰∏™Áõ¥ÊñπÂõæÊúâÂÖ´‰∏™ÊñπÂêëÔºåÂèØ‰ª•ÊèêÈ´òÂåπÈÖçÁöÑÁ®≥ÂÅ•ÊÄß„ÄÇËøôÊ†∑ÂØπ‰∫éÊØè‰∏™ÂÖ≥ÈîÆÁÇπÂ∞±ÂèØ‰ª•‰∫ßÁîü $4 \\times 4 \\times 8 = 128$ Áª¥ÁöÑÁâπÂæÅÂêëÈáè„ÄÇ\nÊ≠§Êó∂ÁöÑÁâπÂæÅÂêëÈáèÂ∑≤ÁªèÊ∂àÂéª‰∫ÜÂ∞∫Â∫¶ÂèòÂåñ„ÄÅÊóãËΩ¨Á≠âÂá†‰ΩïÂèòÂΩ¢Âõ†Á¥†ÁöÑÂΩ±Âìç„ÄÇÊúÄÂêéËøòÈúÄÂØπÁâπÂæÅÂêëÈáèËøõË°å‰∏ÄÂÆöÁöÑ‰øÆÊ≠£Ôºå‰ª•Ëøõ‰∏ÄÊ≠•Èôç‰ΩéÁÖßÊòéÂèòÂåñÁöÑÂΩ±Âìç„ÄÇÂÖàÂ∞ÜÁâπÂæÅÂêëÈáèÁöÑÂΩí‰∏ÄÂåñ‰∏∫Âçï‰ΩçÈïøÂ∫¶„ÄÇËøôÊ†∑ÂèØ‰ª•‰ΩøÂæóÊèèËø∞Á¨¶‰∏çÊî∂ÂÖâÁÖß‰ªøÂ∞ÑÂèòÊç¢ÁöÑÂΩ±Âìç„ÄÇËÄåÂØπ‰∫éÈùûÁ∫øÊÄßÂÖâÁÖßÊù°‰ª∂ÁöÑÂèòÂåñÔºåSIFT ÈÄöËøáÂØπÂçï‰ΩçÁâπÂæÅÂêëÈáè‰∏≠ÁöÑÂÄºËøõË°åÈòàÂÄºÂåñÂ§ÑÁêÜÔºåÊòØÊØè‰∏™ÂÄº‰∏çÂ§ß‰∫é 0.2 (ËØ•ÂÄºÈÄöËøáÂÆûÈ™åÈ™åËØÅÂæóÂá∫)ÔºåÁÑ∂ÂêéÂÜçÈáçÊñ∞ÂΩí‰∏ÄÂåñ‰∏∫Âçï‰ΩçÂêëÈáè„ÄÇÊúÄÁªàÂæóÂà∞ÁöÑËøô‰∏™ 128 Áª¥ÁöÑÂêëÈáèÂç≥‰∏∫ SIFT ÁâπÂæÅÂêëÈáè„ÄÇ\n5. SIFT ÂåπÈÖçÊñπÊ≥ï SIFT ‰∏≠ÁöÑÂ±ÄÈÉ®ÁâπÂæÅÊèèËø∞ÁÆóÂ≠êÂØπ‰∫éÊóãËΩ¨„ÄÅÂ∞∫Â∫¶Áº©ÊîæÂíå‰∫ÆÂ∫¶ÂèòÂåñ‰øùÊåÅ‰∏çÂèòÔºå‰∏îÂØπ‰∫é 3D ËßÜËßíÂèòÂåñ„ÄÅ‰ªøÂ∞ÑÂèòÊç¢„ÄÅ Âô™Â£∞Á≠â‰πüÂÖ∑ÊúâÂæàÈ´òÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂæóÂà∞‰∏§ÂâØÁõÆÊ†áÂΩ±ÂÉèÁöÑ SIFT ÁâπÂæÅÂêëÈáè‰πãÂêéÔºåÊàë‰ª¨ÈááÁî®ÂÖ≥ÈîÆÁÇπÁâπÂæÅÂêëÈáèÁöÑÊ¨ßÊ∞èË∑ùÁ¶ª‰Ωú‰∏∫‰∏§ÂπÖÂΩ±ÂÉè‰∏≠ÂÖ≥ÈîÆÁÇπÁöÑÁõ∏‰ººÊÄßÂà§ÂÆöÂ∫¶Èáè„ÄÇÂú®Â∑¶ÂõæÂÉè‰∏≠ÂèñÂá∫Êüê‰∏™ÂÖ≥ÈîÆÁÇπÔºåÂπ∂ÈÄöËøáÈÅçÂéÜÊâæÂá∫ÂÖ∂‰∏éÂè≥ÂΩ±ÂÉè‰∏≠Ê¨ßÊ∞èË∑ùÁ¶ªÊúÄÊé•ËøëÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÁÇπ„ÄÇÂ¶ÇÊûúÊúÄÈÇªËøëÂÖ≥ÈîÆÁÇπ‰∏éÁ¨¨‰∫åÈÇªËøëÂÖ≥ÈîÆÁÇπË∑ùÁ¶ªË∑ùÁ¶ª‰πãÊØî‰Ωé‰∫éÊüê‰∏™ÈòàÂÄº (ÁªèÈ™åÂÄº‰∏∫ 0.8)ÔºåÂàôÊé•ÂèóËøô‰∏ÄÂØπÂåπÈÖçÁÇπ„ÄÇ\nÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØ, ÈÄöËøáË∞ÉÊï¥ÂåπÈÖçËøáÁ®ã‰∏≠ÁöÑÈòàÂÄºÔºåÊàë‰ª¨ÂèØ‰ª•ÂΩ±ÂìçÂà∞ÂåπÈÖçÁªìÊûúÁöÑÊ≠£Á°ÆÁéá‰∏éÂåπÈÖçÁÇπÁöÑÊï∞Èáè„ÄÇÊ≠§Â§ñÔºåSIFT ÁÆóÂ≠êÂØπÂæàÂ∞èÁöÑÂΩ±ÂÉèÊàñÂ∞ëÊï∞Âá†‰∏™Áâ©‰Ωì‰πüËÉΩ‰∫ßÁîüÂ§ßÈáèÁöÑÁâπÂæÅÁÇπÔºåËÄå SIFT ÂåπÈÖçËøáÁ®ã‰∏≠ÈááÁî®‰∫ÜÈÄêÂÖ≥ÈîÆÁÇπÈÅçÂéÜÁöÑÊñπÊ≥ï, ËøôÂú®ÂØπÂ§ßÂ∞∫ÂØ∏ÂΩ±ÂÉèÂ§ÑÁêÜÊó∂ÊúâÁùÄÈöæ‰ª•ÊÉ≥Ë±°ÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ\n ÂèÇËÄÉÔºö\n$1$. opencv/opencv_contrib/sift.cpp - github\n$2$. D. G. Lowe, Distinctive Image Features from Scale-Invariant Keypoints, International Journal of Computer Vision, vol. 60, no. 2, pp. 91‚Äì110, 2004.\n$3$. opencv 2.4.9 Ê∫êÁ†ÅÂàÜÊûê\n$4$. „ÄêOpenCV„ÄëSIFTÂéüÁêÜ‰∏éÊ∫êÁ†ÅÂàÜÊûê - Â∞èÈ≠èÁöÑ‰øÆË°åË∑Ø - CSDNÂçöÂÆ¢\n$5$. SIFTÁÆóÊ≥ïËØ¶Ëß£ - zddhubÁöÑ‰∏ìÊ†è - CSDNÂçöÂÆ¢\n$6$. SIFT: Theory and Practice - AI Shack\n$7$. Âº†ÂâëÊ∏ÖÔºåÊΩòÂä±ÔºåÁéãÊ†ëÊ†πÔºå„ÄäÊëÑÂΩ±ÊµãÈáèÂ≠¶(Á¨¨‰∫åÁâà)„Äã\n ","permalink":"https://fang-lansheng.github.io/posts/2019-05-10-opencv-4/","summary":"Êú¨ÊñáÂèäÊé•‰∏ãÊù•Âá†ÁØáÂêåÁ≥ªÂàóÊñáÁ´†ÊòØÂ≠¶‰π† SIFT ÁÆóÊ≥ïÂíå OpenCV SIFT Ê∫êÁ†ÅÊó∂ÁöÑÂ≠¶‰π†Á¨îËÆ∞ÔºåÊï¥ÂêàËá™ÂèÇËÄÉÊñáÁåÆÂèäÂçöÂÆ¢„ÄÇÂº∫ÁÉàÂª∫ËÆÆÈòÖËØª ËÆ∫ÊñáÂéüÊñá„ÄÅGitHub‰∏äÁöÑÊ∫êÁ†Å ‰ª•Âèä @ËµµÊò•Ê±ü ÁöÑ opencv 2.4.9 Ê∫êÁ†ÅÂàÜÊûê„ÄÇ\n Lowe Âú® 2004 Âπ¥ÊèêÂá∫‰∫ÜÂ∞∫Â∫¶‰∏çÂèòÁâπÂæÅÂèòÊç¢ (Scale Invariant Feature Transform, SIFT) ÁÆóÊ≥ï„ÄÇ SIFT ‰∏ªË¶ÅÁî±ÂÖ≥ÈîÆÁÇπÊé¢ÊµãÂô® (detector) ÂíåÊèèËø∞Á¨¶ (descriptor) ÁªÑÊàêÔºåÂÆÉÁöÑÂÆûÁé∞ÂàÜ‰∏∫‰ª•‰∏ãÂõõÊ≠•:\n Â∞∫Â∫¶Á©∫Èó¥ÊûÅÂÄºÊé¢Êµã (scale-space extrema detection)ÔºöÈÄöËøáÈ´òÊñØÂ∑ÆÂàÜÂáΩÊï∞ÊêúÁ¥¢ÊâÄÊúâÂ∞∫Â∫¶ÂíåÂõæÂÉè‰ΩçÁΩÆÔºå‰ª•ËØÜÂà´ÂØπ‰∫éÂ∞∫Â∫¶ÂíåÊñπÂêë‰∏çÂèòÁöÑÊΩúÂú®ÂÖ¥Ë∂£ÁÇπ„ÄÇ ÂÖ≥ÈîÆÁÇπÁ≤æÁ°ÆÂÆö‰Ωç (keypoint localization)ÔºöÁ≤æÁ°ÆÁ°ÆÂÆöÊØè‰∏™ÂÄôÈÄâÁÇπÁöÑÂ∞∫Â∫¶‰∏é‰∫öÂÉèÁ¥†Á∫ß‰ΩçÁΩÆÔºåÊ†πÊçÆÂÖ∂Á®≥ÂÆöÊÄßÈòàÂÄºÈÄâÊã©ÂÖ≥ÈîÆÁÇπ„ÄÇ ÊñπÂêëÂàÜÈÖç (orientation assignment)ÔºöÂü∫‰∫éÂõæÂÉèÁöÑÂ±ÄÈÉ®Ê¢ØÂ∫¶ÊñπÂêëÔºå‰∏∫ÊØè‰∏™ÁâπÊÄßÁÇπÂàÜÈÖç‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÊñπÂêëËßíÂ∫¶„ÄÇÊâÄÊúâÂêéÁª≠ÁöÑÊìç‰ΩúÈÉΩÊòØÁõ∏ÂØπ‰∫éÊâÄÁ°ÆÂÆö‰∏ãÊù•ÁöÑÁâπÂæÅÁÇπÁöÑËßíÂ∫¶„ÄÅÂ∞∫Â∫¶Âíå‰ΩçÁΩÆÁöÑÂü∫Á°Ä‰∏äËøõË°åÁöÑÔºåÂõ†Ê≠§ÁâπÂæÅÁÇπÂÖ∑ÊúâËßíÂ∫¶„ÄÅÂ∞∫Â∫¶Âíå‰ΩçÁΩÆÁöÑ‰∏çÂèòÊÄß„ÄÇ ÂÖ≥ÈîÆÁÇπÊèèËø∞Á¨¶ (keypoint descriptor)ÔºöÂú®ÊâÄÈÄâÂÆöÁöÑÂ∞∫Â∫¶Á©∫Èó¥ÂÜÖÔºåÊµãÈáèÁâπÂæÅÁÇπÈÇªÂüüÂå∫ÂüüÁöÑÂ±ÄÈÉ®ÂõæÂÉèÊ¢ØÂ∫¶ÔºåÂ∞ÜËøô‰∫õÊ¢ØÂ∫¶ËΩ¨Êç¢Êàê‰∏ÄÁßçÂÖÅËÆ∏Â±ÄÈÉ®ËæÉÂ§ßÁ®ãÂ∫¶ÁöÑÂΩ¢Áä∂ÂèòÂΩ¢Âíå‰∫ÆÂ∫¶ÂèòÂåñÁöÑÊèèËø∞Á¨¶ÂΩ¢Âºè„ÄÇ  ‰∏ãÈù¢Â∞ÜÂØπÂÖ∂ËøõË°åËØ¶ÁªÜÈòêÈáäÔºö\n1. Â∞∫Â∫¶Á©∫Èó¥ÊûÅÂÄºÊé¢Êµã ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÁöÑÁ¨¨‰∏ÄÈò∂ÊÆµÊòØËØÜÂà´ÂèØ‰ª•Âú®Âêå‰∏ÄÂØπË±°ÁöÑ‰∏çÂêåËßÜÂõæ‰∏ãÈáçÂ§çÂàÜÈÖçÁöÑ‰ΩçÁΩÆÂíåÂ∞∫Â∫¶„ÄÇÈÄöËøáÂú®Á¥†ÊúâÂèØËÉΩÁöÑÂ∞∫Â∫¶ËøõË°åÊêúÁ¥¢ÔºåÂèØ‰ª•Ê£ÄÊµãÂá∫ÂØπÂõæÂÉèÂ∞∫Â∫¶‰∏çÂèòÁöÑÁ®≥ÂÆöÁâπÂæÅ„ÄÇËøô‰∏ÄËøáÁ®ã‰∏≠‰ΩøÁî®Âà∞ÁöÑÊòØË¢´Áß∞‰∏∫Â∞∫Â∫¶Á©∫Èó¥ (scale space) ÁöÑÂ∞∫Â∫¶ËøûÁª≠ÂáΩÊï∞^[1]„ÄÇ\nKoenderink (The structure of images, 1984) Âíå Lindeberg (Detecting salient blob-like image structures and their scales with a scale-space primal sketch: A method for focus-of-attention, 1993) Â∑≤ÁªèËØÅÊòéÔºåÂîØ‰∏ÄÂèØËÉΩÁöÑÂ∞∫Â∫¶Á©∫Èó¥Ê†∏ÊòØÈ´òÊñØÂáΩÊï∞„ÄÇÂõ†Ê≠§ÔºåÂõæÂÉèÁöÑÂ∞∫Â∫¶Á©∫Èó¥Ë¢´ÂÆö‰πâ‰∏∫ÂáΩÊï∞ $L(x, y, \\sigma)$ÔºåÂÆÉÊòØÁî±ÂèØÂèòÂ∞∫Â∫¶È´òÊñØÂáΩÊï∞ $G(x, y, \\sigma)$ ‰∏éËæìÂÖ•ÂõæÂÉè $I(x,y)$ Âç∑ÁßØÂæóÂà∞ÁöÑÔºö","title":"OpenCV‚Äî‚ÄîSIFT ÁÆóÊ≥ïÂèäÊ∫êÁ†ÅÂàÜÊûêÔºà1ÔºâÔºöÂéüÁêÜÁÆÄ‰ªã"},{"content":" Êú¨Êñá‰∏ªË¶ÅÂèÇËÄÉËá™ OpenCV ÂÆòÊñπÊïôÁ®ã OpenCV: Camera calibration With OpenCVÔºåÂ§ßÈÉ®ÂàÜÂÜÖÂÆπÊòØÂØπÂéüÊñáÁöÑÁõ¥Êé•ÁøªËØë„ÄÇ\n 1. ÂâçË®Ä Âà©Áî®ÊâãÊú∫Áõ∏Êú∫ÂØπÊ£ãÁõòÂõæÊãçÊëÑÂæóÂà∞ÁöÑÂõæÂÉèÂ¶Ç‰∏ãÔºàÂ∑¶Ëæπ‰∏∫ÂéüÂõæÔºåÂè≥Ëæπ‰∏∫‰∫åÂÄºÂåñÂ§ÑÁêÜÂêéÁöÑÂõæÂÉèÔºâÔºö\nÂ¶Ç‰∏ãÊâÄÁ§∫ÔºåÂ∞ÜÂÖ∂‰∏éÂéüÂßãÊ£ãÁõòÂõæÂØπÊØîÔºàÂ∑¶‰æß‰∏∫ÂéüÂßãÂõæÂÉèÔºåÂè≥‰æß‰∏∫ÊâãÊú∫Áõ∏Êú∫ÊãçÊëÑÂΩ±ÂÉèÔºâ„ÄÇÂèØ‰ª•ÊòéÊòæÂú∞ÁúãÂá∫Âá∫Áé∞‰∫Ü‰∏ÄÂÆöÁ®ãÂ∫¶ÁöÑÁï∏Âèò„ÄÇ\n2. Áõ∏Êú∫Ê£ÄÊ†°ÂéüÁêÜ ÁõÆÂâçÂ∏∏Áî®ÁöÑÁõ∏Êú∫Ê£ÄÊ†°ÊñπÊ≥ïÊòØÈôÑÂä†ÂèÇÊï∞Ê≥ïÔºåÂÖ∂ÂÖ≥ÈîÆÂú®‰∫éÈÄöËøá‰∏ÄÁªÑÈ¢ùÂ§ñÂèÇÊï∞ÊãüÂêàÈïúÂ§¥Áï∏ÂèòÔºå‰∏ªË¶ÅÊòØÂæÑÂêëÁï∏ÂèòÂíåÂàáÂêëÁï∏ÂèòÔºö\n$$ \\begin{align} dx \u0026amp;= x(k_1r^2+k_2r^4+k_3r^6) + p_1(r^2+2x^2) + p_2xy \\ dy \u0026amp;= \\underbrace{y(k_1r^2+k_2r^4+k_3r^6)}\\text{radial distortion} + \\underbrace{p_2(r^2+2y^2) + p_1xy}\\text{tangential distortion} \\end{align} $$\nÂºè‰∏≠Ôºå\n$$ x=x\u0026rsquo;-x_0, \\quad y=y\u0026rsquo;-y_0 \\ r^2 = x^2 + y^2 $$\nÂÖ∂‰∏≠ $(x\u0026rsquo;,y\u0026rsquo;)$ ‰∏∫ÂÉèÁâáÂùêÊ†áÁöÑÂéüÂßãÈáèÊµãÂÄºÔºå$k_i,(i=1,2,3)$ ‰∏∫ÂæÑÂêëÁï∏ÂèòÂèÇÊï∞Ôºå$p_j, (j=1,2)$ ‰∏∫ÂàáÂêëÁï∏ÂèòÂèÇÊï∞„ÄÇÊâÄ‰ª• 5 ‰∏™Áï∏ÂèòÂèÇÊï∞ÊûÑÊàêÁöÑÁü©Èòµ‰∏∫Ôºö\n$$ distortion_coeffients = (\\begin{matrix}k_1\u0026amp;k_2\u0026amp;p_1\u0026amp;p_2\u0026amp;k_3\\end{matrix}) $$\nÂØπ‰∫éÂΩ±ÂÉèÂùêÊ†áÂíåÁâ©Á©∫Èó¥ÂùêÊ†áÁöÑËΩ¨Êç¢ÂÖ≥Á≥ª‰∏∫Ôºö\n$$ \\begin{bmatrix}x\\y\\f\\end{bmatrix}= K \\begin{bmatrix}X\\Y\\Z\\end{bmatrix}= \\begin{bmatrix}f_x\u0026amp;0\u0026amp;c_x\\0\u0026amp;f_y\u0026amp;c_y\\0\u0026amp;0\u0026amp;1\\end{bmatrix} \\begin{bmatrix}X\\Y\\Z\\end{bmatrix} $$\nÂÖ∂‰∏≠Ôºå$(x, y, w)$ ÊòØÂÉèÁÇπÂùêÊ†áÂú®ÈΩêÊ¨°ÂùêÊ†áÁ≥ª‰∏ãÁöÑË°®Á§∫Ôºà$w=Z$ÔºâÔºå$(X,Y,Z)$ ‰∏∫ËØ•ÁÇπÂØπÂ∫îÁöÑÁâ©ÊñπÂùêÊ†á„ÄÇËØ•ÂºèÊúâ 4 ‰∏™Êú™Áü•Êï∞Ôºå$f_x$Ôºå$f_y$ ‰∏∫Áõ∏Êú∫ÁÑ¶Ë∑ù„ÄÇ$c_x$Ôºå$c_y$ ‰∏∫ÂÉè‰∏ªÁÇπÂùêÊ†á„ÄÇÂåÖÂê´Ëøô 4 ‰∏™ÂèÇÊï∞ÁöÑÁü©Èòµ $K$ Ë¢´Áß∞‰∏∫Áõ∏Êú∫Áü©Èòµ„ÄÇ\nÁõ∏Êú∫Ê£ÄÊ†°ÁöÑËøáÁ®ãÂç≥ÊòØÊ±ÇËß£Áõ∏Êú∫Áü©ÈòµÂíåÁï∏ÂèòÁü©ÈòµÁöÑËøáÁ®ã„ÄÇËøô‰∫õÂèÇÊï∞ÂèØ‰ª•ÈÄöËøá‰∏Ä‰∫õÂü∫Êú¨ÁöÑÂá†‰ΩïÊñπÁ®ãËøõË°åËß£ÁÆóÔºåÂÖ∂‰∏≠‰ΩøÁî®ÁöÑÂÖ¨ÂºèÂèñÂÜ≥‰∫éÈÄâÊã©ÁöÑÊ†°ÂáÜÂØπË±°ÔºåÊú¨ÊñáÊâÄÂèÇËÄÉÁöÑ OpenCV Áõ∏Êú∫Ê£ÄÊ†°ÊñπÊ≥ïÊîØÊåÅ‰∏âÁßçÁ±ªÂûãÁöÑÊ†°ÂáÜÂØπË±°Ôºö\n ÁªèÂÖ∏ÁöÑÈªëÁôΩÊ£ãÁõòÂõæÊ°à ÂØπÁß∞ÁöÑÂúÜÂúàÂõæÊ°à ‰∏çÂØπÁß∞ÁöÑÂúÜÂúàÂõæÊ°à  ‰∏∫‰∫ÜÊèêÈ´òÁ≤æÂ∫¶„ÄÅÂøΩÁï•Âô™Â£∞ÁöÑÂΩ±ÂÉèÔºåÊàë‰ª¨ÂæÄÂæÄÈúÄË¶ÅÊãçÊëÑÂ§öÂº†ÁÖßÁâá‰ª•‰æøËøõË°åÊï¥‰ΩìËß£ÁÆó„ÄÇËÄåÊú¨Âõû‰ΩøÁî®ÁöÑÈªëÁôΩÊ£ãÁõòÂõæÊ°àÁêÜËÆ∫‰∏äÊâÄÈúÄÁöÑÁõ∏ÁâáÊï∞ÈáèÊúÄÂ§öÔºå‰∏∫Ê≠§ÔºåÊàë‰∏ÄÂÖ±Âú®‰∏çÂêå‰ΩçÁΩÆÊãçÊëÑ‰∫Ü 10 Âº†Áõ∏ÁâáÔºàËßÅ‰∏ãÂõæÔºâ„ÄÇ\n3. Ê£ÄÊ†°ËøáÁ®ã ÈÄöËøáÂÜçÂÆûÁé∞ OpenCV ÁöÑÁõ∏Êú∫Ê£ÄÊ†°Ê†∑‰æãÔºåÊàë‰ª¨‰∏ªË¶ÅÂÆåÊàê‰∫Ü‰ª•‰∏ãËøáÁ®ãÔºö\n Á°ÆÂÆöÁï∏ÂèòÁü©Èòµ Á°ÆÂÆöÁõ∏Êú∫Áü©Èòµ ÊâπÈáèËé∑ÂèñÂΩ±ÂÉèÊï∞ÊçÆ ‰ªé .XML/YAML Êñá‰ª∂‰∏≠ËØªÂèñÈÖçÁΩÆ ‰øùÂ≠òÁªìÊûúÂà∞ .XML/YAML ‰∏≠ ËÆ°ÁÆóÊäïÂΩ±ËØØÂ∑Æ  ÂÖ∂‰∏≠ÔºåËæìÂÖ•ÁöÑÈÖçÁΩÆÊñá‰ª∂ input.XML ÈÉ®ÂàÜÂÜÖÂÆπÂ¶Ç‰∏ãÔºö\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;opencv_storage\u0026gt; \u0026lt;Settings\u0026gt; \u0026lt;!-- ÊØèÂº†Ê£ÄÊµãÁöÑÊ£ãÁõòÁÇπ‰∏∫ 9√ó6 ‰∏™ --\u0026gt; \u0026lt;BoardSize_Width\u0026gt; 9\u0026lt;/BoardSize_Width\u0026gt; \u0026lt;BoardSize_Height\u0026gt;6\u0026lt;/BoardSize_Height\u0026gt; \u0026lt;!-- ËæìÂÖ•ÂõæÂÉèÁöÑÁ±ªÂûãÔºåËøôÈáåÊàë‰ª¨ÈÄâÊã©‚ÄúÊ£ãÁõòÂõæ‚Äù --\u0026gt; \u0026lt;Calibrate_Pattern\u0026gt;\u0026#34;CHESSBOARD\u0026#34;\u0026lt;/Calibrate_Pattern\u0026gt; \u0026lt;!-- ËæìÂÖ•ÂõæÂÉèÔºåÁî®‰∫éÁõ∏Êú∫Ê£ÄÊ†° --\u0026gt; \u0026lt;Input\u0026gt;\u0026#34;inputImages.xml\u0026#34;\u0026lt;/Input\u0026gt; \u0026lt;!-- ËæìÂá∫ÁöÑÊó•ÂøóÊñá‰ª∂ --\u0026gt; \u0026lt;Write_outputFileName\u0026gt;\u0026#34;out_camera_data.xml\u0026#34;\u0026lt;/Write_outputFileName\u0026gt; \u0026lt;/Settings\u0026gt; \u0026lt;/opencv_storage\u0026gt; Áî®‰∫éÊ£ÄÊ†°ÁöÑËæìÂÖ•ÂõæÂÉèÔºàÂç≥‰∏äÊñá‰∏≠ÁöÑÂçÅÂº†Áõ∏ÁâáÔºâÈÄöËøá inputImages.xml Á°ÆÂÆöÔºö\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;opencv_storage\u0026gt; \u0026lt;images\u0026gt; imgs/IMG_01.jpg imgs/IMG_02.jpg imgs/IMG_03.jpg imgs/IMG_04.jpg imgs/IMG_05.jpg imgs/IMG_06.jpg imgs/IMG_07.jpg imgs/IMG_08.jpg imgs/IMG_09.jpg imgs/IMG_10.jpg \u0026lt;/images\u0026gt; \u0026lt;/opencv_storage\u0026gt; ËøêË°å‰ª£Á†ÅÔºåOpenCV ÈÄöËøá findChessboardCorners() ÂáΩÊï∞ÊâæÂà∞ÊØèÂº†Áõ∏Áâá‰∏≠ÊåáÂÆöÊï∞ÁõÆÁöÑÊ£ãÁõòÁÇπ $(9\\times6)$ÔºåÊØîÂ¶ÇÔºö\n‰∏çËøáÂèØËÉΩÊòØÁî±‰∫éÂÖâÁÖßÁ≠âÂõ†Á¥†ÂΩ±ÂìçÔºåÁ¨¨ÂÖ´Âº†Áõ∏Áâá‰∏≠ÁöÑÁÇπÂπ∂Êú™ÊàêÂäüËØÜÂà´Âá∫„ÄÇÁ®ãÂ∫èÂ∞ÜÂ∞ùËØïÂú®ÊâÄÊúâËæìÂÖ•ÂõæÂÉè‰∏≠ÊâæÂà∞Âπ∂ÁªòÂá∫Ëøô‰∫õÊ£ãÁõòÁÇπÔºö\nÊúÄÁªàÊàë‰ª¨ÈÄöËøá imwrite() ÂáΩÊï∞ÔºåËæìÂá∫Ê†°Ê≠£ÂèòÂΩ¢ÂêéÁöÑÂõæÂÉèÔºö\nÂú®ËæìÂá∫ÁöÑ out_camera_data.xml Êñá‰ª∂‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂà∞Áõ∏Êú∫Ê£ÄÊ†°ËøáÁ®ã‰∏≠ÁöÑÊâÄÊúâÂèÇÊï∞ÔºåÂåÖÊã¨Áõ∏Êú∫Áü©Èòµ„ÄÅÁï∏ÂèòÂèÇÊï∞Áü©ÈòµÂíåÊ£ÄÊµãÁÇπÁöÑÂùêÊ†áÁ≠â„ÄÇ\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;opencv_storage\u0026gt; \u0026lt;calibration_time\u0026gt;\u0026#34;Fri Apr 19 14:12:13 2019\u0026#34;\u0026lt;/calibration_time\u0026gt; \u0026lt;nr_of_frames\u0026gt;9\u0026lt;/nr_of_frames\u0026gt; \u0026lt;image_width\u0026gt;900\u0026lt;/image_width\u0026gt;\t\u0026lt;!-- ËæìÂÖ•ÂõæÂÉèÂÆΩÂ∫¶ --\u0026gt; \u0026lt;image_height\u0026gt;1200\u0026lt;/image_height\u0026gt;\t\u0026lt;!-- ËæìÂÖ•ÂõæÂÉèÈ´òÂ∫¶ --\u0026gt; \u0026lt;board_width\u0026gt;9\u0026lt;/board_width\u0026gt; \u0026lt;board_height\u0026gt;6\u0026lt;/board_height\u0026gt; \u0026lt;square_size\u0026gt;50.\u0026lt;/square_size\u0026gt; \u0026lt;fix_aspect_ratio\u0026gt;1.\u0026lt;/fix_aspect_ratio\u0026gt; \u0026lt;flags\u0026gt;6158\u0026lt;/flags\u0026gt; \u0026lt;fisheye_model\u0026gt;0\u0026lt;/fisheye_model\u0026gt; \u0026lt;camera_matrix type_id=\u0026#34;opencv-matrix\u0026#34;\u0026gt;\t\u0026lt;!-- Áõ∏Êú∫Áü©Èòµ --\u0026gt; \u0026lt;rows\u0026gt;3\u0026lt;/rows\u0026gt; \u0026lt;cols\u0026gt;3\u0026lt;/cols\u0026gt; \u0026lt;dt\u0026gt;d\u0026lt;/dt\u0026gt; \u0026lt;data\u0026gt; 1.1462822465241961e+03 0. 450. 0. 1.1462822465241961e+03 600. 0. 0.1. \u0026lt;/data\u0026gt; \u0026lt;/camera_matrix\u0026gt; \u0026lt;distortion_coefficients type_id=\u0026#34;opencv-matrix\u0026#34;\u0026gt;\t\u0026lt;!-- Áï∏ÂèòÁü©Èòµ --\u0026gt; \u0026lt;rows\u0026gt;5\u0026lt;/rows\u0026gt; \u0026lt;cols\u0026gt;1\u0026lt;/cols\u0026gt; \u0026lt;dt\u0026gt;d\u0026lt;/dt\u0026gt; \u0026lt;data\u0026gt; 5.2379099530930096e-01 -4.0406900157339871e+00 0. 0.8.8954798029406810e+00 \u0026lt;/data\u0026gt; \u0026lt;/distortion_coefficients\u0026gt; \u0026lt;avg_reprojection_error\u0026gt;6.1070913167798235e-01\u0026lt;/avg_reprojection_error\u0026gt; \u0026lt;per_view_reprojection_errors type_id=\u0026#34;opencv-matrix\u0026#34;\u0026gt; \u0026lt;rows\u0026gt;9\u0026lt;/rows\u0026gt; \u0026lt;cols\u0026gt;1\u0026lt;/cols\u0026gt; \u0026lt;dt\u0026gt;f\u0026lt;/dt\u0026gt; \u0026lt;data\u0026gt; 7.01961458e-01 6.66437447e-01 5.47557890e-01 5.58410525e-01 4.83857125e-01 5.62040746e-01 6.58297598e-01 6.49597406e-01 6.34681106e-01 \u0026lt;/data\u0026gt; \u0026lt;/per_view_reprojection_errors\u0026gt; \u0026lt;extrinsic_parameters type_id=\u0026#34;opencv-matrix\u0026#34;\u0026gt; \u0026lt;rows\u0026gt;9\u0026lt;/rows\u0026gt; \u0026lt;cols\u0026gt;6\u0026lt;/cols\u0026gt; \u0026lt;dt\u0026gt;d\u0026lt;/dt\u0026gt; \u0026lt;data\u0026gt;\u0026lt;!-- Áï• --\u0026gt;\u0026lt;/data\u0026gt; \u0026lt;/extrinsic_parameters\u0026gt; \u0026lt;image_points type_id=\u0026#34;opencv-matrix\u0026#34;\u0026gt; \u0026lt;rows\u0026gt;9\u0026lt;/rows\u0026gt; \u0026lt;cols\u0026gt;54\u0026lt;/cols\u0026gt; \u0026lt;dt\u0026gt;\u0026#34;2f\u0026#34;\u0026lt;/dt\u0026gt; \u0026lt;data\u0026gt;\u0026lt;!-- Áï• --\u0026gt;\u0026lt;/data\u0026gt; \u0026lt;/image_points\u0026gt; \u0026lt;grid_points\u0026gt;\u0026lt;!-- Áï• --\u0026gt;\u0026lt;/grid_points\u0026gt; \u0026lt;/opencv_storage\u0026gt; Áõ∏Êú∫Áü©ÈòµÔºö\n$$ \\begin{align} Camera_Matrix \u0026amp;= \\begin{bmatrix}f_x\u0026amp;0\u0026amp;c_x\\0\u0026amp;f_y\u0026amp;c_y\\0\u0026amp;0\u0026amp;1\\end{bmatrix}\\ \u0026amp;=\\begin{bmatrix}1146.282\u0026amp; 0 \u0026amp;450 \\0\u0026amp; 1146.282\u0026amp; 600\\ 0\u0026amp;0\u0026amp;1\\end{bmatrix} \\end{align} $$\nÂÖ∂‰∏≠ÔºåÁõ∏Êú∫ÁÑ¶Ë∑ùÁöÑÁ≤æÁ°ÆÂÄº‰∏∫ $f_x=f_y=1.1462822465241961e+03$Ôºå‰∏î $c_x=450$Ôºå$c_y=600$ÔºåÈâ¥‰∫éËæìÂÖ•ÂõæÂÉèÁöÑÂ∞∫ÂØ∏‰∏∫ $900\\times1200$ÔºåÂèØËßÅÁ®ãÂ∫èÁõ¥Êé•Â∞ÜÂõæÂÉè‰∏≠ÂøÉ‰Ωú‰∏∫ÂÉè‰∏ªÁÇπ„ÄÇ\nÁï∏ÂèòÁü©ÈòµÔºö\n$$ \\begin{align} distortion_coeffients \u0026amp;= \\begin{bmatrix}k_1\u0026amp;k_2\u0026amp;p_1\u0026amp;p_2\u0026amp;k_3\\end{bmatrix}\\ \u0026amp;= \\begin{bmatrix}0.52379 \u0026amp; -4.04069 \u0026amp; 0 \u0026amp; 0 \u0026amp; 8.89548\\end{bmatrix} \\end{align} $$\n ÂæÑÂêëÁï∏ÂèòÂèÇÊï∞ÁöÑÁ≤æÁ°ÆÂÄº‰∏∫Ôºö  $k_1 = 5.2379099530930096e-01$ $k_2 = -4.0406900157339871e+00$ $k_3 = 8.8954798029406810e+00$   ÂàáÂêëÁï∏ÂèòÂèÇÊï∞Ôºö $p_1 = p _ 2 = 0$  ","permalink":"https://fang-lansheng.github.io/posts/2019-04-18-opencv-3/","summary":"Êú¨Êñá‰∏ªË¶ÅÂèÇËÄÉËá™ OpenCV ÂÆòÊñπÊïôÁ®ã OpenCV: Camera calibration With OpenCVÔºåÂ§ßÈÉ®ÂàÜÂÜÖÂÆπÊòØÂØπÂéüÊñáÁöÑÁõ¥Êé•ÁøªËØë„ÄÇ\n 1. ÂâçË®Ä Âà©Áî®ÊâãÊú∫Áõ∏Êú∫ÂØπÊ£ãÁõòÂõæÊãçÊëÑÂæóÂà∞ÁöÑÂõæÂÉèÂ¶Ç‰∏ãÔºàÂ∑¶Ëæπ‰∏∫ÂéüÂõæÔºåÂè≥Ëæπ‰∏∫‰∫åÂÄºÂåñÂ§ÑÁêÜÂêéÁöÑÂõæÂÉèÔºâÔºö\nÂ¶Ç‰∏ãÊâÄÁ§∫ÔºåÂ∞ÜÂÖ∂‰∏éÂéüÂßãÊ£ãÁõòÂõæÂØπÊØîÔºàÂ∑¶‰æß‰∏∫ÂéüÂßãÂõæÂÉèÔºåÂè≥‰æß‰∏∫ÊâãÊú∫Áõ∏Êú∫ÊãçÊëÑÂΩ±ÂÉèÔºâ„ÄÇÂèØ‰ª•ÊòéÊòæÂú∞ÁúãÂá∫Âá∫Áé∞‰∫Ü‰∏ÄÂÆöÁ®ãÂ∫¶ÁöÑÁï∏Âèò„ÄÇ\n2. Áõ∏Êú∫Ê£ÄÊ†°ÂéüÁêÜ ÁõÆÂâçÂ∏∏Áî®ÁöÑÁõ∏Êú∫Ê£ÄÊ†°ÊñπÊ≥ïÊòØÈôÑÂä†ÂèÇÊï∞Ê≥ïÔºåÂÖ∂ÂÖ≥ÈîÆÂú®‰∫éÈÄöËøá‰∏ÄÁªÑÈ¢ùÂ§ñÂèÇÊï∞ÊãüÂêàÈïúÂ§¥Áï∏ÂèòÔºå‰∏ªË¶ÅÊòØÂæÑÂêëÁï∏ÂèòÂíåÂàáÂêëÁï∏ÂèòÔºö\n$$ \\begin{align} dx \u0026amp;= x(k_1r^2+k_2r^4+k_3r^6) + p_1(r^2+2x^2) + p_2xy \\ dy \u0026amp;= \\underbrace{y(k_1r^2+k_2r^4+k_3r^6)}\\text{radial distortion} + \\underbrace{p_2(r^2+2y^2) + p_1xy}\\text{tangential distortion} \\end{align} $$\nÂºè‰∏≠Ôºå\n$$ x=x\u0026rsquo;-x_0, \\quad y=y\u0026rsquo;-y_0 \\ r^2 = x^2 + y^2 $$\nÂÖ∂‰∏≠ $(x\u0026rsquo;,y\u0026rsquo;)$ ‰∏∫ÂÉèÁâáÂùêÊ†áÁöÑÂéüÂßãÈáèÊµãÂÄºÔºå$k_i,(i=1,2,3)$ ‰∏∫ÂæÑÂêëÁï∏ÂèòÂèÇÊï∞Ôºå$p_j, (j=1,2)$ ‰∏∫ÂàáÂêëÁï∏ÂèòÂèÇÊï∞„ÄÇÊâÄ‰ª• 5 ‰∏™Áï∏ÂèòÂèÇÊï∞ÊûÑÊàêÁöÑÁü©Èòµ‰∏∫Ôºö\n$$ distortion_coeffients = (\\begin{matrix}k_1\u0026amp;k_2\u0026amp;p_1\u0026amp;p_2\u0026amp;k_3\\end{matrix}) $$\nÂØπ‰∫éÂΩ±ÂÉèÂùêÊ†áÂíåÁâ©Á©∫Èó¥ÂùêÊ†áÁöÑËΩ¨Êç¢ÂÖ≥Á≥ª‰∏∫Ôºö\n$$ \\begin{bmatrix}x\\y\\f\\end{bmatrix}= K \\begin{bmatrix}X\\Y\\Z\\end{bmatrix}= \\begin{bmatrix}f_x\u0026amp;0\u0026amp;c_x\\0\u0026amp;f_y\u0026amp;c_y\\0\u0026amp;0\u0026amp;1\\end{bmatrix} \\begin{bmatrix}X\\Y\\Z\\end{bmatrix} $$","title":"OpenCV‚Äî‚ÄîÁõ∏Êú∫Ê£ÄÊ†°"},{"content":"ÁâπÂæÅÂÄºÂàÜËß£ 1. ÁâπÂæÅÂÄº‰∏éÁâπÂæÅÂêëÈáè Âú®Á∫øÊÄß‰ª£Êï∞‰∏≠ÔºåÂØπ‰∫é $n$ Èò∂ÊñπÈòµ $A$ÔºåÂ¶ÇÊûúÂ≠òÂú®Êüê‰∏™Êï∞ $\\lambda$ ÂèäÊüê‰∏™ $n$ Áª¥ÈùûÈõ∂ÂàóÂêëÈáè $v$Ôºå‰ΩøÂæó\n$$ Av = \\lambda v $$\nÂàôÁß∞ $\\lambda$ ÊòØÊñπÈòµ $A$ ÁöÑ‰∏Ä‰∏™ÁâπÂæÅÂÄºÔºå$v$ ÊòØÊñπÈòµ $A$ ÁöÑÂ±û‰∫éÁâπÂæÅÂÄº $\\lambda$ ÁöÑ‰∏Ä‰∏™ÁâπÂæÅÂêëÈáè„ÄÇ\nÂØπ‰∏äÂºèËøõË°åÂèòÊç¢Ôºö\n$$ \\begin{align} Av-\\lambda v \u0026amp;= \\overrightarrow{0} \\ Av-\\lambda I v \u0026amp;= \\overrightarrow{0} \\ (A - \\lambda I)v \u0026amp;= \\overrightarrow{0} \\ (\\lambda I - A)v \u0026amp;= \\overrightarrow{0} \\end{align} $$\nÂàôÁß∞\n $\\lambda E - A$ ‰∏∫ $A$ ÁöÑÁâπÂæÅÁü©Èòµ Ë°åÂàóÂºè $f(\\lambda) = |\\lambda E- A|$ ‰∏∫ $A$ ÁöÑÁâπÂæÅÂ§öÈ°πÂºè $|\\lambda E - A| = 0$ ÊòØ $A$ ÁöÑÁâπÂæÅÊñπÁ®ã $(\\lambda E - A)v = \\overrightarrow{0}$ ÊòØ $A$ ÂÖ≥‰∫éËØ•ÁâπÂæÅÂÄº $\\lambda$ ÁöÑÈΩêÊ¨°Á∫øÊÄßÊñπÁ®ãÁªÑ  $A$ ÁöÑ‰∏ªÂØπËßíÁ∫ø‰∏äÂÖÉÁ¥†‰πãÂíåÁß∞‰∏∫Áü©ÈòµÁöÑËøπÔºàtraceÔºâÔºåËÆ∞‰∏∫ $tr(A)$ÔºåÂç≥\n$$ tr(A) = a_{11} + a_{22} + \\cdots + a_{nn} $$\nËøπÂíåÁâπÂæÅÂÄºÊúâÁùÄÂæàÈáçË¶ÅÁöÑËÅîÁ≥ªÔºö\n$$ tr(A) = \\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n $$\nË°åÂàóÂºè‰πü‰∏éÁâπÂæÅÂÄºÊúâÂÖ≥ËÅîÔºö\n$$ |A| = \\lambda_1\\lambda_2\\cdots\\lambda_n $$\n2. ÁâπÂæÅÂÄºÂàÜËß£ Â¶ÇÊûúÊàë‰ª¨Ê±ÇÂá∫‰∫ÜÊñπÈòµ $A$ ÁöÑ $n$ ‰∏™ÁâπÂæÅÂÄº $\\lambda_1 \\leq \\lambda_2 \\leq \u0026hellip; \\leq \\lambda_n$ ‰ª•ÂèäËøô $n$ ‰∏™ÁâπÂæÅÂÄºÊâÄÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáè ${v_1, v_2, \u0026hellip;, v_n}$Ôºå‰∏îËøô $n$ ‰∏™ÁâπÂæÅÂêëÈáèÁ∫øÊÄßÊó†ÂÖ≥„ÄÇÈÇ£‰πàÁâπÂæÅÂÄºÂàÜËß£ÔºåEVDÔºàEigen Value DecompositionÔºâÂ∞±ÊòØÂ∞ÜÊñπÈòµ $A$ Áî®‰∏ãÈù¢ÁöÑÂΩ¢ÂºèË°®Á§∫Ôºö\n$$ A = Q \\Sigma Q^{-1} $$\nÂÖ∂‰∏≠Ôºå$Q$ ÊòØËøô $n$ ‰∏™ÁâπÂæÅÂêëÈáèÁªÑÊàêÁöÑ $n \\times n$ Áª¥Áü©ÈòµÔºå$\\Sigma$ ÊòØ‰ª•Ëøô $n$ ‰∏™ÁâπÂæÅÂÄº‰∏∫‰∏ªÂØπËßíÁ∫øÁöÑ $n \\times n$ Áª¥Áü©Èòµ„ÄÇ\nËã•Â∞ÜÁü©Èòµ $Q$ ‰∏≠ÁöÑ $n$ ‰∏™ÁâπÂæÅÂêëÈáèÊ†áÂáÜÂåñÔºåÂç≥Êª°Ë∂≥ $||v_i||_2 = 1$ÔºåÊàñËÄÖËØ¥ $v_i^Tv_i = 1$ÔºåÊ≠§Êó∂ $Q$ ‰∏≠ÁöÑ $n$ ‰∏™ÁâπÂæÅÂêëÈáè‰∏∫Ê†áÂáÜÊ≠£‰∫§Âü∫ÔºåÊª°Ë∂≥ $Q^TQ = I$ÔºåÂç≥ $Q^T = Q^{-1}$ÔºåÊ≠§Êó∂ $Q$ ÊòØ‰∏Ä‰∏™ÈÖâÁü©Èòµ„ÄÇËøôÊ†∑Êàë‰ª¨ÂèØ‰ª•ÊääÁâπÂæÅÂàÜËß£Ë°®ËææÂºèÂÜô‰Ωú\n$$ A = Q \\Sigma Q^T $$\n3. ÁâπÂæÅÂÄº‰∏éÁâπÂæÅÂêëÈáèÁöÑÂá†‰ΩïÊÑè‰πâ ‰ªéÊüêÁßçÊÑè‰πâ‰∏äÂ∞ÜÔºå‰∏Ä‰∏™Áü©ÈòµÂ∞±ÊòØ‰∏Ä‰∏™Á∫øÊÄßÂèòÊç¢ÔºåÂõ†‰∏∫‰∏Ä‰∏™Áü©Èòµ‰∏é‰∏Ä‰∏™ÂêëÈáèÁõ∏‰πòÁöÑÁªìÊûúÔºåÁõ∏ÂΩì‰∫éÂØπËøô‰∏™ÂêëÈáèËøõË°å‰∫ÜÁ∫øÊÄßÂèòÊç¢„ÄÇÊØîÂ¶ÇÔºåÂØπ‰∫éÁü©Èòµ\n$$ M = \\begin{bmatrix}3 \u0026amp; 0 \\ 0 \u0026amp; 1\\end{bmatrix} $$\nÂÆÉÂØπÂ∫îÁöÑÁ∫øÊÄßÂèòÊç¢Â¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö\nËøôÊòØÂõ†‰∏∫ÔºåÁü©Èòµ $M$ ‰∏é‰∏Ä‰∏™ÂêëÈáè $\\begin{bmatrix} x \u0026amp; y \\end{bmatrix}^T$ Áõ∏‰πòÁöÑÁªìÊûú‰∏∫\n$$ \\begin{bmatrix}3 \u0026amp; 0 \\ 0 \u0026amp; 1\\end{bmatrix} \\begin{bmatrix}x\\y\\end{bmatrix} = \\begin{bmatrix}3x\\y\\end{bmatrix} $$\nÁî±‰∫éÁü©Èòµ $M$ ÊòØÂØπÁß∞ÁöÑÔºåÊâÄ‰ª•Ëøô‰∏™ÂèòÊç¢ÊòØ‰∏Ä‰∏™ÂØπ $x$„ÄÅ$y$ ËΩ¥ÊñπÂêëÁöÑÊãâ‰º∏ÂèòÊç¢ÔºöÊØè‰∏Ä‰∏™ÂØπËßíÁ∫ø‰∏äÁöÑÂÖÉÁ¥†Â∞Ü‰ºöÂØπ‰∏Ä‰∏™Áª¥Â∫¶ËøõË°åÊãâ‰º∏ÂèòÊç¢ÔºåÂΩìÂÄº $\u0026gt; 1$ Êó∂‰∏∫ÊãâÈïøÔºåÂÄº $\u0026lt; 1$ Êó∂Áº©Áü≠„ÄÇËÄåÂΩìÁü©Èòµ $M$ ‰∏çÂØπÁß∞Êó∂\n$$ M = \\begin{bmatrix}1 \u0026amp; 1\\0 \u0026amp; 1\\end{bmatrix} $$\nÂÆÉÊâÄÊèèËø∞ÁöÑÂèòÊç¢ËßÅ‰∏ãÂõæÔºö\nÂ¶ÇËìùËâ≤ÁÆ≠Â§¥ÊâÄÁ§∫ÔºåËøôÂÖ∂ÂÆûÊòØÂú®Âπ≥Èù¢‰∏äÂØπ‰∏Ä‰∏™ËΩ¥ËøõË°åÁöÑÊãâ‰º∏ÂèòÊç¢„ÄÇÂõæ‰∏≠ÔºåËìùËâ≤ÁöÑÂâ™Â§¥ÊòØ‰∏Ä‰∏™ÊúÄ‰∏ªË¶ÅÁöÑÂèòÂåñÊñπÂêëÔºàÊ≥®ÊÑèÔºåÂèòÂåñÊñπÂêëÂπ∂‰∏çÂîØ‰∏ÄÔºâ„ÄÇÂú®ÊèèËø∞‰∏Ä‰∏™ÂèòÊç¢Êó∂ÔºåÊúÄÈáçË¶ÅÁöÑÂ∞±ÊòØÊèèËø∞Ëøô‰∏™ÂèòÊç¢‰∏ªË¶ÅÁöÑÂèòÂåñÊñπÂêë„ÄÇ\nÂ•áÂºÇÂÄºÂàÜËß£ Â•áÂºÇÂÄºÂàÜËß£ÔºåSVD (Singular Value Decomposition) ÊòØÁ∫øÊÄß‰ª£Êï∞‰∏≠‰∏ÄÁßçÈáçË¶ÅÁöÑÁü©ÈòµÂàÜËß£ÔºåÂú®‰ø°Âè∑Â§ÑÁêÜ„ÄÅÁªüËÆ°Â≠¶Á≠âÈ¢ÜÂüüÊúâÈáçË¶ÅÂ∫îÁî®„ÄÇ\n1. Â•áÂºÇÂÄºÂàÜËß£ ÁâπÂæÅÂÄºÂàÜËß£ËÉΩÂ§üÂæóÂà∞ÊñπÈòµÁöÑÁâπÂæÅ„ÄÇÂØπ‰∫éÊôÆÈÄöÁöÑÁü©ÈòµÔºåÂèØ‰ª•ÈááÁî®Â•áÂºÇÂÄºÂàÜËß£ÁöÑÊñπÊ≥ïÊèèËø∞ÂÖ∂ÁâπÂæÅÔºö\n$$ A = U \\Sigma V^T $$\nÂÖ∂‰∏≠Ôºå\n $A$ ÊòØ‰∏Ä‰∏™ $M \\times N$ ÁöÑÁü©Èòµ $U$ ÊòØ‰∏Ä‰∏™ $M \\times M$ ÁöÑÊñπÈòµÔºà$U$ ‰∏≠ÁöÑÂêëÈáèÁß∞‰∏∫Â∑¶Â•áÂºÇÂêëÈáèÔºåÈÉΩÊòØÊ≠£‰∫§ÁöÑÔºâ $\\Sigma$ ÊòØ‰∏Ä‰∏™ $M \\times N$ ÁöÑÁü©ÈòµÔºàÈô§‰∫ÜÂØπËßíÁ∫øÔºåÂÖ∂‰ªñÂÖÉÁ¥†ÈÉΩ‰∏∫ 0ÔºåÂØπËßíÁ∫ø‰∏äÁöÑÂÖÉÁ¥†Áß∞‰∏∫Â•áÂºÇÂÄºÔºâ $V$ ÊòØ‰∏Ä‰∏™ $N \\times N$ ÁöÑÊñπÈòµÔºà$V$ ‰∏≠ÁöÑÂêëÈáèÁß∞‰∏∫Âè≥Â•áÂºÇÂêëÈáèÔºåÈÉΩÊòØÊ≠£‰∫§ÁöÑÔºâ  2. Â•áÂºÇÂÄºÁöÑËÆ°ÁÆó Â∞Ü $A$ ÁöÑËΩ¨ÁΩÆ‰∏é $A$ Áõ∏‰πòÔºåÈÇ£‰πàÂØπ‰∫é $N \\times N$ ÁöÑÊñπÈòµ $A^TA$ÔºåÂÖ∂ÁâπÂæÅÂÄºÂèäÁâπÂæÅÂêëÈáèÊª°Ë∂≥\n$$ (A^TA)v_i = \\lambda_iv_i $$\nÂÖ∂‰∏≠ÁöÑ $v_i$ Âç≥Âè≥Â•áÂºÇÂêëÈáèÔºå$n$ ‰∏™ÁâπÂæÅÂêëÈáè $v$ ÁªÑÊàê‰∫ÜÁü©Èòµ $V$„ÄÇ\nÂêåÁêÜÔºåÂØπ‰∫é $M \\times M$ ÁöÑÁü©Èòµ $AA^T$ÔºåÂÖ∂ÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáèÊª°Ë∂≥\n$$ (AA^T)u_i = \\lambda_i u_i $$\nÂÖ∂‰∏≠ÁöÑ $u_i$ Âç≥Â∑¶Â•áÂºÇÂêëÈáèÔºå$m$ ‰∏™ÁâπÂæÅÂêëÈáè $u$ ÁªÑÊàê‰∫ÜÁü©Èòµ $U$„ÄÇ\nÊé®ÂØºÂèØÂæóÔºö\n$$ \\begin{align} A \u0026amp;= U \\Sigma V^T \\ AV \u0026amp;= U \\Sigma V^T V \\ AV \u0026amp;= U \\Sigma \\end{align} $$\nÂç≥\n$$ \\begin{align} A v_i \u0026amp;= \\sigma_i u_i \\ \\sigma_i \u0026amp;= Av_i / u_i \\end{align} $$\nËøôÊ†∑ÂèØ‰ª•Ê±ÇÂæóÊâÄÊúâÁöÑÂ•áÂºÇÂÄº $\\sigma$ÔºåËøõËÄåÊ±ÇÂá∫Â•áÂºÇÂÄºÁü©Èòµ $\\Sigma$ „ÄÇÊ≠§Â§ñËøòÊúâ\n$$ \\sigma_i = \\sqrt{\\lambda_i} $$\nÂ•áÂºÇÂÄº $\\sigma_i$ ‰πüÂèØ‰ª•ÈÄöËøáÊ±ÇÂá∫ $A^TA$ ÁöÑÁâπÂæÅÂÄºÂèñÂπ≥ÊñπÊ†πÂæóÂà∞„ÄÇ\n3. SVD ÁöÑÊÄßË¥® Â•áÂºÇÂÄº $\\sigma$ ‰∏éÁâπÂæÅÂÄºÁ±ª‰ººÔºåÂú®Áü©Èòµ $\\Sigma$ ‰∏≠‰πüÊòØ‰ªéÂ§ßÂà∞Â∞èÈôçÂ∫èÊéíÂàóÔºåËÄå‰∏î $\\sigma$ ÂáèÂ∞ëÂæóÁâπÂà´Âø´ÔºåÂú®ÂæàÂ§öÊÉÖÂÜµ‰∏ãÔºåÂâç 10% ÁîöËá≥ 1% ÁöÑÂ•áÂºÇÂÄºÁöÑÂíåÂ∞±Âç†ÂÖ®ÈÉ®Â•áÂºÇÂÄº‰πãÂíåÁöÑ 99% ‰ª•‰∏ä‰∫Ü„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÊàë‰ª¨‰πüÂèØ‰ª•Áî®Ââç $r$ ‰∏™Â•áÂºÇÂÄºÊù•Ëøë‰ººÊèèËø∞Áü©ÈòµÔºåÂç≥ÈÉ®ÂàÜÂ•áÂºÇÂÄºÂàÜËß£Ôºö\n$$ A_{m\\times n} \\approx U_{m\\times r}\\Sigma_{r \\times r} V^T_{r \\times n} $$\nÁî±‰∫é $r$ ÊØî $n$ Â∞èÂæàÂ§öÔºåÂ≠òÂÇ®Áü©ÈòµÂíåÁü©ÈòµËÆ°ÁÆóÁöÑËä±Ë¥π‰πüÂ§ßÂ§ßÂáèÂ∞ë„ÄÇÂΩìÁÑ∂Ôºå$r$ Ë∂äÊé•Ëøë‰∫é $n$ÔºåÁõ∏‰πòÁöÑÁªìÊûú‰πüË∂äÊé•Ëøë‰∫é $A$ „ÄÇ\nÊ†πÊçÆËøô‰∏™ÊÄßË¥®ÔºåSVD ÂèØ‰ª•Áî®‰∫é PCAÔºàPrincipal Component AnalysisÔºå‰∏ªÊàêÂàÜÂàÜÊûêÔºâÈôçÁª¥ÔºåÊù•ÂÅöÊï∞ÊçÆÂéãÁº©ÂíåÂéªÂô™„ÄÇ‰πüÂèØ‰ª•Áî®‰∫éÊé®ËçêÁÆóÊ≥ïÔºåÂ∞ÜÁî®Êà∑ÂíåÁî®Êà∑ÂñúÂ•ΩÂØπÂ∫îÁöÑÁü©ÈòµÂÅöÁâπÂæÅÂàÜËß£ÔºåËøõËÄåÂæóÂà∞ÈöêÂê´ÁöÑÁî®Êà∑ÈúÄÊ±ÇÊù•ÂÅöÊé®ËçêÔºå‰∫ãÂÆû‰∏ä Netflix Á≠âÂÖ¨Âè∏ÁöÑÊé®ËçêÁÆóÊ≥ïÈÉΩÈááÁî®‰∫Ü SVD„ÄÇÂêåÊó∂Â•áÂºÇÂÄºÂàÜËß£‰πüÂèØ‰ª•Áî®‰∫é NLPÔºàNatural Language ProcessingÔºåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºâÔºåÊØîÂ¶Ç LSIÔºàLatent Semantic IndexingÔºåÊΩúÂú®ËØ≠‰πâÁ¥¢ÂºïÔºâÁ≠â„ÄÇ\nRQ ÂàÜËß£ 1. QR ÂàÜËß£ ‰∫ÜËß£ RQ ÂàÜËß£‰πãÂâçÔºåÂÖàË¶ÅÊéåÊè° QR ÂàÜËß£„ÄÇQR ÂàÜËß£Â∞ÜÁü©Èòµ $A$ ÂàÜËß£‰∏∫‰∏Ä‰∏™Ê≠£‰∫§Áü©Èòµ $Q$ ‰∏é‰∏ä‰∏âËßíÁü©Èòµ $R$ ÁöÑÁßØÔºåÂç≥\n$$ A = QR $$\n‰ΩøÁî® Householder ÂèòÊç¢ÂèØ‰ª•ÂØπÂÖ∂ËøõË°åÊ±ÇËß£ÔºåÊ≠§Â§Ñ‰∏çÂÜçËµòËø∞ÔºåÂèØÂèÇËÄÉÊñáÊú´ÈìæÊé•„ÄÇ\n2. RQ ÂàÜËß£ ÂØπÁü©Èòµ $A$ ËøõË°å RQ ÂàÜËß£Âç≥Â∞ÜÂÖ∂ÂàÜËß£‰∏∫‰∏Ä‰∏™‰∏ä‰∏âËßíÈòµ $R$ ‰∏é‰∏Ä‰∏™Ê≠£‰∫§Áü©ÈòµÔºàOrthogonal MatrixÔºâ$Q$ ÁöÑ‰πòÁßØÔºåË¶ÅÊ±ÇÁü©Èòµ $A$ Êª°Áß©„ÄÇ\nÂØπ‰∫é $n \\times n$ ÁöÑÁü©Èòµ $A$ÔºåËã•ÁªôÂÆöÁü©Èòµ $P$ÔºåÊª°Ë∂≥\n$$ P = \\begin{bmatrix} \u0026amp; \u0026amp; 1 \\ \u0026amp; \\cdots \u0026amp; \\ 1 \u0026amp; \u0026amp; \\end{bmatrix} $$\nÂàô $AP$ ‰ºöÂÄíÁΩÆ $A$ ‰∏≠ÂàóÁöÑÈ°∫Â∫èÔºå$PA$ ‰ºöÂÄíÁΩÆ $A$ ‰∏≠Ë°åÁöÑÈ°∫Â∫è„ÄÇ‰∏î $P^T = P$Ôºå$PP = I$Ôºå‰∫éÊòØ $P^{-1} = P = P^T$Ôºå$P$ ‰∏∫Ê≠£‰∫§Áü©Èòµ„ÄÇËøõË°å‰ª•‰∏ãÊ≠•È™§ÁöÑËÆ°ÁÆóÔºö\n ËÆ°ÁÆó $\\tilde{A} = PA$ÔºåÂç≥ÂÄíÁΩÆ $A$ ‰∏≠Ë°åÁöÑÈ°∫Â∫è ËÆ°ÁÆó QR ÂàÜËß£Ôºö$\\tilde{A}^T = \\tilde{Q}\\tilde{R}$ ‰ª§ $Q = P \\tilde{Q}^T$ÔºåÂç≥ÂÄíÁΩÆ $\\tilde{Q}^T$ ‰∏≠Ë°åÁöÑÈ°∫Â∫èÔºà$Q$ ÊòØÊ≠£‰∫§Áü©ÈòµÔºâ ‰ª§ $R = P\\tilde{R}^TP$  Âú®ÊúÄÂêé‰∏ÄÊ≠•‰∏≠Ôºå‰∏ä‰∏âËßíÁü©Èòµ $\\tilde{R}$ ÈÄöËøá‰∏§Ê¨°Áü©Èòµ $P$ Áõ∏‰πòÂèòÊàê‰∏Ä‰∏™‰∏ã‰∏âËßíÁü©Èòµ\n$$ \\begin{bmatrix}* \u0026amp; * \u0026amp; * \\ \u0026amp; * \u0026amp; * \\ \u0026amp; \u0026amp; * \\end{bmatrix} \\rightarrow \\begin{bmatrix} \u0026amp; \u0026amp; * \\ \u0026amp; * \u0026amp; * \\* \u0026amp; * \u0026amp; * \\end{bmatrix} \\rightarrow \\begin{bmatrix}* \u0026amp; \u0026amp; \\ * \u0026amp; * \u0026amp; \\ * \u0026amp; * \u0026amp; *\\end{bmatrix} $$\nÁî±‰∏äÈù¢ÁöÑ $R$ Âíå $Q$ ÂèØ‰ª•ÂæóÂà∞\n$$ RQ = (P\\tilde{R}^TR)(P\\tilde{Q}^T) = P\\tilde{R}^T\\tilde{Q}^T = P(\\tilde{Q}\\tilde{R})^T=P(\\tilde{A}^T)^T=P\\tilde{A}=PPA=A $$\n ÂèÇËÄÉÔºö\n$1$. Â•áÂºÇÂÄºÂàÜËß£ - Áª¥Âü∫ÁôæÁßëÔºåËá™Áî±ÁöÑÁôæÁßëÂÖ®‰π¶\n$2$. Êú∫Âô®Â≠¶‰π†‰∏≠ÁöÑÊï∞Â≠¶(5)-Âº∫Â§ßÁöÑÁü©ÈòµÂ•áÂºÇÂÄºÂàÜËß£(SVD)ÂèäÂÖ∂Â∫îÁî® - LeftNotEasy - ÂçöÂÆ¢Âõ≠\n$3$. AMS :: Feature Column from the AMS\n$4$. Â•áÂºÇÂÄºÂàÜËß£(SVD)ÂéüÁêÜ‰∏éÂú®ÈôçÁª¥‰∏≠ÁöÑÂ∫îÁî® - ÂàòÂª∫Âπ≥Pinard - ÂçöÂÆ¢Âõ≠\n$5$. Â•áÂºÇÂÄºÂàÜËß£ÔºàSVDÔºâÂéüÁêÜ - JackGaoÁöÑÂçöÂÆ¢ - CSDNÂçöÂÆ¢\n$6$. QRÂàÜËß£ - Áª¥Âü∫ÁôæÁßëÔºåËá™Áî±ÁöÑÁôæÁßëÂÖ®‰π¶\n$7$. G. H. Golub, C. Reinsch, Singular Value Decomposition and Least Squares Solutions.\n$8$. D. Kalman, A Singularly Valuable Decomposition: The SVD of a Matrix.\n ","permalink":"https://fang-lansheng.github.io/posts/2019-04-12-math-1/","summary":"ÁâπÂæÅÂÄºÂàÜËß£ 1. ÁâπÂæÅÂÄº‰∏éÁâπÂæÅÂêëÈáè Âú®Á∫øÊÄß‰ª£Êï∞‰∏≠ÔºåÂØπ‰∫é $n$ Èò∂ÊñπÈòµ $A$ÔºåÂ¶ÇÊûúÂ≠òÂú®Êüê‰∏™Êï∞ $\\lambda$ ÂèäÊüê‰∏™ $n$ Áª¥ÈùûÈõ∂ÂàóÂêëÈáè $v$Ôºå‰ΩøÂæó\n$$ Av = \\lambda v $$\nÂàôÁß∞ $\\lambda$ ÊòØÊñπÈòµ $A$ ÁöÑ‰∏Ä‰∏™ÁâπÂæÅÂÄºÔºå$v$ ÊòØÊñπÈòµ $A$ ÁöÑÂ±û‰∫éÁâπÂæÅÂÄº $\\lambda$ ÁöÑ‰∏Ä‰∏™ÁâπÂæÅÂêëÈáè„ÄÇ\nÂØπ‰∏äÂºèËøõË°åÂèòÊç¢Ôºö\n$$ \\begin{align} Av-\\lambda v \u0026amp;= \\overrightarrow{0} \\ Av-\\lambda I v \u0026amp;= \\overrightarrow{0} \\ (A - \\lambda I)v \u0026amp;= \\overrightarrow{0} \\ (\\lambda I - A)v \u0026amp;= \\overrightarrow{0} \\end{align} $$\nÂàôÁß∞\n $\\lambda E - A$ ‰∏∫ $A$ ÁöÑÁâπÂæÅÁü©Èòµ Ë°åÂàóÂºè $f(\\lambda) = |\\lambda E- A|$ ‰∏∫ $A$ ÁöÑÁâπÂæÅÂ§öÈ°πÂºè $|\\lambda E - A| = 0$ ÊòØ $A$ ÁöÑÁâπÂæÅÊñπÁ®ã $(\\lambda E - A)v = \\overrightarrow{0}$ ÊòØ $A$ ÂÖ≥‰∫éËØ•ÁâπÂæÅÂÄº $\\lambda$ ÁöÑÈΩêÊ¨°Á∫øÊÄßÊñπÁ®ãÁªÑ  $A$ ÁöÑ‰∏ªÂØπËßíÁ∫ø‰∏äÂÖÉÁ¥†‰πãÂíåÁß∞‰∏∫Áü©ÈòµÁöÑËøπÔºàtraceÔºâÔºåËÆ∞‰∏∫ $tr(A)$ÔºåÂç≥","title":"Á∫øÊÄß‰ª£Êï∞‚Äî‚ÄîÁâπÂæÅÂÄºÂàÜËß£„ÄÅÂ•áÂºÇÂÄºÂàÜËß£‰∏é RQ ÂàÜËß£"},{"content":"Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÔºàComputer VisionÔºâ‰∏éÊëÑÂΩ±ÊµãÈáèÔºàPhotogrammetryÔºâ‰∏≠ÔºåÊàë‰ª¨Âú®‰∏çÂêåÂΩ±ÂÉè‰πãÈó¥ÊâæÂà∞Áõ∏ÂåπÈÖçÁöÑÁâπÂæÅÔºåÂ∑≤Âª∫Á´ã‰∏§ÂπÖÂΩ±ÂÉè‰πãÈó¥ÁöÑËÅîÁ≥ªÔºåÊèêÂèñÂá∫Êàë‰ª¨ÊâÄÈúÄË¶ÅÁöÑ‰ø°ÊÅØ„ÄÇËøô‰∫õÁâπÂæÅ‰∏ªË¶ÅÂàÜ‰∏∫Ôºö\n ËæπÁºòÔºàEdgesÔºâ ËßíÁÇπÔºàCornersÔºâ ÂÖ¥Ë∂£Âå∫Âüü ROIÔºàRegions of InterestÔºâ  ÂÖ∂‰∏≠ÔºåÊèêÂèñÁÇπÁâπÂæÅÁöÑÁÆóÂ≠êÁß∞‰∏∫ÂÖ¥Ë∂£ÁÆóÂ≠êÊàñÊúâÂà©ÁÆóÂ≠êÔºàInterest OperatorÔºâÔºåÂç≥ËøêÁî®ÊüêÁßçÁÆóÊ≥ï‰ªéÂΩ±ÂÉèÁßçÊèêÂèñÊàë‰ª¨ÊÑüÂÖ¥Ë∂£ÁöÑ„ÄÅÊúâÂà©‰∫éÊüêÁßçÁõÆÁöÑÁöÑÁÇπ„ÄÇ\n‰∫∫ÁúºÂØπËßíÁÇπÁöÑËØÜÂà´ÈÄöÂ∏∏ÊòØÂú®‰∏Ä‰∏™Â±ÄÈÉ®ÁöÑÂ∞èÂå∫ÂüüÊàñÂ∞èÁ™óÂè£ÂÜÖÂÆåÊàêÁöÑ„ÄÇÂ¶ÇÊûúÂú®ÂêÑ‰∏™ÊñπÂêë‰∏äÁßªÂä®Ëøô‰∏™ÁâπÂæÅÁöÑÂ∞èÁ™óÂè£ÔºåÁ™óÂè£ÂÜÖÂå∫ÂüüÁöÑÁÅ∞Â∫¶ÂèëÁîü‰∫ÜËæÉÂ§ßÁöÑÂèòÂåñÔºåÈÇ£‰πàÂ∞±ËÆ§‰∏∫Âú®Á™óÂè£ÂÜÖÈÅáÂà∞‰∫ÜËßíÁÇπ„ÄÇÂ¶ÇÊûúËøô‰∏™ÁâπÂÆöÁöÑÁ™óÂè£Âú®ÂõæÂÉèÂêÑ‰∏™ÊñπÈù¢ÁßªÂä®Êó∂ÔºåÁ™óÂè£ÂÜÖÂõæÂÉèÁöÑÁÅ∞Â∫¶Ê≤°ÊúâÂèëÁîüÂèòÂåñÔºåÈÇ£‰πàÁ™óÂè£ÂÜÖÂ∞±‰∏çÂ≠òÂú®ËßíÁÇπÔºõÂ¶ÇÊûúÁ™óÂè£Âú®Êüê‰∏Ä‰∏™ÊñπÂêëÁßªÂä®Êó∂ÔºåÁ™óÂè£ÂÜÖÂõæÂÉèÁöÑÁÅ∞Â∫¶ÂèëÁîü‰∫ÜËæÉÂ§ßÁöÑÂèòÂåñÔºåËÄåÂú®Âè¶‰∏Ä‰∫õÊñπÂêë‰∏äÊ≤°ÊúâÂèëÁîüÂèòÂåñÔºåÈÇ£‰πàÁ™óÂè£ÂÜÖÁöÑÂõæÂÉèÂèØËÉΩÂè™ÊòØ‰∏ÄÊù°Áõ¥Á∫øÁöÑÁ∫øÊÆµ„ÄÇ\nËßíÁÇπÁöÑÁâπÊÆä‰πãÂ§ÑÂú®‰∫éÔºåÂÆÉÊòØ‰∏§Êù°ËæπÁöÑ‰∫§ÁÇπÔºåÊâÄ‰ª•ÂÆÉ‰ª£Ë°®Ëøô‰∏§Êù°ËæπÁöÑÊñπÂêëÂèëÁîüÂèòÂåñÁöÑÁÇπ„ÄÇÂõ†Ê≠§ÔºåÂõæÂÉèÂú®ËØ•ÁÇπÁöÑÊ¢ØÂ∫¶ÔºàÂú®ÂêÑ‰∏™ÊñπÂêë‰∏äÔºâÂÖ∑ÊúâÊòéÊòæÁöÑÂèòÂåñ„ÄÇÂà©Áî®Ëøô‰∏ÄÁÇπÔºåÂâç‰∫∫ÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÁÆóÊ≥ïÔºåÂÖ∂‰∏≠ÊØîËæÉÁªèÂÖ∏ÁöÑÊúâ Moravec ÁÆóÂ≠ê„ÄÅForstner ÁÆóÂ≠ê‰ª•Âèä Harris ÁÆóÂ≠êÁ≠â„ÄÇ\nMoravec ÁÆóÂ≠ê Âü∫Êú¨ÂéüÁêÜ Moravec ‰∫é 1977 Âπ¥ÊèêÂá∫Âà©Áî®ÁÅ∞Â∫¶ÊñπÂ∑ÆÊèêÂèñÁÇπÁâπÂæÅÁöÑÁÆóÂ≠êÔºåÂÖ∂Ê≠•È™§‰∏∫Ôºö\n1. ËÆ°ÁÆóÂêÑÂÉèÂÖÉÁöÑÂÖ¥Ë∂£ÂÄº $IV$ÔºàInterest ValueÔºâ„ÄÇ Âú®‰ª•ÂÉèÁ¥† $(c, r)$ ‰∏∫‰∏≠ÂøÉÁöÑ $w \\times w$ ÁöÑÁ™óÂè£‰∏≠ÔºàÂ¶Ç $5 \\times 5$ ÁöÑÁ™óÂè£ÔºâÔºåËÆ°ÁÆóÂõõ‰∏™ÊñπÂêëÁõ∏ÈÇªÂÉèÁ¥†ÁÅ∞Â∫¶Â∑ÆÁöÑÂπ≥ÊñπÂíåÔºö\n$$ \\begin{cases} V_1 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c+i,r}-g_{c+i+1, r})^2 \\[2ex] V_2 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c+i,r+i}-g_{c+i+1, r+i+1})^2 \\[2ex] V_3 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c,r+i}-g_{c+i+1, r+i+1})^2 \\[2ex] V_4 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c+i,r-i}-g_{c+i+1, r-i-1})^2 \\end{cases} $$\nÂÖ∂‰∏≠ $k = INT(w/2)$„ÄÇÂèñÂÖ∂‰∏≠ÊúÄÂ∞èËÄÖ‰Ωú‰∏∫ËØ•ÂÉèÁ¥† $(c,r)$ ÁöÑÂÖ¥Ë∂£ÂÄºÔºö\n$$ IV_{c,r} = min {V_1, V_2, V_3, V_4} $$\n2. ÁªôÂÆö‰∏ÄÁªèÈ™åÈòàÂÄºÔºåÂ∞ÜÂÖ¥Ë∂£ÂÄºÂ§ß‰∫éËØ•ÈòàÂÄºÁöÑÁÇπÔºàÂç≥ÂÖ¥Ë∂£ÂÄºËÆ°ÁÆóÁ™óÂè£ÁöÑ‰∏≠ÂøÉÁÇπÔºâ‰Ωú‰∏∫ÂÄôÈÄâÁÇπ„ÄÇ ÈòàÂÄºÁöÑÈÄâÊã©Â∫î‰ª•ÂÄôÈÄâÁÇπ‰∏≠ÂåÖÊã¨ÊâÄÈúÄË¶ÅÁöÑÁâπÂæÅÁÇπËÄåÂèà‰∏çÂê´ËøáÂ§öÁöÑÈùûÁâπÂæÅÁÇπ‰∏∫ÂéüÂàô„ÄÇ\n3. ÈÄâÂèñÂÄôÈÄâÁÇπ‰∏≠ÁöÑÊûÅÂÄºÁÇπ‰Ωú‰∏∫ÁâπÂæÅÁÇπ„ÄÇ Âú®‰∏ÄÂÆöÂ§ßÂ∞èÁöÑÁ™óÂè£ÂÜÖÔºàÂèØ‰∏çÂêå‰∫éÂÖ¥Ë∂£ÂÄºËÆ°ÁÆóÁ™óÂè£Ôºå‰æãÂ¶Ç $5 \\times 5$ ÂÉèÂÖÉÔºå$7 \\times 7$ ÂÉèÂÖÉÊàñ $9 \\times 9$ ÂÉèÂÖÉÔºâÔºåÂ∞ÜÂÄôÈÄâÁÇπ‰∏≠ÂÖ¥Ë∂£ÂÄº‰∏çÊòØÊúÄÂ§ßËÄÖÂùáÂéªÊéâÔºå‰ªÖÁïô‰∏ã‰∏Ä‰∏™ÂÖ¥Ë∂£ÂÄºÊúÄÂ§ßËÄÖÔºåËØ•ÂÉèÁ¥†Âç≥‰∏∫‰∏Ä‰∏™ÁâπÂæÅÁÇπÔºàÊúâÁöÑÊñáÁåÆ‰∏≠Áß∞Ê≠§Ê≠•È™§‰∏∫ÊäëÂà∂Â±ÄÈÉ®ÈùûÊúÄÂ§ßÔºâ„ÄÇ\n‰ª£Á†ÅÂÆûÁé∞ /// Moravec ÁÇπÁâπÂæÅÊèêÂèñÁÆóÊ≥ï void Moravec(InputArray image, vector\u0026lt;KeyPoint\u0026gt;\u0026amp; keyPoints, int kSize, int threshold) { Mat img; cvtColor(image, img, COLOR_BGR2GRAY); unsigned char* pImg = img.data; int height = img.rows; int width = img.cols; int size = height * width; double* candidatePoints = new double[size]; for (int i = 0; i \u0026lt; size; i++) candidatePoints[i] = 0; int k = kSize / 2; /// I. ËÆ°ÁÆóÂêÑÂÉèÂÖÉÁöÑÂÖ¥Ë∂£ÂÄº  for (int i = k; i \u0026lt; height - k; i++) { for (int j = k; j \u0026lt; width - k; j++) { int V1, V2, V3, V4; V1 = V2 = V3 = V4 = 0; // Âú®‰ª•ÂÉèÁ¥† (i, j) ‰∏∫‰∏≠ÂøÉÁöÑ k * k ÁöÑÂΩ±ÂÉèÁ™óÂè£‰∏≠  // ËÆ°ÁÆóÊ∞¥Âπ≥„ÄÅ45¬∞„ÄÅÁ´ñÁõ¥„ÄÅ135¬∞Âõõ‰∏™ÊñπÂêë‰∏äÁõ∏ÈÇªÂÉèÁ¥†ÁÅ∞Â∫¶Â∑ÆÁöÑÂπ≥ÊñπÂíå  for (int r = -k; r \u0026lt; k; r++) { V1 += (pImg[(i + r) * width + j] - pImg[(i + r + 1) * width + j]) * (pImg[(i + r) * width + j] - pImg[(i + r + 1) * width + j]); V2 += (pImg[(i + r) * width + j + r] - pImg[(i + r + 1) * width + j + r + 1]) * (pImg[(i + r) * width + j + r] - pImg[(i + r + 1) * width + j + r + 1]); V3 += (pImg[i * width + j + r] - pImg[i * width + j + r + 1]) * (pImg[i * width + j + r] - pImg[i * width + j + r + 1]); V4 += (pImg[(i + r) * width + j - r] - pImg[(i + r + 1) * width + j - r - 1]) * (pImg[(i + r) * width + j - r] - pImg[(i + r + 1) * width + j - r - 1]); } // ÂèñÂÖ∂‰∏≠ÁöÑÊúÄÂ∞èÂÄº‰Ωú‰∏∫ËØ•ÂÉèÁ¥† (i, j) ÁöÑÂÖ¥Ë∂£ÂÄº  int value = min(min(V1, V2), min(V3, V4)); // II. Â∞ÜÂÖ¥Ë∂£ÂÄºÂ§ß‰∫éÈòàÂÄºÁöÑÁÇπ‰Ωú‰∏∫ÂÄôÈÄâÁÇπ  if (value \u0026gt; threshold) candidatePoints[i * width + j] = value; } } /// III. ÈÄâÂèñÂÄôÈÄâÁÇπ‰∏≠ÁöÑÊûÅÂÄºÁÇπ‰Ωú‰∏∫ÁâπÂæÅÁÇπÔºàÈùûÊûÅÂ§ßÂÄºÊäëÂà∂Ôºâ  int max_i, max_j, flag; double max_val; for (int i = k; i \u0026lt; height - k; i += kSize) { for (int j = k; j \u0026lt; width - k; j += kSize) { max_i = 0; max_j = 0; max_val = 0; flag = 0; for (int m = -k; m \u0026lt; k; m++) { for (int n = -k; n \u0026lt; k; n++) { double val = candidatePoints[(i + m) * width + j + n]; if (val \u0026gt; max_val) { max_i = i + m; max_j = j + n; max_val = val; flag = 1; } } } if (flag) keyPoints.push_back(KeyPoint((float)max_j, (float)max_i, 2)); } } } ÊèêÂèñÁªìÊûú Forstner ÁÆóÂ≠ê Âü∫Êú¨ÂéüÁêÜ ËØ•ÁÆóÂ≠êÈÄöËøáËÆ°ÁÆóÂêÑÂÉèÁ¥†ÁöÑ Robert Ê¢ØÂ∫¶ÂíåÂÉèÁ¥† $(c, r)$ ‰∏∫‰∏≠ÂøÉÁöÑ‰∏Ä‰∏™Á™óÂè£ÔºàÂ¶Ç $5 \\times 5)$ ÁöÑÁÅ∞Â∫¶ÂçèÊñπÂ∑ÆÁü©ÈòµÔºåÂú®ÂΩ±ÂÉè‰∏≠ÂØªÊâæÂÖ∑ÊúâÂ∞ΩÂèØËÉΩÂ∞èËÄåÊé•ËøëÂúÜÁöÑËØØÂ∑ÆÊ§≠ÂúÜÁöÑÁÇπ‰Ωú‰∏∫ÁâπÂæÅÁÇπÔºåÂÖ∂Ê≠•È™§‰∏∫Ôºö\n1. ËÆ°ÁÆóÂêÑÂÉèÁ¥†ÁöÑ Robert Ê¢ØÂ∫¶„ÄÇ\n$$ \\left.\\begin{matrix} g_u = \\frac{\\partial g}{\\partial u} = g_{i + 1, j + 1} - g_{i, j} \\ g_v = \\frac{\\partial g}{\\partial v} = g_{i, j + 1} - g_{i + 1, j} \\end{matrix}\\right} $$\n2. ËÆ°ÁÆó $l \\times l$ÔºàÂ¶Ç $5 \\times 5$ ÊàñÊõ¥Â§ßÔºâÁ™óÂè£‰∏≠ÁÅ∞Â∫¶ÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ„ÄÇ\n$$ Q = N^{-1} = \\begin{bmatrix}\\sum g^2_u \u0026amp; \\sum g_ug_v \\ \\sum g_vg_u \u0026amp; \\sum g^2_v\\end{bmatrix}^{-1} $$\nÂÖ∂‰∏≠Ôºö\n$$ \\begin{align} \u0026amp;\\sum g_u^2 = \\sum_{i = c - k}^{c+k-1} \\sum_{j=r-k}^{r+k+1}(g_{i+1, j+1} - g_{i,j})^2 \\[2ex] \u0026amp;\\sum g_v^2 = \\sum_{i = c - k}^{c+k-1} \\sum_{j=r-k}^{r+k+1}(g_{i, j+1} - g_{i+1,j})^2 \\[2ex] \u0026amp;\\sum g_ug_v = \\sum_{i = c - k}^{c+k-1} \\sum_{j=r-k}^{r+k+1}(g_{i+1, j+1} - g_{i,j})(g_{i, j+1} - g_{i+1,j}) \\end{align} $$\n3. ËÆ°ÁÆóÂÖ¥Ë∂£ÂÄº $q$ Âíå $w$\n$$ w = \\frac{1}{trQ} = \\frac{DetN}{trN} \\[2ex] q = \\frac{4DetN}{(trN)^2} $$\nÂÖ∂‰∏≠Ôºå$DetN$ ‰ª£Ë°®Áü©Èòµ $N$ ÁöÑË°åÂàóÂºèÔºõ$trN$ ‰ª£Ë°®Áü©Èòµ $N$ ÁöÑËøπ„ÄÇ\nÂèØ‰ª•ËØÅÊòéÔºå$q$ Âç≥ÂÉèÁ¥† $(c, r)$ ÂØπÂ∫îËØØÂ∑ÆÊ§≠ÂúÜÁöÑÂúÜÂ∫¶Ôºö\n$$ q = 1 - \\frac{(a^2-b^2)^2}{(a^2+b^2)^2} $$\n**4. Á°ÆÂÆöÂæÖÈÄâÁÇπ„ÄÇ**Â¶ÇÊûúÂÖ¥Ë∂£ÂÄºÂ§ß‰∫éÁªôÂÆöÁöÑÈòàÂÄºÔºåÂàôËØ•ÂÉèÂÖÉ‰∏∫ÂæÖÈÄâÁÇπ„ÄÇÈòàÂÄº‰∏∫ÁªèÈ™åÂÄºÔºåÂèØÂèÇËÄÉ‰∏ãÂàóÂÄºÔºö\n$$ \\left.\\begin{align} T_q \u0026amp;= 0.5 \\sim 0.75 \\ T_w \u0026amp;= \\begin{cases} f\\overline{w}\u0026amp;(f=0.5 \\sim 1.5) \\ cw_c\u0026amp;(c = 5) \\end{cases} \\end{align}\\right} $$\nÂÖ∂‰∏≠ $\\overline{w}$ ‰∏∫ÊùÉÂπ≥ÂùáÂÄºÔºõ$w_c$ ‰∏∫ÊùÉÁöÑ‰∏≠ÂÄº„ÄÇÂΩì $q \u0026gt; T_q$Ôºå‰∏î $w \u0026gt; T_w$ Êó∂ÔºåËØ•ÂÉèÂÖÉ‰∏∫ÂæÖÈÄâÁÇπ„ÄÇ\n**5. ÈÄâÂèñÊûÅÂÄºÁÇπ„ÄÇ**‰ª•ÊùÉÂÄº $w$ ‰∏∫‰æùÊçÆÈÄâÊã©ÊûÅÂÄºÁÇπÔºåÂç≥Âú®‰∏Ä‰∏™ÈÄÇÂΩìÁ™óÂè£‰∏≠ÈÄâÊã©ÊúÄÂ§ßÁöÑÂæÖÈÄâÁÇπÔºåËÄåÂéªÊéâÂÖ∂‰ΩôÁöÑÁÇπ„ÄÇ\n‰ª£Á†ÅÂÆûÁé∞ void Forstner(InputArray image, vector\u0026lt;KeyPoint\u0026gt; \u0026amp; keyPoints, int kSize) { Mat img; cvtColor(image, img, COLOR_BGR2GRAY); unsigned char* pImg = img.data; int height = img.rows; int width = img.cols; int size = height * width; int k = kSize / 2; double grad_u = 0, grad_v = 0;\t// Robert Ê¢ØÂ∫¶  double DetN = 0.0, trN = 0.0;\t// DetN: Áü©Èòµ N ÁöÑË°åÂàóÂºèÔºåtrN: Áü©Èòµ N ÁöÑËøπ  double weight_total = 0, weight_mean = 0;\t// ÊùÉÁöÑÊÄªÂÄº‰∏éÂùáÂÄº  double q = 0, w = 0;\t// ÂÖ¥Ë∂£ÂÄº q„ÄÅw  double* wPoints = new double[size];\t// Â£∞ÊòéÂ≠òÂÇ®ÂÖ¥Ë∂£ÂÄºÁöÑÊï∞ÁªÑ  double* qPoints = new double[size]; for (int i = 0; i \u0026lt; size; i++) {\t// ÁªôÊï∞ÁªÑËµãÂàùÂÄº  wPoints[i] = 0; qPoints[i] = 0; } for (int i = k; i \u0026lt; height - k; i++) { for (int j = k; j \u0026lt; width - k; j++) { double N[4] = { 0 }; for (int m = -k; m \u0026lt; k; m++) { for (int n = -k; n \u0026lt; k; n++) { grad_u = pImg[(i + m + 1) * width + j + n + 1] - pImg[(i + m) * width + j + n]; grad_v = pImg[(i + m + 1) * width + j + n] - pImg[(i + m) * width + j + n + 1]; N[0] += grad_u * grad_u; N[1] += grad_u * grad_v; N[2] = N[1]; N[3] += grad_v * grad_v; } } DetN = N[0] * N[3] - N[1] * N[2];\t// DetN = determinant(N);  trN = N[0] + N[3];\t// trN = trace(N);  if (trN != 0) { w = DetN / trN;\t// ÊùÉ  q = 4 * DetN / (trN * trN);\t// ÂúÜÂ∫¶  weight_total += w; wPoints[i * width + j] = w; qPoints[i * width + j] = q; } } } weight_mean = weight_total / size;\t// ÊùÉÂπ≥ÂùáÂÄº  double Tq = 0.75;\t// ÂèÇËÄÉÈòàÂÄº  double Tw = 0.5 * weight_mean; for (int i = k; i \u0026lt; height - k; i += kSize) { for (int j = k; j \u0026lt; width - k; j += kSize) { int max_i = 0, max_j = 0, flag = 0; double max_weight = 0; for (int m = -k; m \u0026lt; k; m++) { for (int n = -k; n \u0026lt; k; n++) { double val_q = qPoints[(i + m) * width + j + n]; double val_w = wPoints[(i + m) * width + j + n]; // Â§ß‰∫éÈòàÂÄºÊó∂ÔºåËØ•ÁÇπ‰∏∫ÂæÖÈÄâÁÇπ  if ((val_q \u0026gt; Tq) \u0026amp;\u0026amp; (val_w \u0026gt; Tw)) { // ‰ªéÂÄôÈÄâÁÇπ‰∏≠ÈÄâÂá∫ÊûÅÂÄºÁÇπ  if (val_w \u0026gt; max_weight) { max_i = i + m; max_j = j + n; max_weight = val_w; flag = 1; } } } } if (flag)\t// Ëã•ÊúâÊûÅÂÄºÁÇπÔºåÂàôÂ≠òÂÇ®Âú® keyPoints ‰∏≠  keyPoints.push_back(KeyPoint((float)max_j, (float)max_i, 2)); } } } ÊèêÂèñÁªìÊûú ÂØÜÂØÜÈ∫ªÈ∫ªÔºàËæ£ÁúºÁùõÔºâÔºåÊÑüËßâÊïàÊûú‰∏ÄËà¨ Harris ÁÆóÂ≠ê Âü∫Êú¨ÂéüÁêÜ Harris ËßíÁÇπÊèêÂèñÁÆóÊ≥ïÊòØ Chris Harris Âíå Mike Stephens Âú® H.Moravec ÁÆóÊ≥ïÁöÑÂü∫Á°Ä‰∏äÂèëÂ±ïÂá∫ÁöÑÈÄöËøáËá™Áõ∏ÂÖ≥Áü©ÈòµÁöÑËßíÁÇπÊèêÂèñÁÆóÊ≥ïÔºåÂèàÁß∞ Plessey ÁÆóÊ≥ï„ÄÇËøôÁßçÁÆóÂ≠êÂèó‰ø°Âè∑Â§ÑÁêÜ‰∏≠Ëá™Áõ∏ÂÖ≥ÂáΩÊï∞ÁöÑÂêØÂèëÔºåÁªôÂá∫‰∏éËá™Áõ∏ÂÖ≥ÂáΩÊï∞Áõ∏ËÅîÁ≥ªÁöÑÁü©Èòµ $M$„ÄÇ$M$ ÈòµÁöÑÁâπÂæÅÂÄºÊòØËá™Áõ∏ÂÖ≥ÂáΩÊï∞ÁöÑ‰∏ÄÈò∂Êõ≤ÁéáÔºåÂ¶ÇÊûú‰∏§‰∏™Êõ≤ÁéáÂÄºÈÉΩÈ´òÔºåÈÇ£‰πàÂ∞±ËÆ§‰∏∫ËØ•ÁÇπÊòØÁâπÂæÅËßíÁÇπ„ÄÇ\nÂØπ‰∫éÂõæÂÉè $I(x, y)$ÔºåÂÖ∂Âú®ÊâÄÊúâÊñπÂêë‰∏ä‰ΩçÁßª $(u, v)$ ÂêéÁöÑËá™Áõ∏‰ººÊÄßÂèØË°®Á§∫‰∏∫Ôºö\n$$ E(u,v) = \\sum_{x,y} \\underbrace{w(x,y)}\\text{window function} , [\\underbrace{I(x+u,y+v)}\\text{shifted intensity}-\\underbrace{I(x,y)}_\\text{intensity}]^2 $$\nÂÖ∂‰∏≠Á™óÂè£ÂáΩÊï∞ $w(x, y)$ Êó¢ÂèØ‰ª•ÊòØÂ∏∏Êï∞Ôºå‰πüÂèØ‰ª•ÊòØÈ´òÊñØÂáΩÊï∞„ÄÇ\nÊ†πÊçÆÊ≥∞ÂãíÂ±ïÂºÄÔºåÂØπÂõæÂÉè $I(x, y)$ Âú®Âπ≥Áßª $(u, v)$ ÂêéËøõË°å‰∏ÄÈò∂Ëøë‰ººÔºö\n$$ \\begin{align} I(x+u,y+v) \u0026amp;= I(x, y) + I_x(x, y)u + I_y(x, y)v + O(u^2, v^2) \\[2ex] \u0026amp;\\approx I(x, y) + I_x(x, y)u + I_y(x, y)v \\end{align} $$\nÂÖ∂‰∏≠Ôºå$I_x$Ôºå$I_y$ ÊòØÂõæÂÉè $I(x, y)$ ÁöÑÂÅèÂØºÊï∞„ÄÇÂõ†Ê≠§ÂèØ‰ª•Êé®ÂæóÔºö\n$$ E(u, v) \\approx \\sum_w[I_x(x, y)u + I_y(x, y)v]^2 = \\begin{bmatrix}u \u0026amp; v\\end{bmatrix} M \\begin{bmatrix} u \\ v \\end{bmatrix} $$\nÂÖ∂‰∏≠\n$$ M = \\sum_{x, y} w(x, y) \\begin{bmatrix}I_xI_x \u0026amp; I_xI_y \\ I_xI_y \u0026amp; I_yI_y\\end{bmatrix} = \\begin{bmatrix}\\sum_wI_x^2 \u0026amp; \\sum_wI_xI_y \\ \\sum_wI_xI_y \u0026amp; \\sum_wI_y^2\\end{bmatrix} = \\begin{bmatrix}A \u0026amp; C \\ C \u0026amp; B\\end{bmatrix} $$\nÂç≥ÂõæÂÉè $I(x, y)$ Âú®ÁÇπ $(x, y)$ Â§ÑÂπ≥Áßª $(u, v)$ ÂêéÁöÑËá™Áõ∏ÂÖ≥ÂáΩÊï∞ÂèØ‰ª•Ëøë‰ºº‰∏∫‰∫åÊ¨°È°πÂáΩÊï∞Ôºö\n$$ E(x, y; u, v) \\approx Au^2 + 2Cuv + Bv^2 $$\nÂÖ∂‰∏≠\n$$ A = \\sum_wI_x^2, \\quad B = \\sum_wI_y^2, \\quad C = \\sum_wI_xI_y $$\n‰∫åÊ¨°È°πÂáΩÊï∞Êú¨Ë¥®‰∏äÂ∞±ÊòØ‰∏Ä‰∏™Ê§≠ÂúÜÂáΩÊï∞„ÄÇÊ§≠ÂúÜÁöÑÊâÅÁéá‰∏éÂ∞∫ÂØ∏ÊòØÁî± $M(x, y)$ ÁöÑÁâπÂæÅÂÄº $\\lambda_1$Ôºå$\\lambda_2$ ÂÜ≥ÂÆöÁöÑÔºåÊ§≠ÂúÜÁöÑÊñπÂêëÊòØÁî± $M(x, y)$ ÁöÑÁâπÂæÅÁü¢ÈáèÂÜ≥ÂÆöÁöÑÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫ÔºåÊ§≠ÂúÜÊñπÁ®ã‰∏∫Ôºö\n$$ \\begin{bmatrix}u \u0026amp; v\\end{bmatrix} M(x, y) \\begin{bmatrix}u \\ v\\end{bmatrix} = 1 $$\nÊ§≠ÂúÜÂáΩÊï∞ÁâπÂæÅÂÄº‰∏éÂõæÂÉè‰∏≠ÁöÑËßíÁÇπ„ÄÅÁõ¥Á∫øÔºàËæπÁºòÔºâÂíåÂπ≥Èù¢‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂ¶Ç‰∏ãÂõæÊâÄÁ§∫„ÄÇÂÖ±ÂèØÂàÜ‰∏∫‰∏âÁßçÊÉÖÂÜµÔºö\n ÂõæÂÉè‰∏≠ÁöÑÁõ¥Á∫ø„ÄÇ‰∏Ä‰∏™ÁâπÂæÅÂÄºÂ§ßÔºåÂè¶‰∏Ä‰∏™ÁâπÂæÅÂÄºÂ∞èÔºå$\\lambda_1 \\gg \\lambda_1$ Êàñ $\\lambda_2 \\gg \\lambda_1$„ÄÇËá™Áõ∏ÂÖ≥ÂáΩÊï∞Âú®Êüê‰∏ÄÊñπÂêë‰∏äÂ§ßÔºåÂú®ÂÖ∂‰ªñÊñπÂêë‰∏äÂ∞è„ÄÇ ÂõæÂÉè‰∏≠ÁöÑÂπ≥Èù¢„ÄÇ‰∏§‰∏™ÁâπÂæÅÂÄºÈÉΩÂ∞èÔºå‰∏îËøë‰ººÁõ∏Á≠â„ÄÇËá™Áõ∏ÂÖ≥ÂáΩÊï∞Êï∞ÂÄºÂú®ÂêÑ‰∏™ÊñπÂêë‰∏äÈÉΩÂ∞è„ÄÇ ÂõæÂÉè‰∏≠ÁöÑËßíÁÇπ„ÄÇ‰∏§‰∏™ÁâπÂæÅÂÄºÈÉΩÂ§ßÔºå‰∏îËøë‰ººÁõ∏Á≠â„ÄÇËá™Áõ∏ÂÖ≥ÂáΩÊï∞Êï∞ÂÄºÂú®ÊâÄÊúâÊñπÂêë‰∏äÈÉΩÂ§ß„ÄÇ  Harris ÁÆóÊ≥ïÈÄöËøáËÆ°ÁÆóËßíÁÇπÂìçÂ∫îÂÄº $R$ Êù•Âà§Êñ≠ËßíÁÇπÔºåÂÖ∂ËÆ°ÁÆóÂÖ¨Âºè‰∏∫Ôºö\n$$ R = det(M)-k(trace(M))^2 $$\nÂºè‰∏≠Ôºå$det(M)$ ‰∏∫Áü©Èòµ $M = \\begin{bmatrix}A\u0026amp;C\\\\C\u0026amp;B\\end{bmatrix}$ ÁöÑË°åÂàóÂºèÔºõ$trace(M)$ ‰∏∫Áü©Èòµ $M$ ÁöÑÁõ¥ËøπÔºõ$k$ ‰∏∫ÈªòËÆ§Â∏∏Êï∞ÔºåÂèñÂÄºËåÉÂõ¥‰∏∫ $0.04 \\sim 0.06$„ÄÇ‰∫ãÂÆû‰∏äÔºåÁâπÂæÅÊòØÈöêÂê´Âú® $det(M)$ Âíå $trace(M)$ ‰∏≠ÁöÑÔºåÂõ†‰∏∫Ôºö\n$$ det(M) = \\lambda_1\\lambda_2 = AB - C^2 \\[2ex] trace(M) = \\lambda_1 + \\lambda_2 = A + B $$\nÊ†πÁªù‰∏äËø∞ËÆ®ËÆ∫ÔºåHarris ËßíÁÇπÊ£ÄÊµãÁÆóÊ≥ïÁöÑÊ≠•È™§ÂèØ‰ª•ÊÄªÁªì‰∏∫Ôºö\n1. ËÆ°ÁÆóÂõæÂÉè $I(x, y)$ Âú® $x$ Âíå $y$ ÊñπÂêë‰∏äÁöÑÊ¢ØÂ∫¶ $I_x$„ÄÅ$I_y$Ôºö\n$$ I_x = \\frac{\\partial I}{\\partial x} = I \\bigotimes (-1 \\ 0 \\ 1), \\quad I_y = \\frac{\\partial I}{\\partial y} = I \\bigotimes (-1 \\ 0 \\ 1)^T $$\n2. ËÆ°ÁÆóÂõæÂÉè‰∏§‰∏™ÊñπÂêëÊ¢ØÂ∫¶ÁöÑ‰πòÁßØ„ÄÇ\n$$ I_x^2=I_x \\cdot I_x, \\quad I_y^2 = I_y \\cdot I_y, \\quad I_xI_y = I_x \\cdot I_y $$\n**3. ‰ΩøÁî®È´òÊñØÂáΩÊï∞ÂØπ $I_x^2$„ÄÅ$I_y^2$ Âíå $I_xI_y$ ËøõË°åÈ´òÊñØÊª§Ê≥¢ÔºåÁîüÊàêÁü©Èòµ $M$ ÁöÑÂÖÉÁ¥† $A$„ÄÅ$B$ Âíå $C$„ÄÇ**ËøôÈáåÔºåÈ´òÊñØÂç∑ÁßØÊ®°ÊùøÁöÑ $\\sigma$ Âèñ $0.3 \\sim 0.9$„ÄÇ\n$$ A = G(\\tilde{s}) \\bigotimes I_x^2, \\quad B = G(\\tilde{s}) \\bigotimes I_y^2, \\quad C = G(\\tilde{s}) \\bigotimes I_xI_y $$\n4. ËÆ°ÁÆóÊØè‰∏™ÂÉèÁ¥†ÁöÑ Harris ÂìçÂ∫îÂÄº $R$„ÄÇ\n$$ R = det(M)-k(trace(M))^2 $$\n5. Âú® $3 \\times 3$ Êàñ $5 \\times 5$ ÁöÑÈ¢ÜÂüüÂÜÖËøõË°åÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºåÂ±ÄÈÉ®ÊûÅÂÄºÁÇπÂç≥‰∏∫ÂõæÂÉè‰∏≠ÁöÑËßíÁÇπ„ÄÇ\n‰ª£Á†ÅÂÆûÁé∞ void Harris(InputArray image, vector\u0026lt;KeyPoint\u0026gt; \u0026amp; keyPoints) { Mat img = image.getMat(); if (img.channels() == 3) cvtColor(img, img, COLOR_BGR2GRAY); img.convertTo(img, CV_64F); Mat xKernel = (Mat_\u0026lt;double\u0026gt;(1, 3) \u0026lt;\u0026lt; -1, 0, 1); Mat yKernel = xKernel.t(); Mat gX, gY; // filter2D: Convolves an image with the kernel.  filter2D(img, gX, CV_64F, xKernel);\t// x ÊñπÂêëÊ¢ØÂ∫¶  filter2D(img, gY, CV_64F, yKernel);\t// y ÊñπÂêëÊ¢ØÂ∫¶  Mat gX2, gY2, gXY; gX2 = gX.mul(gX);\t// gX2 = gX * gX  gY2 = gY.mul(gY);\t// gY2 = gY * gY  gXY = gX.mul(gY);\t// gXY = gX * gY  // getGaussianKernel: Returns Gaussian filter coefficients.  // Aperture size: 7; œÉ: 0.9  Mat gaussKernel = getGaussianKernel(5, 0.9); // ÂØπÊ¢ØÂ∫¶ÂÄºËøõË°åÈ´òÊñØÊª§Ê≥¢  filter2D(gX2, gX2, CV_64F, gaussKernel); filter2D(gY2, gY2, CV_64F, gaussKernel); filter2D(gXY, gXY, CV_64F, gaussKernel); Mat R(img.size(), img.type()); // R: ÊØè‰∏™ÂÉèÁ¥†ÁöÑ Harris ÂìçÂ∫îÂÄº  double k = 0.04;\t// k: ÈªòËÆ§Â∏∏Êï∞  double detM, trM;\t// det(M): Ë°åÂàóÂºèÔºåtr(M): Ëøπ  for (int i = 0; i \u0026lt; img.rows; i++) { for (int j = 0; j \u0026lt; img.cols; j++) { // ÂàÜÂà´ËÆ°ÁÆóÊØè‰∏™ÂÉèÁ¥†ÁöÑ Harris ÂìçÂ∫îÂÄº  detM = gX2.at\u0026lt;double\u0026gt;(i, j) * gX2.at\u0026lt;double\u0026gt;(i, j) - gXY.at\u0026lt;double\u0026gt;(i, j) * gXY.at\u0026lt;double\u0026gt;(i, j); trM = gX2.at\u0026lt;double\u0026gt;(i, j) + gY2.at\u0026lt;double\u0026gt;(i, j); R.at\u0026lt;double\u0026gt;(i, j) = detM - k * trM * trM; } } double maxStrength;\t// ÂõæÂÉè‰∏≠ÁöÑÊúÄÂ§ßÂÄº  // minMaxLoc: Finds the global minimum and maximum in an array.  minMaxLoc(R, NULL, \u0026amp;maxStrength); Mat dilated, localMax; // dilate: Dilates an image by using a specific structuring element.  // ÈªòËÆ§ 3 √ó 3 ÂÜÖÊ†∏ËÜ®ËÉÄÔºåËÜ®ËÉÄ‰πãÂêéÈô§Â±ÄÈÉ®ÊúÄÂ§ßÂÄº‰∏éÂéüÊù•Áõ∏ÂêåÔºåÂÖ∂‰ªñÈùûÂ±ÄÈÉ®ÊúÄÂ§ßÂÄºÂùáË¢´ËØ•ÈÇªÂüüÂÜÖÁöÑÊúÄÂ§ßÂÄºÁÇπÂèñ‰ª£  dilate(R, dilated, Mat()); // compare: Performs the per-element comparison of two arrays or an array and scalar value.  // ‰∏éÂéüÂõæÁõ∏ÊØîÔºåÊâæÂá∫ÂÄºÁõ∏ÂêåÁöÑÔºàCMP_EQÔºâÁÇπÔºåËøô‰∫õÁÇπÈÉΩÊòØÂ±ÄÈÉ®ÊúÄÂ§ßÂÄºÁÇπÔºå‰øùÂ≠òÂà∞ localMax ‰∏≠  compare(R, dilated, localMax, CMP_EQ); Mat cornerMap; double qualityLevel = 0.01; double threshold = qualityLevel * maxStrength; cornerMap = R \u0026gt; threshold; bitwise_and(cornerMap, localMax, cornerMap); // ÈÅçÂéÜÊâæÂá∫‰øùÁïôÁöÑÊûÅÂÄºÁÇπ  Mat_\u0026lt;uchar\u0026gt;::const_iterator it = cornerMap.begin\u0026lt;uchar\u0026gt;(); Mat_\u0026lt;uchar\u0026gt;::const_iterator itd = cornerMap.end\u0026lt;uchar\u0026gt;(); for (int i = 0; it != itd; it++, i++) { if (*it) keyPoints.push_back(KeyPoint((float)(i % img.cols), (float)(i / img.cols), 2)); } }  **Ê≥®Ôºö**Âú® OpenCV ‰πüÊúâÁé∞ÊàêÁöÑ cornerHarris() ÂáΩÊï∞ËÆ°ÁÆóËßíÁÇπÔºåÂèØ‰ª•Áõ¥Êé•Ë∞ÉÁî®„ÄÇ\n ÊèêÂèñÁªìÊûú ÊïàÊûú‰∏çÈîôÔºå‰∏é Moravec ÁÆóÂ≠êÊØîËæÉÊé•Ëøë  ÂèÇËÄÉÔºö\n$1$. Âº†ÂâëÊ∏ÖÔºåÊΩòÂä±ÔºåÁéãÊ†ëÊ†πÔºå„ÄäÊëÑÂΩ±ÊµãÈáèÂ≠¶ÔºàÁ¨¨‰∫åÁâàÔºâ„Äã\n$2$. H. P. Moravec, Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover, PhD Thesis, Stanford University, Stanford, CA, USA, 1980.\n$3$. OpenCV: Harris Corner Detection\n$4$. HarrisËßíÁÇπ - ‚òÜRonny‰∏∂ - ÂçöÂÆ¢Âõ≠\n ","permalink":"https://fang-lansheng.github.io/posts/2019-04-02-opencv-2/","summary":"Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÔºàComputer VisionÔºâ‰∏éÊëÑÂΩ±ÊµãÈáèÔºàPhotogrammetryÔºâ‰∏≠ÔºåÊàë‰ª¨Âú®‰∏çÂêåÂΩ±ÂÉè‰πãÈó¥ÊâæÂà∞Áõ∏ÂåπÈÖçÁöÑÁâπÂæÅÔºåÂ∑≤Âª∫Á´ã‰∏§ÂπÖÂΩ±ÂÉè‰πãÈó¥ÁöÑËÅîÁ≥ªÔºåÊèêÂèñÂá∫Êàë‰ª¨ÊâÄÈúÄË¶ÅÁöÑ‰ø°ÊÅØ„ÄÇËøô‰∫õÁâπÂæÅ‰∏ªË¶ÅÂàÜ‰∏∫Ôºö\n ËæπÁºòÔºàEdgesÔºâ ËßíÁÇπÔºàCornersÔºâ ÂÖ¥Ë∂£Âå∫Âüü ROIÔºàRegions of InterestÔºâ  ÂÖ∂‰∏≠ÔºåÊèêÂèñÁÇπÁâπÂæÅÁöÑÁÆóÂ≠êÁß∞‰∏∫ÂÖ¥Ë∂£ÁÆóÂ≠êÊàñÊúâÂà©ÁÆóÂ≠êÔºàInterest OperatorÔºâÔºåÂç≥ËøêÁî®ÊüêÁßçÁÆóÊ≥ï‰ªéÂΩ±ÂÉèÁßçÊèêÂèñÊàë‰ª¨ÊÑüÂÖ¥Ë∂£ÁöÑ„ÄÅÊúâÂà©‰∫éÊüêÁßçÁõÆÁöÑÁöÑÁÇπ„ÄÇ\n‰∫∫ÁúºÂØπËßíÁÇπÁöÑËØÜÂà´ÈÄöÂ∏∏ÊòØÂú®‰∏Ä‰∏™Â±ÄÈÉ®ÁöÑÂ∞èÂå∫ÂüüÊàñÂ∞èÁ™óÂè£ÂÜÖÂÆåÊàêÁöÑ„ÄÇÂ¶ÇÊûúÂú®ÂêÑ‰∏™ÊñπÂêë‰∏äÁßªÂä®Ëøô‰∏™ÁâπÂæÅÁöÑÂ∞èÁ™óÂè£ÔºåÁ™óÂè£ÂÜÖÂå∫ÂüüÁöÑÁÅ∞Â∫¶ÂèëÁîü‰∫ÜËæÉÂ§ßÁöÑÂèòÂåñÔºåÈÇ£‰πàÂ∞±ËÆ§‰∏∫Âú®Á™óÂè£ÂÜÖÈÅáÂà∞‰∫ÜËßíÁÇπ„ÄÇÂ¶ÇÊûúËøô‰∏™ÁâπÂÆöÁöÑÁ™óÂè£Âú®ÂõæÂÉèÂêÑ‰∏™ÊñπÈù¢ÁßªÂä®Êó∂ÔºåÁ™óÂè£ÂÜÖÂõæÂÉèÁöÑÁÅ∞Â∫¶Ê≤°ÊúâÂèëÁîüÂèòÂåñÔºåÈÇ£‰πàÁ™óÂè£ÂÜÖÂ∞±‰∏çÂ≠òÂú®ËßíÁÇπÔºõÂ¶ÇÊûúÁ™óÂè£Âú®Êüê‰∏Ä‰∏™ÊñπÂêëÁßªÂä®Êó∂ÔºåÁ™óÂè£ÂÜÖÂõæÂÉèÁöÑÁÅ∞Â∫¶ÂèëÁîü‰∫ÜËæÉÂ§ßÁöÑÂèòÂåñÔºåËÄåÂú®Âè¶‰∏Ä‰∫õÊñπÂêë‰∏äÊ≤°ÊúâÂèëÁîüÂèòÂåñÔºåÈÇ£‰πàÁ™óÂè£ÂÜÖÁöÑÂõæÂÉèÂèØËÉΩÂè™ÊòØ‰∏ÄÊù°Áõ¥Á∫øÁöÑÁ∫øÊÆµ„ÄÇ\nËßíÁÇπÁöÑÁâπÊÆä‰πãÂ§ÑÂú®‰∫éÔºåÂÆÉÊòØ‰∏§Êù°ËæπÁöÑ‰∫§ÁÇπÔºåÊâÄ‰ª•ÂÆÉ‰ª£Ë°®Ëøô‰∏§Êù°ËæπÁöÑÊñπÂêëÂèëÁîüÂèòÂåñÁöÑÁÇπ„ÄÇÂõ†Ê≠§ÔºåÂõæÂÉèÂú®ËØ•ÁÇπÁöÑÊ¢ØÂ∫¶ÔºàÂú®ÂêÑ‰∏™ÊñπÂêë‰∏äÔºâÂÖ∑ÊúâÊòéÊòæÁöÑÂèòÂåñ„ÄÇÂà©Áî®Ëøô‰∏ÄÁÇπÔºåÂâç‰∫∫ÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÁÆóÊ≥ïÔºåÂÖ∂‰∏≠ÊØîËæÉÁªèÂÖ∏ÁöÑÊúâ Moravec ÁÆóÂ≠ê„ÄÅForstner ÁÆóÂ≠ê‰ª•Âèä Harris ÁÆóÂ≠êÁ≠â„ÄÇ\nMoravec ÁÆóÂ≠ê Âü∫Êú¨ÂéüÁêÜ Moravec ‰∫é 1977 Âπ¥ÊèêÂá∫Âà©Áî®ÁÅ∞Â∫¶ÊñπÂ∑ÆÊèêÂèñÁÇπÁâπÂæÅÁöÑÁÆóÂ≠êÔºåÂÖ∂Ê≠•È™§‰∏∫Ôºö\n1. ËÆ°ÁÆóÂêÑÂÉèÂÖÉÁöÑÂÖ¥Ë∂£ÂÄº $IV$ÔºàInterest ValueÔºâ„ÄÇ Âú®‰ª•ÂÉèÁ¥† $(c, r)$ ‰∏∫‰∏≠ÂøÉÁöÑ $w \\times w$ ÁöÑÁ™óÂè£‰∏≠ÔºàÂ¶Ç $5 \\times 5$ ÁöÑÁ™óÂè£ÔºâÔºåËÆ°ÁÆóÂõõ‰∏™ÊñπÂêëÁõ∏ÈÇªÂÉèÁ¥†ÁÅ∞Â∫¶Â∑ÆÁöÑÂπ≥ÊñπÂíåÔºö\n$$ \\begin{cases} V_1 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c+i,r}-g_{c+i+1, r})^2 \\[2ex] V_2 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c+i,r+i}-g_{c+i+1, r+i+1})^2 \\[2ex] V_3 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c,r+i}-g_{c+i+1, r+i+1})^2 \\[2ex] V_4 \u0026amp;=\u0026amp; \\sum_{i = -k}^{k-1}(g_{c+i,r-i}-g_{c+i+1, r-i-1})^2 \\end{cases} $$","title":"OpenCV‚Äî‚ÄîÁÇπÁâπÂæÅÊèêÂèñÁÆóÂ≠êÔºöMoravecÔºåForstner ‰∏é Harris"},{"content":"ËæπÁºòÊ£ÄÊµãÔºàEdge DetectionÔºâÊòØÂõæÂÉèÂ§ÑÁêÜÁöÑÂü∫Á°ÄÂÜÖÂÆπ„ÄÇÊú¨Êñá‰∏≠ÔºåÊàë‰ªéOpenCVÂÆòÁΩë‰∏ä‰∏ãËΩΩ‰∫ÜÊúÄÊñ∞ÁâàÊú¨ÁöÑ OpenCV 4.0.1Ôºà2018-12-22ÔºâÔºåÂÄüÂä©ÂÆòÊñπÊñáÊ°£ÂíåÁΩëÁªúÊïôÁ®ãÂÆåÊàê‰∫ÜÁéØÂ¢ÉÈÖçÁΩÆ‰∏éÊµãËØïÔºåÂÖ∑‰ΩìÊ≠•È™§‰∏çÂÜçËµòËø∞„ÄÇ\nÂú® OpenCV ‰∏≠ÂèØÁî®‰∫éËæπÁºòÊ£ÄÊµãÁöÑÁÆóÂ≠ê‰∏ªË¶ÅÊúâÔºö\n Canny ÁÆóÂ≠ê Sobel ÁÆóÂ≠ê Laplace ÁÆóÂ≠ê   Canny ÁÆóÂ≠ê ÁêÜËÆ∫ Canny ÁÆóÊ≥ïÊòØÁî± John F. Canny ‰∫é 1987 Âπ¥Âú® A computational approach to edge detection ‰∏ÄÊñáÊèêÂá∫Êù•ÁöÑÔºåÂÆÉÊó®Âú®Êª°Ë∂≥‰∏â‰∏™‰∏ªË¶ÅÊ†áÂáÜÔºö\n **‰ΩéÈîôËØØÁéáÔºàLow error rateÔºâÔºö**ÊÑèÂë≥ÁùÄÂè™Ê£ÄÊµãÂÆûÈôÖÂ≠òÂú®ÁöÑËæπÁºò„ÄÇ **È´òÂÆö‰ΩçÊÄßÔºàGood localizationÔºâÔºö**Â∞ÜÊ£ÄÊµãÂà∞ÁöÑËæπÁºòÂÉèÁ¥†‰∏éÂÆûÈôÖËæπÁºòÂÉèÁ¥†‰πãÈó¥ÁöÑË∑ùÁ¶ªÊúÄÂ∞èÂåñ„ÄÇ **ÊúÄÂ∞èÂìçÂ∫îÔºàMinimal responseÔºâÔºö**ÊØè‰∏™ËæπÁºòÂè™Êúâ‰∏Ä‰∏™Ê£ÄÊµãÂô®ÂìçÂ∫î„ÄÇ  Canny ÁÆóÂ≠êÁöÑÊ≠•È™§Â¶Ç‰∏ãÔºö\n Êª§Êéâ‰ªª‰ΩïÂô™Â£∞„ÄÇËøô‰∏ÄËøáÁ®ãÂ∞Ü‰ΩøÁî®È´òÊñØÊª§Ê≥¢Âô®„ÄÇ‰æãÂ¶ÇÔºå‰∏Ä‰∏™ 5√ó5 ÁöÑÈ´òÊñØÂÜÖÊ†∏Â¶Ç‰∏ãÊâÄÁ§∫Ôºö  $$ K = \\dfrac{1}{159}\\begin{bmatrix} 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; 4 \u0026amp; 2 \\ 4 \u0026amp; 9 \u0026amp; 12 \u0026amp; 9 \u0026amp; 4 \\ 5 \u0026amp; 12 \u0026amp; 15 \u0026amp; 12 \u0026amp; 5 \\ 4 \u0026amp; 9 \u0026amp; 12 \u0026amp; 9 \u0026amp; 4 \\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; 4 \u0026amp; 2 \\end{bmatrix} $$\n ËÆ°ÁÆóÂõæÂÉèÁöÑÂº∫Â∫¶Ê¢ØÂ∫¶„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÈÅµÂæ™Á±ª‰ºº‰∫é Sobel ÁÆóÂ≠êÁöÑÊñπÊ≥ïÔºö\n  Âú® $x$ Âíå $y$ ÊñπÂêëÂ∫îÁî®‰∏ÄÁª¥Âç∑ÁßØÊé©Ê®°Ôºö\n$$ G_{x} = \\begin{bmatrix} -1 \u0026amp; 0 \u0026amp; +1 \\ -2 \u0026amp; 0 \u0026amp; +2 \\ -1 \u0026amp; 0 \u0026amp; +1 \\end{bmatrix} $$\n$$ G_{y} = \\begin{bmatrix} -1 \u0026amp; -2 \u0026amp; -1 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ +1 \u0026amp; +2 \u0026amp; +1 \\end{bmatrix} $$\n  ÊâæÂà∞Ê¢ØÂ∫¶Âº∫Â∫¶ÂíåÊñπÂêëÔºö\n$$ \\begin{array}{l} G = \\sqrt{ G_{x}^{2} + G_{y}^{2} } \\ \\theta = \\arctan(\\dfrac{ G_{y} }{ G_{x} }) \\end{array} $$\nÂ∞ÜÊ¢ØÂ∫¶ÊñπÂêëÂõõËàç‰∫îÂÖ•‰∏∫Âõõ‰∏™ÂèØËÉΩÁöÑËßíÂ∫¶‰πã‰∏ÄÔºàÂç≥ 0¬∞Ôºå45¬∞Ôºå90¬∞ Êàñ 135¬∞Ôºâ„ÄÇ\n    ÈùûÊûÅÂ§ßÂÄºÊäëÂà∂„ÄÇÂà†Èô§ÈùûËæπÁºòÈÉ®ÂàÜÁöÑÂÉèÁ¥†Ôºå‰ªÖ‰øùÁïô‰Ωú‰∏∫ÂÄôÈÄâËæπÁºòÁöÑÁªÜÁ∫ø„ÄÇ\n  ÊªûÂêéÈòàÂÄºÔºàHysteresisÔºâ„ÄÇÂ∫îÁî®‰∏§‰∏™ÈòàÂÄºÔºà‰∏äÈôê upper Âíå‰∏ãÈôê lower ÔºâÔºö\n Â¶ÇÊûúÂÉèÁ¥†Ê¢ØÂ∫¶È´ò‰∫é‰∏äÈôêÔºåÂàôÂ∞ÜÂÖ∂ËßÜ‰∏∫ËæπÁºò Â¶ÇÊûúÂÉèÁ¥†Ê¢ØÂ∫¶‰Ωé‰∫é‰∏ãÈôêÔºåÂàôËàçÂºÉ Â¶ÇÊûúÂÉèÁ¥†Ê¢ØÂ∫¶Âú®‰∏§‰∏™ÈòàÂÄº‰πãÈó¥ÔºåÂàô‰ªÖÂΩìÂÆÉËøûÊé•Âà∞È´ò‰∫é‰∏äÈòàÂÄºÁöÑÂÉèÁ¥†Êó∂ÊâçÂ∞ÜÂÖ∂‰Ωú‰∏∫ËæπÁºò   Âª∫ËÆÆÁöÑÈòàÂÄºÔºö$2:1 \\leq upper : lower \\leq 3:1‚Äã$\n   Â∫îÁî® OpenCV ‰∏≠ Canny ÂáΩÊï∞ÁöÑÁî®Ê≥ï‰∏∫Ôºö\nvoid cv::Canny(\tinputArray\timage, OutputArray\tedges, double\tthreshold1, double\tthreshold2, int\tapertureSize = 3, bool\tL2gradient = false )  imageÔºöËæìÂÖ•ÂõæÂÉèÔºåÂç≥Ê∫êÂõæÂÉèÔºåMat Á±ªÂçïÈÄöÈÅì8‰ΩçÂõæÂÉè„ÄÇ edgesÔºöËæìÂá∫ÁöÑËæπÁºòÂõæÔºåÈúÄË¶ÅÂíåÊ∫êÂõæÁâáÊúâ‰∏ÄÊ†∑ÁöÑÂ∞∫ÂØ∏ÂíåÁ±ªÂûã„ÄÇ threshold1ÔºöÁ¨¨‰∏Ä‰∏™ÊªûÂêéÊÄßÈòàÂÄº„ÄÇ threshold2ÔºöÁ¨¨‰∫å‰∏™ÊªûÂêéÊÄßÈòàÂÄº„ÄÇ apertureSizeÔºöË°®Á§∫Â∫îÁî® Sobel ÁÆóÂ≠êÁöÑÂ≠îÂæÑÂ§ßÂ∞èÔºåÂÖ∂ÊúâÈªòËÆ§ÂÄº 3„ÄÇ L2gradientÔºö‰∏Ä‰∏™ËÆ°ÁÆóÂõæÂÉèÊ¢ØÂ∫¶ÂπÖÂÄºÁöÑÊ†áËØÜÔºåÊúâÈªòËÆ§ÂÄº false„ÄÇ  ÂÖ∑‰ΩìÂÆûÁé∞Ôºö\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;opencv2/core.hpp\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt; using namespace cv; int main() { Mat src, src_gray; src = imread(\u0026#34;lenna.tif\u0026#34;);\t// ËØªÂèñÂõæÂÉè \tif( src.empty() ) { printf(\u0026#34;Error: can not open the image.\\n\u0026#34;); return 1; } cvtColor(src, src_gray, COLOR_BGR2GRAY); // Canny ÁÆóÂ≠ê \tMat dst_canny = src.clone();\t// ÂàõÂª∫ÂõæÂÉèÂâØÊú¨ \tCanny(src, dst_canny, 150, 100); Mat dst_canny_2, edges_canny; dst_canny_2.create(src.size(), src.type()); blur(src_gray, edges_canny, Size(3, 3));\t// Áî® 3√ó3 ÂÜÖÊ†∏ÈôçÂô™ \tCanny(edges_canny, edges_canny, 3, 3 * 3, 3);\t// ËøêË°å Canny ÁÆóÂ≠ê \tdst_canny_2 = Scalar::all(0); // ‰ΩøÁî® Canny ÁÆóÂ≠êËæìÂá∫ÁöÑËæπÁºòÂõæ edges_canny ‰Ωú‰∏∫Êé©Á†ÅÔºå \t// Êù•Â∞ÜÂéüÂõæ src Êã∑Ë¥ùÂà∞ËæìÂá∫ÂõæÂÉè dst_canny_2 ‰∏≠ \tsrc.copyTo(dst_canny_2, edges_canny); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-ÂéüÂõæ\u0026#34;, src); waitKey(2); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-CannyÁÆóÂ≠ê\u0026#34;, dst_canny); waitKey(1); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-CannyÁÆóÂ≠ê„ÄêÈ´òÁ∫ßÂÆûÁé∞„Äë\u0026#34;, dst_canny_2); waitKey(0); return 0; } ÁªìÊûúÂ¶Ç‰∏ãÔºö\nSobel ÁÆóÂ≠ê ÁêÜËÆ∫  Sobel ÁÆóÂ≠êÊòØ‰∏Ä‰∏™Á¶ªÊï£ÂæÆÂàÜÁÆóÂ≠ê„ÄÇÂÆÉËÆ°ÁÆóÂõæÂÉèÂº∫Â∫¶ÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄº„ÄÇ Sobel ÁÆóÂ≠êÁªìÂêà‰∫ÜÈ´òÊñØÂπ≥ÊªëÂíåÂ∑ÆÂàÜ„ÄÇ  ÂÅáËÆæË¶ÅÊìç‰ΩúÁöÑÂõæÂÉè‰∏∫ $I$Ôºö\n  ÂàÜÂà´ËÆ°ÁÆó‰∏§‰∏™ÊñπÂêëÁöÑÂØºÊï∞Ôºö\n  **Ê∞¥Âπ≥ÂèòÂåñÔºö**Â∞Ü $I$ ‰∏é‰∏Ä‰∏™Â•áÊï∞Â§ßÂ∞èÔºàsizeÔºâÁöÑÂÜÖÊ†∏ $G_x$ ËøõË°åÂç∑ÁßØ„ÄÇÊØîÂ¶ÇÔºåÂΩìÂÜÖÊ†∏Â§ßÂ∞è‰∏∫ 3 Êó∂Ôºå$G_x$ ‰∏∫Ôºö\n$$ G_{x} = \\begin{bmatrix} -1 \u0026amp; 0 \u0026amp; +1 \\ -2 \u0026amp; 0 \u0026amp; +2 \\ -1 \u0026amp; 0 \u0026amp; +1 \\end{bmatrix} * I $$\n  **ÂûÇÁõ¥ÂèòÂåñÔºö**Â∞Ü $I$ ‰∏é‰∏Ä‰∏™Â•áÊï∞Â§ßÂ∞èÔºàsizeÔºâÁöÑÂÜÖÊ†∏ $G_y$ ËøõË°åÂç∑ÁßØ„ÄÇÊØîÂ¶ÇÔºåÂΩìÂÜÖÊ†∏Â§ßÂ∞è‰∏∫ 3 Êó∂Ôºå$G_y$ ‰∏∫Ôºö\n$$ G_{y} = \\begin{bmatrix} -1 \u0026amp; -2 \u0026amp; -1 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ +1 \u0026amp; +2 \u0026amp; +1 \\end{bmatrix} * I $$\n    Âú®ÂõæÂÉèÁöÑÊØè‰∏™ÁÇπÔºåÊàë‰ª¨ÈÄöËøáÁªÑÂêà‰∏äÈù¢ÁöÑ‰∏§‰∏™ÁªìÊûúÊù•ËÆ°ÁÆóËØ•ÁÇπÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄºÔºö\n$$ G = \\sqrt{ G_{x}^{2} + G_{y}^{2} } $$\nËôΩÁÑ∂ÊúâÊó∂‰ºö‰ΩøÁî®‰ª•‰∏ãÊõ¥ÁÆÄÂçïÁöÑÂÖ¨ÂºèÔºö\n$$ G = |G_{x}| + |G_{y}| $$\n   Ê≥®ÊÑèÔºö\nÂΩìÂÜÖÊ†∏ÁöÑÂ§ßÂ∞èÔºàsizeÔºâ‰∏∫ 3 Êó∂Ôºå‰∏äËø∞ Sobel ÂÜÖÊ†∏ÂèØËÉΩ‰ºöÊúâÂæàÊòéÊòæÁöÑËØØÂ∑ÆÔºàÊØïÁ´ü Sobel ÁÆóÂ≠êÂè™ÊòØËÆ°ÁÆóÂØºÊï∞ÁöÑËøë‰ººÂÄºÔºâ„ÄÇOpenCV ÈÄöËøá‰ΩøÁî® Scharr() ÂáΩÊï∞Ëß£ÂÜ≥‰∫ÜÂ§ßÂ∞è‰∏∫ 3 ÁöÑÂÜÖÊ†∏Â≠òÂú®ÁöÑËøôÁßçËØØÂ∑Æ„ÄÇËøôÊØîÊ†áÂáÜ Sobel ÂáΩÊï∞Êõ¥Âø´‰ΩÜÊõ¥ÂáÜÁ°Æ„ÄÇÂÖ∑‰ΩìÂÜÖÊ†∏Â¶Ç‰∏ãÔºö\n$$ G_{x} = \\begin{bmatrix} -3 \u0026amp; 0 \u0026amp; +3 \\ -10 \u0026amp; 0 \u0026amp; +10 \\ -3 \u0026amp; 0 \u0026amp; +3 \\end{bmatrix} $$\n$$ G_{y} = \\begin{bmatrix} -3 \u0026amp; -10 \u0026amp; -3 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ +3 \u0026amp; +10 \u0026amp; +3 \\end{bmatrix} $$\n Â∫îÁî® OpenCV ‰∏≠ Sobel ÂáΩÊï∞ÁöÑÁî®Ê≥ï‰∏∫Ôºö\nvoid cv::Sobel( InputArray src, OutputArray dst, int ddepth, int dx, int dy, int ksize = 3, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT )\t  srcÔºöËæìÂÖ•ÂõæÂÉè\n  dstÔºö‰∏é src ÂÖ∑ÊúâÁõ∏ÂêåÂ§ßÂ∞èÂíåÁõ∏ÂêåÈÄöÈÅìÊï∞ÁöÑËæìÂá∫ÂõæÂÉè\n  ddepthÔºöËæìÂá∫ÂõæÂÉèÁöÑÊ∑±Â∫¶ÔºåÊîØÊåÅÂ¶Ç‰∏ã src.depth() Âíå ddepth ÁöÑÁªÑÂêàÔºö\n   Input depth (src.depth()) Output depth (ddepth)     CV_8U -1/CV_16S/CV_32F/CV_64F   CV_16U / CV_16S -1/CV_32F/CV_64F   CV_32F -1/CV_32F/CV_64F   CV_64F -1/CV_64F      dxÔºöx ÊñπÂêë‰∏äÁöÑÂ∑ÆÂàÜÈò∂Êï∞„ÄÇ\n  dyÔºöy ÊñπÂêë‰∏äÁöÑÂ∑ÆÂàÜÈò∂Êï∞„ÄÇ\n  ksizeÔºöSobel ÂÜÖÊ†∏ÁöÑÂ§ßÂ∞èÔºõÂøÖÈ°ªÂèñ 1Ôºå3Ôºå5 Êàñ 7„ÄÇ\n  scaleÔºöËÆ°ÁÆóÂØºÊï∞ÂÄºÊó∂ÂèØÈÄâÁöÑÁº©ÊîæÂõ†Â≠êÔºåÈªòËÆ§ÂÄºÊòØ1ÔºåË°®Á§∫ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÊòØÊ≤°ÊúâÂ∫îÁî®Áº©ÊîæÁöÑ„ÄÇ\n  deltaÔºöË°®Á§∫Âú®ÁªìÊûúÂ≠òÂÖ• dst ‰πãÂâçÂèØÈÄâÁöÑ delta ÂÄº„ÄÇ\n  borderTypeÔºöËæπÁïåÊ®°ÂºèÔºåÈªòËÆ§ÂÄº‰∏∫BORDER_DEFAULT„ÄÇËØ¶ËßÅ BorderTypes„ÄÇ\n  Âõ†‰∏∫ Sobel ÁÆóÂ≠êÁªìÂêà‰∫ÜÈ´òÊñØÂπ≥ÊªëÂíåÂ∑ÆÂàÜÔºåÂõ†Ê≠§‰ºöÂÖ∑ÊúâÊõ¥Â§öÁöÑÊäóÂô™ÊÄß„ÄÇ\nÂÖ∑‰ΩìÂÆûÁé∞Ôºö\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;opencv2/core.hpp\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt; using namespace cv; int main() { Mat src = imread(\u0026#34;lenna.tif\u0026#34;);\t// ËØªÂèñÂõæÂÉè \tif( src.empty() ) { printf(\u0026#34;Error: can not open the image.\\n\u0026#34;); return 1; } Mat src_blur, src_blur_gray, dst_sobel; // È´òÊñØÊª§Ê≥¢Âô®ÂéªÂô™Ôºàkernel size = 3Ôºâ \tGaussianBlur(src, src_blur, Size(3, 3), 0, 0, BORDER_DEFAULT); // Â∞ÜÂõæÂÉèËΩ¨Êç¢‰∏∫ÁÅ∞Â∫¶ÂõæÂÉè \tcvtColor(src_blur, src_blur_gray, COLOR_BGR2GRAY); Mat grad_x, grad_y; Mat abs_grad_x, abs_grad_y; // ÂàÜÂà´Ê±Ç X„ÄÅY ÊñπÂêëÊ¢ØÂ∫¶ \tSobel(src_blur_gray, grad_x, CV_16S, 1, 0, 3, 1, 1, BORDER_DEFAULT);\tSobel(src_blur_gray, grad_y, CV_16S, 0, 1, 3, 1, 1, BORDER_DEFAULT); // ËΩ¨Êç¢Âõû CV_8U \tconvertScaleAbs(grad_x, abs_grad_x); convertScaleAbs(grad_y, abs_grad_y); // ÂêàÂπ∂Ê¢ØÂ∫¶ \taddWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0, dst_sobel); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-ÂéüÂõæ\u0026#34;, src); waitKey(3); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-SobelÁÆóÂ≠ê-XÊñπÂêë\u0026#34;, abs_grad_x); waitKey(2); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-SobelÁÆóÂ≠ê-YÊñπÂêë\u0026#34;, abs_grad_y); waitKey(1); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-SobelÁÆóÂ≠ê-Êï¥‰ΩìÊñπÂêë\u0026#34;, dst_sobel); waitKey(0); return 0; } Laplace ÁÆóÂ≠ê ÁêÜËÆ∫   Ê†πÊçÆÊï∞Â≠óÂõæÂÉèÂ§ÑÁêÜÁõ∏ÂÖ≥Áü•ËØÜÔºåÊàë‰ª¨Áü•ÈÅì‰∫åÈò∂ÂØºÊï∞ÂèØÁî®‰∫éÊ£ÄÊµãËæπÁºò„ÄÇÁî±‰∫éÂõæÂÉèÊòØ‰∫åÁª¥Ôºà2DÔºâÁöÑÔºåÊàë‰ª¨ÈúÄË¶ÅÂú®‰∏§‰∏™ÊñπÂêë‰∏äËøõË°åÊ±ÇÂØº„ÄÇ\n  Laplace ÁÆóÂ≠êÂÆö‰πâ‰∏∫Ôºö\n$$ Laplace(f) = \\dfrac{\\partial^{2} f}{\\partial x^{2}} + \\dfrac{\\partial^{2} f}{\\partial y^{2}} $$\n  Âú® OpenCV ‰∏≠ÔºåLaplace ÁÆóÂ≠êÈÄöËøáÂáΩÊï∞ Laplacian() ÂÆûÁé∞„ÄÇÂÆûÈôÖ‰∏äÔºåÁî±‰∫éÊãâÊôÆÊãâÊñØÁÆóÂ≠êÂà©Áî®‰∫ÜÂõæÂÉèÁöÑÊ¢ØÂ∫¶ÔºåÂõ†Ê≠§ÂÆÉÂú®ÂÜÖÈÉ®Ë∞ÉÁî® Sobel ÁÆóÂ≠êÊù•ÊâßË°åÂÖ∂ËøêÁÆóÔºå\n  Â∫îÁî® OpenCV ‰∏≠ Laplacian() ÂáΩÊï∞ÁöÑÁî®Ê≥ï‰∏∫Ôºö\nvoid cv::Laplacian( InputArray src, OutputArray dst, int ddepth, int ksize = 1, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT )\t srcÔºöËæìÂÖ•ÂõæÂÉè dstÔºö‰∏é src ÂÖ∑ÊúâÁõ∏ÂêåÂ§ßÂ∞èÂíåÁõ∏ÂêåÈÄöÈÅìÊï∞ÁöÑËæìÂá∫ÂõæÂÉè ddepthÔºöËæìÂá∫ÂõæÂÉèÁöÑÊ∑±Â∫¶„ÄÇ ksizeÔºöÁî®‰∫éËÆ°ÁÆó‰∫åÈò∂ÂØºÊï∞ÁöÑÊª§Ê≥¢Âô®ÁöÑÂ≠îÂæÑÂ∞∫ÂØ∏ÔºõÂÆÉÂèØ‰ª•ÊòØ FILTER_SCHARR„ÄÅ1„ÄÅ3„ÄÅ5 Êàñ 7„ÄÇ scaleÔºöËÆ°ÁÆóÊãâÊôÆÊãâÊñØÂÄºÊó∂ÂèØÈÄâÁöÑÊØî‰æãÂõ†Â≠êÔºåÈªòËÆ§ÂÄºÊòØ1ÔºåË°®Á§∫ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÊòØÊ≤°ÊúâÂ∫îÁî®Áº©ÊîæÁöÑ„ÄÇ deltaÔºöË°®Á§∫Âú®ÁªìÊûúÂ≠òÂÖ• dst ‰πãÂâçÂèØÈÄâÁöÑ delta ÂÄº„ÄÇ borderTypeÔºöËæπÁïåÊ®°ÂºèÔºåÈªòËÆ§ÂÄº‰∏∫BORDER_DEFAULT„ÄÇËØ¶ËßÅ BorderTypes„ÄÇ  Laplacian() ÂáΩÊï∞‰∏ªË¶ÅÂà©Áî® Sobel ÁÆóÂ≠êËøõË°åËøêÁÆó„ÄÇÂÆÉÈÄöËøáÂä†‰∏ä Sobel ÁÆóÂ≠êËøêÁÆóÂá∫ÁöÑÂõæÂÉè $x$ ÊñπÂêëÂíå $y$ ÊñπÂêë‰∏äÁöÑÂØºÊï∞ÔºåÊù•ÂæóÂà∞ËæìÂÖ•ÂõæÂÉèÁöÑÊãâÊôÆÊãâÊñØÂèòÊç¢ÁªìÊûú„ÄÇÂΩì ksize == 1 Êó∂ÔºåÊãâÊôÆÊãâÊñØÂáΩÊï∞ÁöÑÊª§Ê≥¢Á™óÂè£‰∏∫Ôºö\n$$ \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\ 1 \u0026amp; -4 \u0026amp; 1 \\ 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nÂÖ∑‰ΩìÂÆûÁé∞Ôºö\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;opencv2/core.hpp\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt; using namespace cv; int main() { // ËØªÂèñÂõæÂÉè \tMat src = imread(\u0026#34;lenna.tif\u0026#34;);\tif( src.empty() ) { printf(\u0026#34;Error: can not open the image.\\n\u0026#34;); return 1; } // Laplace ÁÆóÂ≠ê \tMat src_blur, src_blur_gray; Mat dst_laplace, abs_dst_laplace; // È´òÊñØÊª§Ê≥¢Âô®ÂéªÂô™Ôºàkernel size = 3Ôºâ \tGaussianBlur(src, src_blur, Size(3, 3), 0, 0, BORDER_DEFAULT); // Â∞ÜÂõæÂÉèËΩ¨Êç¢‰∏∫ÁÅ∞Â∫¶ÂõæÂÉè \tcvtColor(src_blur, src_blur_gray, COLOR_BGR2GRAY); // Â∫îÁî® Laplace ÁÆóÂ≠ê \tLaplacian(src_blur_gray, dst_laplace, CV_16S, 3, 1, 0, BORDER_DEFAULT); // ËÆ°ÁÆóÁªùÂØπÂÄºÔºåÂ∞ÜÁªìÊûúËΩ¨Êç¢‰∏∫ 8 ‰Ωç \tconvertScaleAbs(dst_laplace, abs_dst_laplace); imshow(\u0026#34;ËæπÁºòÊèêÂèñ-ÂéüÂõæ\u0026#34;, src); waitKey(1); imshow(\u0026#34;ËæπÁºòÊ£ÄÊµã-LaplaceÁÆóÂ≠ê\u0026#34;, abs_dst_laplace); waitKey(0); return 0; }  ÂèÇËÄÉÔºö\n „ÄêOpenCVÂÖ•Èó®ÊïôÁ®ã‰πãÂçÅ‰∫å„ÄëOpenCVËæπÁºòÊ£ÄÊµã - ÊØõÊòü‰∫ëÔºàÊµÖÂ¢®ÔºâÁöÑ‰∏ìÊ†è - CSDNÂçöÂÆ¢ OpenCV: Image Filtering OpenCV: Image Processing (imgproc module) OpenCV: Canny Edge Detector OpenCV: Sobel Derivatives OpenCV: Laplace Operator   ","permalink":"https://fang-lansheng.github.io/posts/2019-03-19-opencv-1/","summary":"ËæπÁºòÊ£ÄÊµãÔºàEdge DetectionÔºâÊòØÂõæÂÉèÂ§ÑÁêÜÁöÑÂü∫Á°ÄÂÜÖÂÆπ„ÄÇÊú¨Êñá‰∏≠ÔºåÊàë‰ªéOpenCVÂÆòÁΩë‰∏ä‰∏ãËΩΩ‰∫ÜÊúÄÊñ∞ÁâàÊú¨ÁöÑ OpenCV 4.0.1Ôºà2018-12-22ÔºâÔºåÂÄüÂä©ÂÆòÊñπÊñáÊ°£ÂíåÁΩëÁªúÊïôÁ®ãÂÆåÊàê‰∫ÜÁéØÂ¢ÉÈÖçÁΩÆ‰∏éÊµãËØïÔºåÂÖ∑‰ΩìÊ≠•È™§‰∏çÂÜçËµòËø∞„ÄÇ\nÂú® OpenCV ‰∏≠ÂèØÁî®‰∫éËæπÁºòÊ£ÄÊµãÁöÑÁÆóÂ≠ê‰∏ªË¶ÅÊúâÔºö\n Canny ÁÆóÂ≠ê Sobel ÁÆóÂ≠ê Laplace ÁÆóÂ≠ê   Canny ÁÆóÂ≠ê ÁêÜËÆ∫ Canny ÁÆóÊ≥ïÊòØÁî± John F. Canny ‰∫é 1987 Âπ¥Âú® A computational approach to edge detection ‰∏ÄÊñáÊèêÂá∫Êù•ÁöÑÔºåÂÆÉÊó®Âú®Êª°Ë∂≥‰∏â‰∏™‰∏ªË¶ÅÊ†áÂáÜÔºö\n **‰ΩéÈîôËØØÁéáÔºàLow error rateÔºâÔºö**ÊÑèÂë≥ÁùÄÂè™Ê£ÄÊµãÂÆûÈôÖÂ≠òÂú®ÁöÑËæπÁºò„ÄÇ **È´òÂÆö‰ΩçÊÄßÔºàGood localizationÔºâÔºö**Â∞ÜÊ£ÄÊµãÂà∞ÁöÑËæπÁºòÂÉèÁ¥†‰∏éÂÆûÈôÖËæπÁºòÂÉèÁ¥†‰πãÈó¥ÁöÑË∑ùÁ¶ªÊúÄÂ∞èÂåñ„ÄÇ **ÊúÄÂ∞èÂìçÂ∫îÔºàMinimal responseÔºâÔºö**ÊØè‰∏™ËæπÁºòÂè™Êúâ‰∏Ä‰∏™Ê£ÄÊµãÂô®ÂìçÂ∫î„ÄÇ  Canny ÁÆóÂ≠êÁöÑÊ≠•È™§Â¶Ç‰∏ãÔºö\n Êª§Êéâ‰ªª‰ΩïÂô™Â£∞„ÄÇËøô‰∏ÄËøáÁ®ãÂ∞Ü‰ΩøÁî®È´òÊñØÊª§Ê≥¢Âô®„ÄÇ‰æãÂ¶ÇÔºå‰∏Ä‰∏™ 5√ó5 ÁöÑÈ´òÊñØÂÜÖÊ†∏Â¶Ç‰∏ãÊâÄÁ§∫Ôºö  $$ K = \\dfrac{1}{159}\\begin{bmatrix} 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; 4 \u0026amp; 2 \\ 4 \u0026amp; 9 \u0026amp; 12 \u0026amp; 9 \u0026amp; 4 \\ 5 \u0026amp; 12 \u0026amp; 15 \u0026amp; 12 \u0026amp; 5 \\ 4 \u0026amp; 9 \u0026amp; 12 \u0026amp; 9 \u0026amp; 4 \\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; 4 \u0026amp; 2 \\end{bmatrix} $$","title":"OpenCV‚Äî‚ÄîÂÜÖÁΩÆËæπÁºòÊ£ÄÊµãÁÆóÂ≠êÔºöCannyÔºåSobel ‰∏é Laplace"},{"content":" PAT Advanced Level Âà∑È¢òÔºà‰∏ÄÔºâ\nPTA | Á®ãÂ∫èËÆæËÆ°Á±ªÂÆûÈ™åËæÖÂä©ÊïôÂ≠¶Âπ≥Âè∞\n PAT A 1025 Ëß£È¢òÊÄùË∑Ø\nÈ¢òÁõÆÂ§ßÊÑè‰∏∫ÔºöÊúâ NÔºà‚â§100Ôºâ ‰∏™ËÄÉÂú∫ÔºåÊØè‰∏™ËÄÉÂú∫ÊúâËã•Âπ≤Êï∞ÈáèÔºàK‚â§300ÔºâÁöÑËÄÉÁîü„ÄÇÁé∞ÁªôÂá∫ÂêÑ‰∏™ËÄÉÂú∫‰∏≠ËÄÉÁîüÁöÑÂáÜËÄÉËØÅÂè∑‰∏éÂàÜÊï∞ÔºåË¶ÅÊ±ÇÂ∞ÜÊâÄÊúâËÄÉÁîüÊåâÂàÜÊï∞‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫èÔºåÂπ∂ÊåâÈ°∫Â∫èËæìÂá∫ÊâÄÊúâËÄÉÁîüÁöÑÂáÜËÄÉËØÅÂè∑„ÄÅÊéíÂêç„ÄÅËÄÉÂú∫Âè∑ÂèäËÄÉÂú∫ÂÜÖÊéíÂêç„ÄÇ\nÈ¢òÁõÆË¶ÅÊ±ÇÁöÑ‰ø°ÊÅØÔºàÂáÜËÄÉËØÅÂè∑„ÄÅÂàÜÊï∞Á≠âÔºâÂèØ‰ª•Áî®ÁªìÊûÑ‰Ωì Student Êù•Â≠òÊîæ„ÄÇÂØπËÄÉÁîüËøõË°åÊéíÂ∫èÂèØ‰ª•‰ΩøÁî® C++ Ê†áÂáÜÂ∫ì‰∏≠ÁöÑ sort() ÂáΩÊï∞ÔºåÈ¢òÁõÆ‰∏≠ÊåáÂá∫ÁªìÊûúÂ∫îÊåâÁÖßËÄÉÁîüÊéíÂêçÈÄíÂ¢ûÊéíÂ∫èÔºåÁõ∏ÂêåÂàÜÊï∞ÁöÑËÄÉÁîüÊéíÂêçÁõ∏Âêå‰∏îÊåâÁÖßÂáÜËÄÉËØÅÂè∑ÈÄíÂ¢ûÊéíÂ∫è„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÈúÄË¶ÅÂáÜ‰ª¨ÁºñÂÜô‰∏Ä‰∏™Á¨¶ÂêàÈ¢òÁõÆÊù°‰ª∂ÁöÑ cmp ÂáΩÊï∞ÔºåÂÆÉÂ∫îËØ•Êª°Ë∂≥Â¶Ç‰∏ãËßÑÂàôÔºö\n ÂΩìÂàÜÊï∞‰∏çÂêåÊó∂ÔºåÊåâÂàÜÊï∞‰ªéÂ§ßÂà∞Â∞èÊéíÂ∫è Âê¶ÂàôÔºåÊåâÂáÜËÄÉËØÅÂè∑‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫è„ÄÇ  ‰πüÂç≥Ôºö\nbool cmp(Student a, Student b) { if (a.score != b.score) return a.score \u0026gt; b.score; // ÂÖàÊåâÂàÜÊï∞‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫è  else return strcmp(a.id, b.id) \u0026lt; 0;\t// Ëã•ÂàÜÊï∞Áõ∏ÂêåÔºåÂàôÊåâÂáÜËÄÉËØÅÂè∑‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫è } ÁÆóÊ≥ïÊú¨Ë∫´ÂàÜ‰∏∫‰∏âÊ≠•Ôºö\n ÊåâËÄÉÂú∫ËØªÂÖ•ÂêÑËÄÉÁîüÁöÑ‰ø°ÊÅØÔºåÂπ∂ÂØπÂΩìÂâçËØªÂÖ•ËÄÉÂú∫ÁöÑÊâÄÊúâËÄÉÁîüËøõË°åÊéíÂ∫è„ÄÇ‰πãÂêéÂ∞ÜËØ•ËÄÉÂú∫ÁöÑÊâÄÊúâËÄÉÁîüÁöÑÊéíÂêçÂÜôÂÖ•ÂÖ∂ÁªìÊûÑ‰Ωì‰∏≠„ÄÇ ÂØπÊâÄÊúâËÄÉÁîüËøõË°åÊéíÂ∫è ÊåâÁÖßÊéíÂ∫èÔºå‰∏ÄÈÅçËÆ°ÁÆóÊÄªÊéíÂêçÔºå‰∏ÄÈÅçËæìÂá∫‰ø°ÊÅØ  ÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;cstdio\u0026gt;#include \u0026lt;cstring\u0026gt;#include \u0026lt;algorithm\u0026gt; using namespace std; struct Student { char id[15];\t// ÂáÜËÄÉËØÅÂè∑  int score;\t// ÂàÜÊï∞  int location_number;\t// ËÄÉÂú∫Âè∑  int local_rank;\t// ËÄÉÂú∫ÂÜÖÊéíÂêç }stu[30010]; bool cmp(Student a, Student b) { if (a.score != b.score) return a.score \u0026gt; b.score;\t// ÂÖàÊåâÂàÜÊï∞‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫è \telse return strcmp(a.id, b.id) \u0026lt; 0;\t// Ëã•ÂàÜÊï∞Áõ∏ÂêåÔºåÂàôÊåâÂáÜËÄÉËØÅÂè∑‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫è } int main() { int n, k, num = 0;\t// num ‰∏∫ÊÄªËÄÉÁîüÊï∞  scanf(\u0026#34;%d\u0026#34;, \u0026amp;n);\t// n ‰∏∫ËÄÉÂú∫Êï∞  for (int i = 1; i \u0026lt;= n; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;k);\t// ËØ•ËÄÉÂú∫ÂÜÖ‰∫∫Êï∞  for (int j = 0; j \u0026lt; k; j++) { scanf(\u0026#34;%s %d\u0026#34;, stu[num].id, \u0026amp;stu[num].score); stu[num].location_number = i; // ËØ•ËÄÉÁîüÁöÑËÄÉÂú∫Âè∑‰∏∫ i  num++;\t// ÊÄªËÄÉÁîüÊï∞ + 1  } sort(stu + num - k, stu + num, cmp);// Â∞ÜËØ•ËÄÉÂú∫ÂÜÖÁöÑËÄÉÁîüÊéíÂ∫è  stu[num - k].local_rank = 1;\t// ËØ•ËÄÉÂú∫Á¨¨ 1 ÂêçÁöÑ local_rank ‰∏∫ 1  for (int j = num - k + 1; j \u0026lt; num; j++) { if (stu[j].score == stu[j - 1].score) {\t// Â¶ÇÊûú‰∏éÂâç‰∏Ä‰ΩçËÄÉÁîüÂêåÂàÜ  stu[j].local_rank = stu[j - 1].local_rank; } else { stu[j].local_rank = j + 1 - (num - k); } } } printf(\u0026#34;%d\\n\u0026#34;, num);\t// ËæìÂá∫ÊÄªËÄÉÁîüÊï∞  sort(stu, stu + num, cmp);\t// Â∞ÜÊâÄÊúâËÄÉÁîüÊéíÂ∫è  int rank = 1;\t// ÂΩìÂâçËÄÉÁîüÁöÑÊéíÂêç  for (int i = 0; i \u0026lt; num; i++) { if (i \u0026gt; 0 \u0026amp;\u0026amp; stu[i].score != stu[i - 1].score) rank = i + 1;\t// ÂΩìÂâçËÄÉÁîü‰∏é‰∏ä‰∏Ä‰∏™ËÄÉÁîüÂàÜÂ±û‰∏çÂêåÊó∂ÔºåËÆ© rank Êõ¥Êñ∞‰∏∫‰πãÂâçÁöÑ‰∫∫Êï∞(i) + 1  printf(\u0026#34;%s \u0026#34;, stu[i].id); printf(\u0026#34;%d %d %d\\n\u0026#34;, rank, stu[i].location_number, stu[i].local_rank); } return 0; }  Êú™ÂÆåÂæÖÁª≠„ÄÇ„ÄÇ„ÄÇ\n ","permalink":"https://fang-lansheng.github.io/posts/2019-03-16-pat-a-1/","summary":"PAT Advanced Level Âà∑È¢òÔºà‰∏ÄÔºâ\nPTA | Á®ãÂ∫èËÆæËÆ°Á±ªÂÆûÈ™åËæÖÂä©ÊïôÂ≠¶Âπ≥Âè∞\n PAT A 1025 Ëß£È¢òÊÄùË∑Ø\nÈ¢òÁõÆÂ§ßÊÑè‰∏∫ÔºöÊúâ NÔºà‚â§100Ôºâ ‰∏™ËÄÉÂú∫ÔºåÊØè‰∏™ËÄÉÂú∫ÊúâËã•Âπ≤Êï∞ÈáèÔºàK‚â§300ÔºâÁöÑËÄÉÁîü„ÄÇÁé∞ÁªôÂá∫ÂêÑ‰∏™ËÄÉÂú∫‰∏≠ËÄÉÁîüÁöÑÂáÜËÄÉËØÅÂè∑‰∏éÂàÜÊï∞ÔºåË¶ÅÊ±ÇÂ∞ÜÊâÄÊúâËÄÉÁîüÊåâÂàÜÊï∞‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫èÔºåÂπ∂ÊåâÈ°∫Â∫èËæìÂá∫ÊâÄÊúâËÄÉÁîüÁöÑÂáÜËÄÉËØÅÂè∑„ÄÅÊéíÂêç„ÄÅËÄÉÂú∫Âè∑ÂèäËÄÉÂú∫ÂÜÖÊéíÂêç„ÄÇ\nÈ¢òÁõÆË¶ÅÊ±ÇÁöÑ‰ø°ÊÅØÔºàÂáÜËÄÉËØÅÂè∑„ÄÅÂàÜÊï∞Á≠âÔºâÂèØ‰ª•Áî®ÁªìÊûÑ‰Ωì Student Êù•Â≠òÊîæ„ÄÇÂØπËÄÉÁîüËøõË°åÊéíÂ∫èÂèØ‰ª•‰ΩøÁî® C++ Ê†áÂáÜÂ∫ì‰∏≠ÁöÑ sort() ÂáΩÊï∞ÔºåÈ¢òÁõÆ‰∏≠ÊåáÂá∫ÁªìÊûúÂ∫îÊåâÁÖßËÄÉÁîüÊéíÂêçÈÄíÂ¢ûÊéíÂ∫èÔºåÁõ∏ÂêåÂàÜÊï∞ÁöÑËÄÉÁîüÊéíÂêçÁõ∏Âêå‰∏îÊåâÁÖßÂáÜËÄÉËØÅÂè∑ÈÄíÂ¢ûÊéíÂ∫è„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÈúÄË¶ÅÂáÜ‰ª¨ÁºñÂÜô‰∏Ä‰∏™Á¨¶ÂêàÈ¢òÁõÆÊù°‰ª∂ÁöÑ cmp ÂáΩÊï∞ÔºåÂÆÉÂ∫îËØ•Êª°Ë∂≥Â¶Ç‰∏ãËßÑÂàôÔºö\n ÂΩìÂàÜÊï∞‰∏çÂêåÊó∂ÔºåÊåâÂàÜÊï∞‰ªéÂ§ßÂà∞Â∞èÊéíÂ∫è Âê¶ÂàôÔºåÊåâÂáÜËÄÉËØÅÂè∑‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫è„ÄÇ  ‰πüÂç≥Ôºö\nbool cmp(Student a, Student b) { if (a.score != b.score) return a.score \u0026gt; b.score; // ÂÖàÊåâÂàÜÊï∞‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫è  else return strcmp(a.id, b.id) \u0026lt; 0;\t// Ëã•ÂàÜÊï∞Áõ∏ÂêåÔºåÂàôÊåâÂáÜËÄÉËØÅÂè∑‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫è } ÁÆóÊ≥ïÊú¨Ë∫´ÂàÜ‰∏∫‰∏âÊ≠•Ôºö\n ÊåâËÄÉÂú∫ËØªÂÖ•ÂêÑËÄÉÁîüÁöÑ‰ø°ÊÅØÔºåÂπ∂ÂØπÂΩìÂâçËØªÂÖ•ËÄÉÂú∫ÁöÑÊâÄÊúâËÄÉÁîüËøõË°åÊéíÂ∫è„ÄÇ‰πãÂêéÂ∞ÜËØ•ËÄÉÂú∫ÁöÑÊâÄÊúâËÄÉÁîüÁöÑÊéíÂêçÂÜôÂÖ•ÂÖ∂ÁªìÊûÑ‰Ωì‰∏≠„ÄÇ ÂØπÊâÄÊúâËÄÉÁîüËøõË°åÊéíÂ∫è ÊåâÁÖßÊéíÂ∫èÔºå‰∏ÄÈÅçËÆ°ÁÆóÊÄªÊéíÂêçÔºå‰∏ÄÈÅçËæìÂá∫‰ø°ÊÅØ  ÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;cstdio\u0026gt;#include \u0026lt;cstring\u0026gt;#include \u0026lt;algorithm\u0026gt; using namespace std; struct Student { char id[15];\t// ÂáÜËÄÉËØÅÂè∑  int score;\t// ÂàÜÊï∞  int location_number;\t// ËÄÉÂú∫Âè∑  int local_rank;\t// ËÄÉÂú∫ÂÜÖÊéíÂêç }stu[30010]; bool cmp(Student a, Student b) { if (a.","title":"PAT Advanced Level Practice (I)"},{"content":"ÊñêÊ≥¢ÈÇ£Â•ëÊï∞Âàó ÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÔºàFibonacci SequenceÔºâÊòØ‰∏Ä‰∏≤Êï∞Â≠óÔºö\n\\[ (0,) 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, \u0026hellip; \\] ÂæàÂÆπÊòìÁúãÂá∫ÔºåÊØè‰∏Ä‰∏™Êï∞ÔºàÈô§Á¨¨ 1„ÄÅ2 ‰∏™ÔºâÈÉΩÁ≠â‰∫éÂÆÉ‰πãÂâçÁöÑ‰∏§‰∏™Êï∞‰πãÂíå„ÄÇÂõ†Ê≠§ÔºåÁî®ÂÖ¨ÂºèÂèØÂΩíÁ∫≥‰∏∫Ôºö \\[ F_n = \\begin{cases} 0, \u0026amp; n=0 \\\\ 1, \u0026amp; n=1 \\\\ F_{n-1} + F_{n-2}, \u0026amp; n\u0026gt;1 \\end{cases} \\]\nËÆ°ÁÆóÂ§çÊùÇÊÄßÁêÜËÆ∫ Âú®Â∞ÜÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÁî® C++ ‰ª£Á†ÅÂÆûÁé∞Âπ∂ËÆ°ÁÆóÊó∂Èó¥„ÄÅÁ©∫Èó¥Â§çÊùÇÂ∫¶‰πãÂâçÔºåÂÖàË¶Å‰∫ÜËß£‰∏Ä‰∏ã‰ªÄ‰πàÊòØÊâÄË∞ìÁöÑ‚ÄúÂ§çÊùÇÂ∫¶‚Äù\nÊó∂Èó¥Â§çÊùÇÂ∫¶  Âú®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏≠Ôºå ÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÔºàTime ComplexityÔºâÊòØ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂÆÉÂÆöÊÄßÊèèËø∞ÁÆóÊ≥ïÁöÑËøêË°åÊó∂Èó¥„ÄÇËøôÊòØ‰∏Ä‰∏™‰ª£Ë°®ÁÆóÊ≥ïËæìÂÖ•ÂÄºÁöÑÂ≠óÁ¨¶‰∏≤ÁöÑÈïøÂ∫¶ÁöÑÂáΩÊï∞„ÄÇ\n ‰∏Ä‰∏™ËØ≠Âè•ÁöÑÈ¢ëÂ∫¶ÊòØÊåáËØ•ËØ≠Âè•Âú®ÁÆóÊ≥ï‰∏≠Ë¢´ÈáçÂ§çÊâßË°åÁöÑÊ¨°Êï∞„ÄÇÁÆóÊ≥ï‰∏≠ÊâÄÊúâËØ≠Âè•ÁöÑÈ¢ëÂ∫¶‰πãÂíåÂç≥‰∏∫ $T(n)$ÔºåÂÆÉÊòØËØ•ÁÆóÊ≥ïÈóÆÈ¢òËßÑÊ®° $n$ ÁöÑÂáΩÊï∞ÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏ªË¶ÅÂàÜÊûê $T(n)$ ÁöÑÊï∞ÈáèÁ∫ß„ÄÇ ÁÆóÊ≥ïÁöÑÂü∫Êú¨ËøêÁÆóÔºàÊúÄÊ∑±Â±ÇÂæ™ÁéØÂÜÖÁöÑËØ≠Âè•ÔºâÁöÑÈ¢ëÂ∫¶‰∏é $T(n)$ ÂêåÊï∞ÈáèÁ∫ß„ÄÇÂõ†Ê≠§ÈÄöÂ∏∏ÈááÁî®ÁÆóÊ≥ï‰∏≠Âü∫Êú¨ËøêÁÆóÁöÑÈ¢ëÂ∫¶ $f(n)$ Êù•ÂàÜÊûêÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇÂõ†Ê≠§ÔºåÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ËÆ∞‰∏∫Ôºö \\[ T(n) = O(f(n)) \\] Âºè‰∏≠Ôºå$O$ ÁöÑÂê´‰πâÊòØ $T(n)$ ÁöÑÊï∞ÈáèÁ∫ßÔºåÂÖ∂‰∏•Ê†ºÁöÑÊï∞Â≠¶ÂÆö‰πâÊòØÔºöËã• $T(n)$ Âíå $f(n)$ ÊòØÂÆö‰πâÂú®Ê≠£Êï¥Êï∞ÈõÜÂêà‰∏äÁöÑ‰∏§‰∏™ÂáΩÊï∞ÔºåÂàôÂ≠òÂú®Ê≠£Â∏∏Êï∞ $C$ Âíå $n_0$Ôºå‰ΩøÂæóÂΩì $n \\geq n_0$ Êó∂ÔºåÈÉΩÊª°Ë∂≥ $0 \\leq T(n) \\leq Cf(n)$„ÄÇ\nÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏ç‰ªÖ‰æùËµñ‰∫éÈóÆÈ¢òÁöÑËßÑÊ®° $n$Ôºå‰πüÂéªÂÜ≥ÂÆö‰∫éÂæÖËæìÂÖ•Êï∞ÊçÆÁöÑÊÄßË¥®ÔºàÂ¶ÇËæìÂÖ•Êï∞ÊçÆÂÖÉÁ¥†ÁöÑÂàùÂßãÁä∂ÊÄÅÔºâ„ÄÇ\n ***ÊúÄÂùèÊó∂Èó¥Â§çÊùÇÂ∫¶Ôºö***Âú®ÊúÄÂùèÊÉÖÂÜµ‰∏ãÔºåÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇ ***Âπ≥ÂùáÊó∂Èó¥Â§çÊùÇÂ∫¶Ôºö***ÊâÄÊúâÂèØËÉΩËæìÂÖ•Á§∫‰æãÂú®Á≠âÊ¶ÇÁéáÂá∫Áé∞ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁÆóÊ≥ïÁöÑÊúüÊúõËøêË°åÊó∂Èó¥„ÄÇ ***ÊúÄÂ•ΩÊó∂Èó¥Â§çÊùÇÂ∫¶Ôºö***Âú®ÊúÄÂ•ΩÊÉÖÂÜµ‰∏ãÔºåÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇ  ‰∏ÄËà¨ÊÄªÊòØËÄÉËôëÂú®ÊúÄÂùèÊÉÖÂÜµ‰∏ãÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶Ôºå‰ª•‰øùËØÅÁÆóÊ≥ïÁöÑËøêË°åÊó∂Â∏∏‰∏ç‰ºöÊØîÂÆÉÊõ¥Èïø„ÄÇ\nÂú®ÂàÜÊûê‰∏Ä‰∏™Á®ãÂ∫èÁöÑÊó∂Èó¥Â§çÊùÇÊÄßÊó∂ÔºåÊúâ‰ª•‰∏ã‰∏§Êù°ËßÑÂàôÔºö\n(a) Âä†Ê≥ïËßÑÂàôÔºö \\[ T(n) = T_1(n) + T_2(n) = O(f(n)) + O(g(n)) = O(max(f(n), g(n))) \\] (b) ‰πòÊ≥ïËßÑÂàôÔºö \\[ T(n) = T_1(n) \\times T_2(n) = O(f(n)) \\times O(g(n)) = O(f(n) \\times g(n)) \\] Êó∂Èó¥Â§çÊùÇÂ∫¶ÂèØË¢´Êàê‰∏∫ÊòØÊ∏êËøõÁöÑÔºå‰∫¶Âç≥ËÄÉÂØüËæìÂÖ•ÂÄºÂ§ßÂ∞èË∂ãËøë‰∫éÊó†Á©∑Êó∂Ôºà$n\\rightarrow\\infty$ÔºâÁöÑÊÉÖÂÜµ„ÄÇ Â∏∏ËßÅÁöÑÊ∏êËøõÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫Ôºö \\[ O(1) \u0026lt; O(\\log_2 n) \u0026lt; O(n) \u0026lt; O(n\\log_2 n) \u0026lt; O(n^2) \u0026lt; O(n^3) \u0026lt; O(2^n) \u0026lt; O(n!) \u0026lt; O(n^n) \\]\nÁ©∫Èó¥Â§çÊùÇÂ∫¶  ÁÆóÊ≥ïÁöÑÁ©∫Èó¥Â§çÊùÇÂ∫¶ $S(n)$ Ë¢´ÂÆö‰πâ‰∏∫ËØ•ÁÆóÊ≥ïÊâÄËÄóË¥πÁöÑÂ≠òÂÇ®Á©∫Èó¥ÔºåÂÆÉÊòØÈóÆÈ¢òËßÑÊ®° $n$ ÁöÑÂáΩÊï∞„ÄÇÊ∏êËøõÁ©∫Èó¥Â§çÊùÇÂ∫¶‰πüÂ∏∏ÁÆÄÁß∞‰∏∫Á©∫Èó¥Â§çÊùÇÂ∫¶ÔºåËÆ∞‰∏∫ $S(n) = O(g(n))$„ÄÇ\n ‰∏Ä‰∏™Á®ãÂ∫èÈô§ÈúÄË¶ÅÂ≠òÂÇ®Á©∫Èó¥Êù•Â≠òÊîæÊú¨Ë∫´ÊâÄÁî®ÁöÑÊåá‰ª§„ÄÅÂ∏∏Êï∞„ÄÅÂèòÈáèÂíåËæìÂÖ•Êï∞ÊçÆÂ§ñÔºåËøòÈúÄË¶Å‰∏Ä‰∫õÂØπÊï∞ÊçÆËøõË°åÊìç‰ΩúÁöÑÂ∑•‰ΩúÂçïÂÖÉÂíåÂ≠òÂÇ®‰∏∫ÂÆûÁé∞ËÆ°ÁÆóÊú∫ÊâÄÈúÄÁöÑ‰∏Ä‰∫õ‰ø°ÊÅØÁöÑËæÖÂä©Á©∫Èó¥ÔºåËã•ËæìÂÖ•Êï∞ÊçÆÊâÄÂç†Á©∫Èó¥Âè™ÂèñÂÜ≥‰∫éÈóÆÈ¢òÊú¨Ë∫´ËÄå‰∏éÁÆóÊ≥ïÊó†ÂÖ≥ÔºåÂàôÂè™ÈúÄÂàÜÊûêÈô§ËæìÂÖ•ÂíåÁ®ãÂ∫èÂ§ñÁöÑÈ¢ùÂ§ñÁ©∫Èó¥„ÄÇ\nÁÆóÊ≥ïÂéüÂú∞Â∑•‰ΩúÊòØÊåáÁÆóÊ≥ïÊâÄÈúÄÁöÑËæÖÂä©Á©∫Èó¥‰∏∫Â∏∏ÈáèÔºåÂç≥ $O(1)$„ÄÇ\nÈÄíÂΩíÁÆóÊ≥ï ÊúÄÁÆÄÂçïÁöÑÁÆóÊ≥ïÂç≥ÊòØÊ†πÊçÆÂÆö‰πâÂÜôÂá∫ÁöÑÈÄíÂΩíÁÆóÊ≥ïÔºåC++ ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö\nint fibonacci(int n) { if (n \u0026lt; 2) { return n; } return fibonacci(n - 1) + fibonacci(n - 2); } Êó∂Èó¥Â§çÊùÇÂ∫¶Ôºö\nÊàë‰ª¨Áü•ÈÅìÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÊï∞ÂàóÁöÑÈÄíÂΩíÊñπÁ®ãÊòØ $T(n) = T(n-1) + T(n-2) + O(1)$„ÄÇËøôÂ∞±ÊÑèÂë≥ÁùÄÔºåËÆ°ÁÆó fibonacci(n) ÁöÑÊó∂Èó¥Á≠â‰∫éËÆ°ÁÆó fibonacci(n-1) ‰∏é fibonacci(n-2) ÁöÑÊó∂Èó¥„ÄÇÂõ†Ê≠§Êàë‰ª¨ÂæàÂÆπÊòìÂæóÁü•ÔºåÂú®ÈÄíÂΩíÊ±ÇËß£ÊñêÊ≥¢ÈÇ£Â•ëÊï∞Êó∂ÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶ÁöÑ‰∏äÁïå (upper bound) ÊòØ $O(2^n)$„ÄÇ‰ΩÜÈÄöËøáÂú®Êï∞Â≠¶‰∏äÁî®Á∫øÊÄßÈÄíÂΩíÂáΩÊï∞Êù•Ë°®Á§∫ÔºåÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞ Fibonacci ÁöÑ‰∏äÁ°ÆÁïå (tight upper bound or supremum)„ÄÇ\nÁé∞Ê†πÊçÆÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÁöÑÂÆö‰πâÔºö \\[ F(n) = F(n-1) + F(n-2) \\] ÂæóÂà∞ÂÖ∂ÁâπÂæÅÊñπÁ®ã‰∏∫Ôºö \\[ x^2 = x + 1 \\\\\\\\ x^2 - x - 1 = 0 \\] Ëß£ÂæóËØ•‰∫åÊ¨°ÊñπÁ®ãÁöÑÊ†π‰∏∫ $\\alpha_1 = \\frac{1 + \\sqrt{5}}{2},\\ \\alpha_2 = \\frac{1 - \\sqrt{5}}{2}$„ÄÇÁé∞Âú®Êàë‰ª¨ÂæóÂà∞ÁöÑÁ∫øÊÄßÈÄíÂΩíÂáΩÊï∞‰∏∫Ôºö \\[ F(n) = (\\alpha_1)^n + (\\alpha_2)^n \\] ÊâÄ‰ª•ÊñêÊ≥¢ÈÇ£Â•ëÂáΩÊï∞ $F(n) = F(n-1) + F(n-2)$ ÁöÑËß£‰∏∫Ôºö \\[ F(n) = (\\frac{1 + \\sqrt{5}}{2})^n + (\\frac{1 - \\sqrt{5}}{2})^n \\] ÊòæÁÑ∂ÔºåÂú®ËøõË°åÊó†Á©∑Â§ßÔºà$n \\rightarrow \\infty$ÔºâÊ∏êËøõÂàÜÊûê Êó∂Ôºå$T(n)$ ‰∏é $F(n)$ Ë°®Á§∫Âê´‰πâÊòØÁõ∏ÂêåÁöÑÔºåÂèØË¢´Ëøë‰ººÂú∞ÁúãÂÅöÁõ∏Á≠âÔºå‰∫éÊòØÂèØ‰ª•ÂæóÂá∫Ôºö \\[ T(n) = O((\\frac{1 + \\sqrt{5}}{2})^n + (\\frac{1 - \\sqrt{5}}{2})^n) \\] ÊàñËÄÖÂèØ‰ª•ÂÜô‰ΩúÂ¶Ç‰∏ãÂΩ¢Âºè ÔºàÊ†πÊçÆÊ∏êËøõÁ¨¶Âè∑ $O$ (Big O notation)ÔºåÂèØ‰ª•ÂøΩÁï•Êéâ‰ΩéÈò∂È°πÔºâÔºö \\[ T(n) = O((\\frac{1 + \\sqrt{5}}{2})^n) \\\\\\\\ T(n) = O(1.618^n) \\] ËøôÂç≥ÊòØÊñêÊ≥¢ÈÇ£Â•ëÈÄíÂΩíÂáΩÊï∞ÁöÑ‰∏äÁ°ÆÁïåÔºàtight upper boundÔºâÔºåÂêåÊó∂Êàë‰ª¨‰πüÂèØ‰ª•Áü•ÈÅìÔºåÈÄíÂΩíÁÆóÊ≥ïÁ°ÆÂàáÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ $O(1.618^n)$„ÄÇ\n**Á©∫Èó¥Â§çÊùÇÂ∫¶Ôºö**ÂáΩÊï∞ÊØèÊ¨°ÊâßË°åÈÉΩ‰ºöË∞ÉÁî®‰∏ÄÊ¨°Â†ÜÊ†àÔºåÊïÖÁ©∫Èó¥Â§çÊùÇÂ∫¶‰∏∫ $O(n)$„ÄÇ\nÂæ™ÁéØÁÆóÊ≥ï Âæ™ÁéØÁÆóÊ≥ïÁöÑÊÄùË∑Ø‰∏∫Ôºö\n ÂΩì $n = 0$ Êó∂ÔºåËøîÂõû $F_0$ ÁöÑÂÄº $0$ÔºàÊ≠§Êó∂Ôºå$F_{n+1}=F_1=1$Ôºå$F_n=F_0=0$ ÁöÑÂÄºÂ∑≤ÁªôÂá∫ ÂΩì $n \u0026gt; 0\\ (1, 2, \u0026hellip;, \\infty)$ Êó∂  Áî®‰∏Ä‰∏™Êï∞ b Â≠òÊîæ $F_n$ ÁöÑÂÄºÔºåÂè¶‰∏Ä‰∏™Êï∞ a Â≠òÊîæ $F_{n+1}$ ÁöÑÂÄº ÊâßË°å‰∏ÄÊ¨°Âæ™ÁéØÔºåa + b ‰∏∫Êñ∞ÁöÑ $F_{n+1}$ ÁöÑÂÄºÔºåÂéüÊúâÁöÑ a ÁöÑÂÄºÂç≥‰∏∫Êñ∞ÁöÑ $F_n$ÔºåÂÜçÂ∞ÜËøô‰∏§‰∏™ÂÄºÂàÜÂà´ËµãÁªô‰∏§‰∏™ÂØπÂ∫îÁöÑÂèòÈáèÔºàtemp ÂÖÖÂΩìÂ™í‰ªãÔºâ ÊåâÊ≠§ÂæÄÂ§çÔºåÁõ¥Âà∞Âæ™ÁéØÁªìÊùü    int fibonacci(int n) { int a = 1, b = 0, temp; while (n \u0026gt; 0) { temp = a; a = a + b; b = temp; n--; } return b; } **Êó∂Èó¥Â§çÊùÇÂ∫¶Ôºö**ÊÄùË∑ØÂæàÊ∏ÖÊô∞ÔºåÂæ™ÁéØÊ¨°Êï∞‰∏é $n$ Á∫øÊÄßÊ≠£Áõ∏ÂÖ≥ÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ $O(n)$ÔºåËæÉ‰πãÈÄíÂΩíÁÆóÊ≥ïÂ•ΩÂæàÂ§ö„ÄÇ\n**Á©∫Èó¥Â§çÊùÇÂ∫¶Ôºö**ÁÆóÊ≥ïÊâÄÈúÄË¶ÅÁöÑËæÖÂä©Á©∫Èó¥‰∏∫Â∏∏ÈáèÔºåÊïÖÁ©∫Èó¥Â§çÊùÇÂ∫¶‰∏∫ $O(1)$„ÄÇ\nÂ∞æÈÄíÂΩíÁÆóÊ≥ï Â∞æÈÄíÂΩíÁÆóÊ≥ïÁöÑÊÄùË∑Ø‰∏éÂæ™ÁéØÁÆóÊ≥ïÁõ∏‰ººÔºåË∞ÉÁî®Ê†ºÂºè‰∏∫ fibonacci(0, 1, n) Ôºö\nint fibonacci(int first, int second, int n) { if (n \u0026lt; 2) { return n; } else if (n == 2) { return first + second; } return fibonacci(second, first + second, n - 1); } **Êó∂Èó¥Â§çÊùÇÂ∫¶Ôºö**ÈÄíÂΩíÊ¨°Êï∞‰∏∫ $n$ÔºåÊïÖ‰∏∫ $O(n)$„ÄÇ\n**Á©∫Èó¥Â§çÊùÇÂ∫¶Ôºö**ÂáΩÊï∞ÊØè‰∏ÄÊ¨°ÊâßË°åÈÉΩ‰ºöË∞ÉÁî®‰∏ÄÊ¨°Â†ÜÊ†àÔºåÊïÖ‰∏∫ $O(n)$„ÄÇ\nÁü©ÈòµÁÆóÊ≥ï ÊØîËæÉ‰∏äËø∞Âá†ÁßçÁÆóÊ≥ïÔºåÂèØ‰ª•ÁúãÂá∫ÔºåÂÆÉ‰ª¨ÈÉΩÂà©Áî®‰∫Ü**ÂàÜÊ≤ªÔºàdivide and conquerÔºâ**ÁöÑÊÄùÊÉ≥‚Äî‚Äî**Â∞ÜÂéüÈóÆÈ¢òÂàíÂàÜÊàêËã•Âπ≤‰∏™ËßÑÊ®°ËæÉÂ∞èËÄåÁªìÊûÑ‰∏éÂéüÈóÆÈ¢òÁõ∏ÂêåÊàñÁõ∏‰ººÁöÑÂ≠êÈóÆÈ¢òÔºåÁÑ∂ÂêéÂàÜÂà´Ëß£ÂÜ≥Ëøô‰∫õÂ≠êÈóÆÈ¢òÔºåÊúÄÂêéÂêàÂπ∂Â≠êÈóÆÈ¢òÁöÑËß£ÔºåÂç≥ÂèØÂæóÂà∞‰∏∫ÂéüÈóÆÈ¢òÁöÑËß£„ÄÇ**Âè™‰∏çËøáÂå∫Âà´Âú®‰∫éÊòØËøõË°åÊôÆÈÄöÂú∞ÂàÜÊ≤ªËøòÊòØÂä®ÊÄÅËßÑÂàí„ÄÇ\nÊàë‰ª¨Â∞ÜÊñêÊ≥¢ÈÇ£Â•ëÊï∞Âàó‰∏≠Áõ∏ÈÇªÁöÑ‰∏§È°π $F(n)$ Âíå $F(n-1)$ ÂÜôÊàê‰∏Ä‰∏™ $2\\times 1$ ÁöÑÁü©ÈòµÔºåÈÄöËøáËÆ°ÁÆóÂèØ‰ª•ÂæóÂà∞‰ª•‰∏ãÁªìËÆ∫Ôºö \\[ \\begin{bmatrix} F_n \\\\ F_{n-1} \\end{bmatrix} = \\begin{bmatrix} F_{n-1} + F_{n-2} \\\\ F_{n-1} \\end{bmatrix} = \\begin{bmatrix} 1 \\times F_{n-1}+1 \\times F_{n-2} \\\\ 1\\times F_{n-1}+0 \\times F_{n-2}\\end{bmatrix} = \\begin{bmatrix}1\u0026amp;1\\\\1\u0026amp;0\\end{bmatrix}\\times\\begin{bmatrix}F_{n-1}\\\\F_{n-2}\\end{bmatrix}\n\\] Êé®ÂæóÔºö \\[ \\begin{bmatrix}F_n\\\\F_{n-1}\\end{bmatrix} =\\begin{bmatrix}1\u0026amp;1\\\\1\u0026amp;0\\end{bmatrix}^{n-1}\\times\\begin{bmatrix}F_{1}\\\\F_{0}\\end{bmatrix} =\\begin{bmatrix}1\u0026amp;1\\\\1\u0026amp;0\\end{bmatrix}^{n-1}\\times\\begin{bmatrix}1\\\\0\\end{bmatrix} \\] Âõ†Ê≠§Ë¶ÅÊ±ÇÊñêÊ≥¢ÈÇ£Â•ëÊï∞ $F(n)$ÔºåÂÖ≥ÈîÆÊòØËÆ°ÁÆó‰∫åÈò∂Áü©Èòµ $\\begin{bmatrix}1\u0026amp;1\\\\1\u0026amp;0\\end{bmatrix}$ ÁöÑ $n - 1$ ÁöÑÊ¨°Êñπ„ÄÇÊúÄÁªàÂèñÁõ∏‰πòÁªìÊûúÁü©ÈòµÁ¨¨‰∏ÄË°åÁ¨¨‰∏ÄÂàóÊï∞Â≠óÂç≥ÂèØ„ÄÇ\nÊ±ÇÊñπÈòµÁöÑ $n-1$ Ê¨°Êñπ‰πç‰∏ÄÁúãÈùûÂ∏∏ÁπÅÂ§çÔºåËÄå‰∏îË¶ÅÊ±ÇËøô‰πàÂ§öÊ¨°ÁöÑ‰πòÊñπÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶‰πü‰∏∫ $O(n)$ÔºåÁõ∏ËæÉ‰∫éÂæ™ÁéØÁÆóÊ≥ïÊ≤°ÊúâËøõÊ≠•„ÄÇ ÈÇ£‰πàÔºå‰∏∫‰ªÄ‰πàÊàë‰ª¨ËøòË¶ÅËÆ®ËÆ∫Ëøô‰∏™ÊñπÊ≥ïÂë¢ÔºüÂõ†‰∏∫ÂπÇËøêÁÆóÊòØÂèØ‰ª•ÈÄöËøá‰∫åÂàÜÂπÇÊù•‰ºòÂåñÂä†ÈÄüÁöÑ„ÄÇÊØîÂ¶ÇËØ¥ÔºåÂΩìÊàë‰ª¨ÈúÄË¶ÅËÆ°ÁÆó $a^n$ Êó∂Ôºö \\[ a^n=\\begin{cases} a^{\\frac{n}{2}}\\times a^{\\frac{n}{2}}\u0026amp;,\\text{ if }x\\text{ is even} \\\\\\\\ a^{\\frac{n-1}{2}}\\times a^{\\frac{n-1}{2}}\\times a\u0026amp;,\\text{ if }x\\text{ is odd} \\end{cases} \\] Ëøô‰∏™ÊñπÊ≥ï‰πüËøêÁî®‰∫ÜÂàÜÊ≤ªÁöÑÊÄùÊÉ≥ÔºåËäÇÁúÅ‰∫ÜÂ§ßÁ∫¶ $\\frac{n}{2} - 1$ Ê¨°‰πòÊ≥ïËøêÁÆó„ÄÇÈÇ£‰πàÊó∂Èó¥Â§çÊùÇÂ∫¶‰πüÁî±ÂéüÊù•ÁöÑ $O(n)$ Èôç‰Ωé‰∏∫ $O(\\log n)$„ÄÇ\nÁÆóÊ≥ïÁöÑ C++ ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö\nvoid multiply(int F[2][2], int M[2][2]);\t// ËÆ°ÁÆóÁü©Èòµ F√óM void power(int F[2][2], int n);\t// Ê±ÇÁü©Èòµ F ÁöÑ n Ê¨°Êñπ  int fibonacci(int first, int second, int n) { int F[2][2] = { {1, 1}, {1, 0} }; if (n == 0) return 0; power(F, n - 1); // ËÆ°ÁÆóÁõÆÊ†á‰∫åÈò∂Áü©ÈòµÁöÑ n - 1 Ê¨°Êñπ  return F[0][0]; } void multiply(int F[2][2], int M[2][2]) { int x = F[0][0] * M[0][0] + F[0][1] * M[1][0]; int y = F[0][0] * M[0][1] + F[0][1] * M[1][1]; int z = F[1][0] * M[0][0] + F[1][1] * M[1][0]; int w = F[1][0] * M[0][1] + F[1][1] * M[1][1]; F[0][0] = x; F[0][1] = y; F[1][0] = z; F[1][1] = w; } void power(int F[2][2], int n) { if (n == 0 || n == 1) return; int M[2][2] = { {1, 1}, {1, 0} }; power(F, n / 2); // ‰∫åÂàÜÂπÇÊñπÊ≥ï  multiply(F, F); if (n % 2 != 0) multiply(F, M); } Êó∂Èó¥Â§çÊùÇÂ∫¶Ôºö$O(\\log n)$„ÄÇ\n**Á©∫Èó¥Â§çÊùÇÂ∫¶Ôºö**ÁÆóÊ≥ïÊâÄÈúÄÁöÑËæÖÂä©Á©∫Èó¥‰ªç‰∏∫Â∏∏ÈáèÔºàËôΩÁÑ∂ÂèØËÉΩÁõ∏ÂØπËæÉÂ§ßÔºâÔºåÊïÖ‰∏∫ $O(1)$„ÄÇ\n  ÂèÇËÄÉÔºö\n$1.$ ÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÅÁ©∫Èó¥Â§çÊùÇÂ∫¶ËØ¶Ëß£ - lxf_styleÁöÑÂçöÂÆ¢ - CSDNÂçöÂÆ¢\n$2.$ Fibonacci Sequence - MathIsFun.com\n$3.$ Fibonacci sequence algorithm in Javascript ‚Äì Developers Writing ‚Äì Medium\n$4.$ ËÆ°ÁÆóÊñêÊ≥¢Á∫≥Â•ëÊï∞ÔºåÂàÜÊûêÁÆóÊ≥ïÂ§çÊùÇÂ∫¶ ¬∑ GoCalf Blog\n$5.$ Time complexity of recursive Fibonacci program - GeeksforGeeks\n$6.$ Program for Fibonacci numbers - GeeksforGeeks\n$7.$ ‰∫åÂàÜÂπÇÔºåÂø´ÈÄüÂπÇÔºåÁü©ÈòµÂø´ÈÄüÂπÇÔºåÂø´ÈÄü‰πò - ‰∏ÅÁ£ä_mlÁöÑÂçöÂÆ¢ - CSDNÂçöÂÆ¢\n ","permalink":"https://fang-lansheng.github.io/posts/2019-03-01-ds-1/","summary":"ÊñêÊ≥¢ÈÇ£Â•ëÊï∞Âàó ÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÔºàFibonacci SequenceÔºâÊòØ‰∏Ä‰∏≤Êï∞Â≠óÔºö\n\\[ (0,) 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, \u0026hellip; \\] ÂæàÂÆπÊòìÁúãÂá∫ÔºåÊØè‰∏Ä‰∏™Êï∞ÔºàÈô§Á¨¨ 1„ÄÅ2 ‰∏™ÔºâÈÉΩÁ≠â‰∫éÂÆÉ‰πãÂâçÁöÑ‰∏§‰∏™Êï∞‰πãÂíå„ÄÇÂõ†Ê≠§ÔºåÁî®ÂÖ¨ÂºèÂèØÂΩíÁ∫≥‰∏∫Ôºö \\[ F_n = \\begin{cases} 0, \u0026amp; n=0 \\\\ 1, \u0026amp; n=1 \\\\ F_{n-1} + F_{n-2}, \u0026amp; n\u0026gt;1 \\end{cases} \\]\nËÆ°ÁÆóÂ§çÊùÇÊÄßÁêÜËÆ∫ Âú®Â∞ÜÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÁî® C++ ‰ª£Á†ÅÂÆûÁé∞Âπ∂ËÆ°ÁÆóÊó∂Èó¥„ÄÅÁ©∫Èó¥Â§çÊùÇÂ∫¶‰πãÂâçÔºåÂÖàË¶Å‰∫ÜËß£‰∏Ä‰∏ã‰ªÄ‰πàÊòØÊâÄË∞ìÁöÑ‚ÄúÂ§çÊùÇÂ∫¶‚Äù\nÊó∂Èó¥Â§çÊùÇÂ∫¶  Âú®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏≠Ôºå ÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÔºàTime ComplexityÔºâÊòØ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂÆÉÂÆöÊÄßÊèèËø∞ÁÆóÊ≥ïÁöÑËøêË°åÊó∂Èó¥„ÄÇËøôÊòØ‰∏Ä‰∏™‰ª£Ë°®ÁÆóÊ≥ïËæìÂÖ•ÂÄºÁöÑÂ≠óÁ¨¶‰∏≤ÁöÑÈïøÂ∫¶ÁöÑÂáΩÊï∞„ÄÇ\n ‰∏Ä‰∏™ËØ≠Âè•ÁöÑÈ¢ëÂ∫¶ÊòØÊåáËØ•ËØ≠Âè•Âú®ÁÆóÊ≥ï‰∏≠Ë¢´ÈáçÂ§çÊâßË°åÁöÑÊ¨°Êï∞„ÄÇÁÆóÊ≥ï‰∏≠ÊâÄÊúâËØ≠Âè•ÁöÑÈ¢ëÂ∫¶‰πãÂíåÂç≥‰∏∫ $T(n)$ÔºåÂÆÉÊòØËØ•ÁÆóÊ≥ïÈóÆÈ¢òËßÑÊ®° $n$ ÁöÑÂáΩÊï∞ÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏ªË¶ÅÂàÜÊûê $T(n)$ ÁöÑÊï∞ÈáèÁ∫ß„ÄÇ ÁÆóÊ≥ïÁöÑÂü∫Êú¨ËøêÁÆóÔºàÊúÄÊ∑±Â±ÇÂæ™ÁéØÂÜÖÁöÑËØ≠Âè•ÔºâÁöÑÈ¢ëÂ∫¶‰∏é $T(n)$ ÂêåÊï∞ÈáèÁ∫ß„ÄÇÂõ†Ê≠§ÈÄöÂ∏∏ÈááÁî®ÁÆóÊ≥ï‰∏≠Âü∫Êú¨ËøêÁÆóÁöÑÈ¢ëÂ∫¶ $f(n)$ Êù•ÂàÜÊûêÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇÂõ†Ê≠§ÔºåÁÆóÊ≥ïÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ËÆ∞‰∏∫Ôºö \\[ T(n) = O(f(n)) \\] Âºè‰∏≠Ôºå$O$ ÁöÑÂê´‰πâÊòØ $T(n)$ ÁöÑÊï∞ÈáèÁ∫ßÔºåÂÖ∂‰∏•Ê†ºÁöÑÊï∞Â≠¶ÂÆö‰πâÊòØÔºöËã• $T(n)$ Âíå $f(n)$ ÊòØÂÆö‰πâÂú®Ê≠£Êï¥Êï∞ÈõÜÂêà‰∏äÁöÑ‰∏§‰∏™ÂáΩÊï∞ÔºåÂàôÂ≠òÂú®Ê≠£Â∏∏Êï∞ $C$ Âíå $n_0$Ôºå‰ΩøÂæóÂΩì $n \\geq n_0$ Êó∂ÔºåÈÉΩÊª°Ë∂≥ $0 \\leq T(n) \\leq Cf(n)$„ÄÇ","title":"Êï∞ÊçÆÁªìÊûÑ‚Äî‚ÄîÊ±ÇËß£ÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÁÆóÊ≥ïÁöÑÂ§çÊùÇÂ∫¶"},{"content":" PAT Basic Level Âà∑È¢òÔºà‰∏ÄÔºâ\nPTA | Á®ãÂ∫èËÆæËÆ°Á±ªÂÆûÈ™åËæÖÂä©ÊïôÂ≠¶Âπ≥Âè∞\n PAT B 1001 Ëß£È¢òÊÄùË∑Ø\nÈ¢òÁõÆÊØîËæÉÂü∫Á°ÄÔºåÂà©Áî®Âæ™ÁéØËØ≠Âè•ÂíåÊù°‰ª∂ËØ≠Âè•ÂÆûÁé∞ÂØπÁõÆÊ†áÊï¥Êï∞ÁöÑ‰∏çÊñ≠Áº©Â∞èÔºåÁõ¥Ëá≥ n == 1ÔºåÈÄöËøáÂèòÈáè step Êù•ËÆ°ÁÆóÊ≠•Êï∞ÔºàÁ†ç‰∫ÜÂ§öÂ∞ëÊ¨°Ôºâ\nÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;stdio.h\u0026gt;int main(){ int n, step = 0; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); // ËæìÂÖ•È¢òÁõÆÁªôÂá∫ÁöÑ n  while (n != 1) { if (n % 2 == 0) n /= 2; else n = (3 * n + 1) / 2; step++; } printf(\u0026#34;%d\\n\u0026#34;, step); return 0; } PAT B 1002 Ëß£È¢òÊÄùË∑Ø\n È¢òÁõÆË¶ÅÊ±ÇËØªÂÖ•‰∏Ä‰∏™Â∞è‰∫é 10e100 ÁöÑÊï¥Êï∞ ÔºàÂ¶ÇÔºå1234567890987654321123456789ÔºâÔºåÂÆÉÂ∑≤ÁªèË∂ÖÂá∫‰∫Ü int (-2^31 ~ 2^31 - 1) ÊàñËÄÖ long int (-2^63 ~ 2^63 - 1) ÁöÑËåÉÂõ¥ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ∞ÜÂÖ∂ÂΩìÂÅöÂ≠óÁ¨¶‰∏≤ËØªÂÖ•„ÄÇ Â∞ÜËØªÂÖ•Â≠óÁ¨¶‰∏≤ÁöÑÊØè‰∏Ä‰Ωç‚ÄúÁõ∏Âä†‚ÄùÂç≥ÂèØÂæóÂà∞ n ÁöÑÂêÑ‰ΩçÊï∞Â≠ó‰πãÂíå sum Áî® / Âíå % ÊñπÊ≥ïËé∑Âèñ sum ÁöÑÊØè‰∏Ä‰Ωç  ÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;stdio.h\u0026gt; int main() { char n; const char *pinyin[] = { \u0026#34;ling\u0026#34;, \u0026#34;yi\u0026#34;, \u0026#34;er\u0026#34;, \u0026#34;san\u0026#34;, \u0026#34;si\u0026#34;, \u0026#34;wu\u0026#34;, \u0026#34;liu\u0026#34;, \u0026#34;qi\u0026#34;, \u0026#34;ba\u0026#34;, \u0026#34;jiu\u0026#34; }; int sum = 0; while ((n = getchar()) != \u0026#39;\\n\u0026#39;) { sum += n - \u0026#39;0\u0026#39;; } // printf(\u0026#34;%d\\n\u0026#34;, sum);  if (sum / 100) // Ê†πÊçÆÈ¢òÁõÆÊù°‰ª∂Ôºåsum ‰∏ç‰ºöË∂ÖËøá 1000  printf(\u0026#34;%s \u0026#34;, pinyin[sum / 100]); if (sum / 10) printf(\u0026#34;%s \u0026#34;, pinyin[sum / 10 % 10]); printf(\u0026#34;%s\\n\u0026#34;, pinyin[sum % 10]); return 0; } PAT B 1009 Ëß£È¢òÊÄùË∑Ø\n ÊÄùË∑Ø‰∏ÄÔºö‰ΩøÁî® gets_s() ÂáΩÊï∞ËØªÂÖ•‰∏ÄÊï¥Ë°åÔºå‰ªéÂ∑¶Ëá≥Âè≥Êûö‰∏æÊØè‰∏Ä‰∏™Â≠óÁ¨¶Ôºå‰ª•Á©∫Ê†º‰∏∫ÂàÜÈöîÁ¨¶ÂØπÂçïËØçËøõË°åÂàíÂàÜÔºåÂπ∂ÊåâÈ°∫Â∫èÂ≠òÊîæÂà∞‰∫åÁª¥Â≠óÁ¨¶Êï∞ÁªÑ‰∏≠ÔºåÊúÄÂêéÊåâÂçïËØçËæìÂÖ•È°∫Â∫èÁöÑÈÄÜÈ°∫Â∫èÊù•ËæìÂá∫ÊâÄÊúâÂçïËØç ÊÄùË∑Ø‰∫åÔºöËæìÂÖ•Êó∂ÔºåÂçïËØç‰πãÈó¥Áî®Á©∫Ê†ºÈó¥ÈöîÔºàÁõ¥Âà∞ËæìÂÖ•ÁªìÊùüÔºâÔºåÂõ†Ê≠§ÂèØ‰ª• scanf Êù•ËØªÂèñËØ•Ë°åÂ≠óÁ¨¶ÔºåÂ∞ÜÊØè‰∏™ÂçïËØçÂä†ÂÖ•Âà∞‰∏Ä‰∏™Â≠óÁ¨¶Êï∞ÁªÑ‰∏≠ÔºåÁÑ∂ÂêéÈÄÜÂ∫èËæìÂá∫ËØ•Â≠óÁ¨¶Êï∞ÁªÑÔºåÊàñÊòØÁî® cin Êù•‰∏çÊñ≠ËØªÂèñÊñ∞ÁöÑÂçïËØçÔºåÂπ∂Â∞ÜÂÖ∂Âä†Âú®ÁõÆÊ†áÂ≠óÁ¨¶‰∏≤ÁöÑÂ§¥ÈÉ®ÔºåÊúÄÂêéËæìÂá∫ËØ•Â≠óÁ¨¶‰∏≤„ÄÇ  ÂèÇËÄÉ‰ª£Á†Å\n ÊÄùË∑Ø‰∏Ä  #include \u0026lt;cstdio\u0026gt;#include \u0026lt;cstring\u0026gt; int main() { char str[90]; gets_s(str); int len = strlen(str), row = 0, col = 0; char ans[90][90]; for (int i = 0; i \u0026lt; len; i++) { if (str[i] != \u0026#39; \u0026#39;) {\t// Â¶ÇÊûú‰∏çÊòØÁ©∫Ê†ºÔºåÂàôÂ≠òÊîæËá≥ ans[row][col]ÔºåÂπ∂‰ª§ col++  ans[row][col++] = str[i]; } else { ans[row][col] = \u0026#39;\\0\u0026#39;; row++; col = 0; } } ans[row][col] = \u0026#39;\\0\u0026#39;; for (int i = row; i \u0026gt;= 0; i--) { printf(\u0026#34;%s\u0026#34;, ans[i]); if (i \u0026gt; 0) printf(\u0026#34; \u0026#34;); } return 0; }  ÊÄùË∑Ø‰∫å  #include \u0026lt;cstdio\u0026gt;\tint main() { int num = 0; char ans[90][90]; while (scanf(\u0026#34;%s\u0026#34;, ans[num]) != EOF) {\t// ‰∏ÄÁõ¥ËæìÂÖ•Áõ¥Âà∞Êñá‰ª∂Êú´Â∞æ  num++;\t// ÂçïËØç‰∏™Êï∞Âä† 1  } for (int i = num - 1; i \u0026gt;= 0; i--) {\t// ÈÄÜÂ∫èËæìÂá∫  printf(\u0026#34;%s\u0026#34;, ans[i]); if (i \u0026gt; 0)\tprintf(\u0026#34; \u0026#34;);\t// Âè•Â≠êÊú´Â∞æÊ≤°ÊúâÂ§ö‰ΩôÁöÑÁ©∫Ê†º  } return 0; } PSÔºöÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®ÂëΩ‰ª§Ë°å‰∏≠ÊâãÂä®ËæìÂÖ•Êó∂ÔºåÁ≥ªÁªü‰∏çÁü•ÈÅì‰ªÄ‰πàÊó∂ÂÄôËææÂà∞‰∫ÜÊâÄË∞ìÁöÑ‚ÄúÊñá‰ª∂Êú´Â∞æ‚ÄùÔºåÂõ†Ê≠§ÈúÄË¶ÅÁî® ctrl+Z ÁªÑÂêàÈîÆÁÑ∂ÂêéÊåâ ENTER ÂõûËΩ¶ÈîÆÁöÑÊñπÂºèÊù•ÂëäËØâÁ≥ªÁªüÂ∑≤ÁªèÂà∞‰∫Ü EOFÔºåÊ≠§Êó∂Á≥ªÁªüÊâç‰ºöÁªìÊùü while Âæ™ÁéØ„ÄÇ\nÊàñËÄÖÔºö\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;string\u0026gt;using namespace std; int main() { string in, out; cin \u0026gt;\u0026gt; out; while (cin \u0026gt;\u0026gt; in) { out = in + \u0026#34; \u0026#34; + out; } cout \u0026lt;\u0026lt; out \u0026lt;\u0026lt; endl; return 0; } PSÔºöËØ•ÊñπÊ≥ï‰πüÈúÄË¶ÅÁî® ctrl+Z ÁªÑÂêàÈîÆÁÑ∂ÂêéÊåâ ENTER ÂõûËΩ¶ÈîÆÁöÑÊñπÂºèÊù•ÂëäËØâÁ≥ªÁªüÂ∑≤ÁªèÂà∞‰∫Ü EOF„ÄÇ\nPAT B 1022 Ëß£È¢òÊÄùË∑Ø\nÂçÅËøõÂà∂ a ÔºàÊï¥Êï∞ÔºâËΩ¨Êç¢‰∏∫ D ËøõÂà∂ (1 \u0026lt; D \u0026lt;= 10) Êï∞ bÔºåÈááÁî®‚ÄúÈô§Âü∫Âèñ‰ΩôÊ≥ï‚ÄùÔºöÂ∞Ü a Èô§‰ª• DÔºåÁÑ∂ÂêéÂ∞ÜÊâÄÂæóÁöÑ‰ΩôÊï∞‰Ωú‰∏∫‰Ωé‰ΩçÂ≠òÂÇ®ÔºõËÄåÂïÜÂàôÁªßÁª≠Èô§‰ª• D Âπ∂ÈáçÂ§ç‰πãÂâçÁöÑÊìç‰ΩúÔºåÊúÄÂêéÂΩìÂïÜ‰∏∫ 0 Êó∂ÔºåÂ∞ÜÊâÄÊúâ‰Ωç‰ªéÈ´òÂà∞‰ΩéËæìÂá∫Â∞±ÂèØ‰ª•ÂæóÂà∞ b„ÄÇ\nÁ§∫‰æãÔºåÂ∞ÜÂçÅËøõÂà∂Êï∞ 11 ËΩ¨Êç¢‰∏∫‰∫åËøõÂà∂Êï∞Ôºö\n 11 Èô§‰ª• 2ÔºåÂæóÂïÜ‰∏∫ 5Ôºå‰ΩôÊï∞‰∏∫ 1Ôºõ 5 Èô§‰ª• 2ÔºåÂæóÂïÜ‰∏∫ 2Ôºå‰ΩôÊï∞‰∏∫ 1Ôºõ 2 Èô§‰ª• 2ÔºåÂæóÂïÜ‰∏∫ 1Ôºå ‰ΩôÊï∞‰∏∫ 0Ôºõ 1 Èô§‰ª• 2ÔºåÂæóÂïÜ‰∏∫ 0Ôºå‰ΩôÊï∞‰∏∫ 1Ôºõ ÂïÜ‰∏∫ 0ÔºåÁªàÊ≠¢„ÄÇÂ∞Ü‰ΩôÊï∞‰ªéÂêéÂêëÂâçËæìÂá∫ÔºåÂæóÂà∞ 1011ÔºåÂç≥‰∏∫ 11 ÁöÑ‰∫åËøõÂà∂Êï∞„ÄÇ  ÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;stdio.h\u0026gt; int main() { int A, B, D; scanf(\u0026#34;%d%d%d\u0026#34;, \u0026amp;A, \u0026amp;B, \u0026amp;D); int sum = A + B; int ans[31], num = 0; do { ans[num++] = sum % D;\t// ans[num] Â≠òÊîæ D ËøõÂà∂Êï∞ÁöÑÁ¨¨ num ‰Ωç  sum /= D; } while (sum != 0); for (int i = num - 1; i \u0026gt;= 0; i--) { printf(\u0026#34;%d\u0026#34;, ans[i]); // Â∞ÜÊâÄÊúâ‰Ωç‰ªéÈ´òÂà∞‰ΩéËæìÂá∫  } return 0; PAT B 1032 Ëß£È¢òÊÄùË∑Ø\nÂ∞ÜÊØè‰∏ÄÂõûËæìÂÖ•ÂàÜÊï∞Âä†Âú®Êï∞ÁªÑÂØπÂ∫îÁ¥¢ÂºïÔºàÂ≠¶Ê†°ÁºñÂè∑ÔºâÁöÑÂÖÉÁ¥†ÂÄº‰∏äÔºåÊúÄÂêéÈÄöËøáÊØîËæÉÊï∞ÁªÑÂÜÖÈÉ®ÂÖÉÁ¥†Â§ßÂ∞èÂà§Êñ≠ÊÄªÂæóÂàÜÊúÄÈ´òÁöÑÂ≠¶Ê†°„ÄÇ\nÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;stdio.h\u0026gt; const int maxN = 100001; int school[maxN] = { 0 }; int main() { int n, schID, score;\t// n ‰∏∫Â≠¶Ê†°Êï∞ÁõÆÔºåschID ‰∏∫Â≠¶Ê†°ÁºñÂè∑Ôºåscore ‰∏∫Â≠¶Ê†°ÂàÜÊï∞  scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n; i++) { scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;schID, \u0026amp;score); school[schID] += score; } int k = 1, MAX = -1;\t// ÊÄªÂàÜÊúÄÈ´òÁöÑÂ≠¶Ê†° ID ÂèäÂÖ∂ÂàÜÊï∞  for (int i = 0; i \u0026lt;= n; i++) { if (school[i] \u0026gt; MAX) { MAX = school[i]; k = i; } } printf(\u0026#34;%d %d\u0026#34;, k, MAX); return 0; } PAT B 1036 Ëß£È¢òÊÄùË∑Ø\n Ê†πÊçÆÈ¢òÁõÆË¶ÅÊ±ÇÔºåÂØπË°åÊï∞ËøõË°åÂõõËàç‰∫îÂÖ•Ôºörow = (col % 2 == 0) ? (col / 2) : ((col + 1) / 2) È¶ñÂ∞æ‰∏§Ë°åËæìÂá∫ col ‰∏™Â≠óÁ¨¶ ÂÖ∂‰ªñË°åËæìÂá∫Ê†ºÂºè‰∏∫ÔºöÂ≠óÁ¨¶ + Á©∫Ê†º * (col - 2) + Â≠óÁ¨¶  ÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;stdio.h\u0026gt; int main() { int col, row; char str; scanf(\u0026#34;%d %c\u0026#34;, \u0026amp;col, \u0026amp;str); row = (col % 2 == 0) ? (col / 2) : ((col + 1) / 2); for (int i = 1; i \u0026lt;= row; i++) { if (i == 1 || i == row) { for (int j = 0; j \u0026lt; col; j++) { printf(\u0026#34;%c\u0026#34;, str); } printf(\u0026#34;\\n\u0026#34;); } else { printf(\u0026#34;%c\u0026#34;, str); for (int j = 1; j \u0026lt; col - 1; j++) { printf(\u0026#34; \u0026#34;); } printf(\u0026#34;%c\\n\u0026#34;, str); } } return 0; }  Êú™ÂÆåÂæÖÁª≠„ÄÇ„ÄÇ„ÄÇ\n ","permalink":"https://fang-lansheng.github.io/posts/2019-02-28-pat-basic-level-practice-1/","summary":"PAT Basic Level Âà∑È¢òÔºà‰∏ÄÔºâ\nPTA | Á®ãÂ∫èËÆæËÆ°Á±ªÂÆûÈ™åËæÖÂä©ÊïôÂ≠¶Âπ≥Âè∞\n PAT B 1001 Ëß£È¢òÊÄùË∑Ø\nÈ¢òÁõÆÊØîËæÉÂü∫Á°ÄÔºåÂà©Áî®Âæ™ÁéØËØ≠Âè•ÂíåÊù°‰ª∂ËØ≠Âè•ÂÆûÁé∞ÂØπÁõÆÊ†áÊï¥Êï∞ÁöÑ‰∏çÊñ≠Áº©Â∞èÔºåÁõ¥Ëá≥ n == 1ÔºåÈÄöËøáÂèòÈáè step Êù•ËÆ°ÁÆóÊ≠•Êï∞ÔºàÁ†ç‰∫ÜÂ§öÂ∞ëÊ¨°Ôºâ\nÂèÇËÄÉ‰ª£Á†Å\n#include \u0026lt;stdio.h\u0026gt;int main(){ int n, step = 0; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); // ËæìÂÖ•È¢òÁõÆÁªôÂá∫ÁöÑ n  while (n != 1) { if (n % 2 == 0) n /= 2; else n = (3 * n + 1) / 2; step++; } printf(\u0026#34;%d\\n\u0026#34;, step); return 0; } PAT B 1002 Ëß£È¢òÊÄùË∑Ø\n È¢òÁõÆË¶ÅÊ±ÇËØªÂÖ•‰∏Ä‰∏™Â∞è‰∫é 10e100 ÁöÑÊï¥Êï∞ ÔºàÂ¶ÇÔºå1234567890987654321123456789ÔºâÔºåÂÆÉÂ∑≤ÁªèË∂ÖÂá∫‰∫Ü int (-2^31 ~ 2^31 - 1) ÊàñËÄÖ long int (-2^63 ~ 2^63 - 1) ÁöÑËåÉÂõ¥ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ∞ÜÂÖ∂ÂΩìÂÅöÂ≠óÁ¨¶‰∏≤ËØªÂÖ•„ÄÇ Â∞ÜËØªÂÖ•Â≠óÁ¨¶‰∏≤ÁöÑÊØè‰∏Ä‰Ωç‚ÄúÁõ∏Âä†‚ÄùÂç≥ÂèØÂæóÂà∞ n ÁöÑÂêÑ‰ΩçÊï∞Â≠ó‰πãÂíå sum Áî® / Âíå % ÊñπÊ≥ïËé∑Âèñ sum ÁöÑÊØè‰∏Ä‰Ωç  ÂèÇËÄÉ‰ª£Á†Å","title":"PAT Basic Level Practice (I)"},{"content":" ‰∏Ä‰∏™Â§öÊúà‰ª•Êù•ÔºåÁ°ÆÂÆöÂ•Ω‰∫ÜËÄÉÁ†îÁöÑËÆ°Âàí„ÄÇÁé∞Âú®ÂºÄÂßãÂáÜÂ§áÂà∑È¢òÂ≠¶‰π†‰∏Ä‰∫õÁÆóÊ≥ïÁöÑÂü∫Á°ÄÔºå‰πüÊòØ‰ªéÂ§¥ÂºÄÂßãÂ≠¶‰π†C/C++ÔºåÂΩìÁÑ∂Âú®Â≠¶‰π†ËøáÁ®ã‰∏≠Ë∏©‰∫Ü‰∏çÂ∞ëÂùëÔºåÂú®ËøôÈáåËøõË°å‰∏Ä‰∫õÊÄªÁªì„ÄÇ\n C++ Êï∞ÊçÆÁ±ªÂûãÂèäÂÖ∂Ë°®Á§∫ËåÉÂõ¥ Ëøô‰∏ÄÈÉ®ÂàÜÊòØÂü∫Á°ÄÔºå‰∏çËøáÊàë‰πãÂâçÈÉΩÊ≤°ÊúâÁ≥ªÁªüÊÄßÁöÑÊêûÊ∏ÖÊ•öÔºåÊâÄ‰ª•Áé∞Âú®ËøòÊòØÂÖàÊääËøô‰∫õÁü•ËØÜÁÇπÂàóÂá∫Êù•ÔºåËØ•ÈÉ®ÂàÜÂèÇËÄÉC++ Êï∞ÊçÆÁ±ªÂûã - ËèúÈ∏üÊïôÁ®ã„ÄÇ\n1. Âü∫Êú¨ÁöÑÂÜÖÁΩÆÁ±ªÂûã C++ ‰∏∫Á®ãÂ∫èÂëòÊèê‰æõ‰∫ÜÁßçÁ±ª‰∏∞ÂØåÁöÑÂÜÖÁΩÆÊï∞ÊçÆÁ±ªÂûãÂíåÁî®Êà∑Ëá™ÂÆö‰πâÁöÑÊï∞ÊçÆÁ±ªÂûã„ÄÇ‰∏ãË°®ÂàóÂá∫‰∫Ü‰∏ÉÁßçÂü∫Êú¨ÁöÑ C++ Êï∞ÊçÆÁ±ªÂûãÔºö\n   Á±ªÂûã ÂÖ≥ÈîÆÂ≠ó     Â∏ÉÂ∞îÂûã bool   Â≠óÁ¨¶Âûã char   Êï¥Âûã int   ÊµÆÁÇπÂûã float   ÂèåÊµÆÁÇπÂûã double   Êó†Á±ªÂûã void   ÂÆΩÂ≠óÁ¨¶Âûã wchar_t    ÂÖ∂‰∏≠Ôºåwchar_t ÂÆûÈôÖ‰∏äÊòØÔºö\ntypedef wchar_t short int; ÊâÄ‰ª• wchar_t ÂÆûÈôÖ‰∏äÁöÑÁ©∫Èó¥ÊòØÂíå short int ‰∏ÄÊ†∑„ÄÇ ‰∏Ä‰∫õÂü∫Êú¨Á±ªÂûãÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∏™ÊàñÂ§ö‰∏™Á±ªÂûã‰øÆÈ•∞Á¨¶ËøõË°å‰øÆÈ•∞Ôºö\n signed unsigned short long  2. ÂêÑÁßçÂèòÈáèÁ±ªÂûãÁöÑÂ§ßÂ∞è    Êï∞ÊçÆÁ±ªÂûã Â≠óËäÇÊï∞ Ë°®Á§∫ËåÉÂõ¥     char 1 $-128 \\sim 127 \\ ||\\ 0 \\sim 255$   unsigned char 1 $0 \\sim 255$   signed char 1 $-128 \\sim 127$   int 4 $-2^{31} \\sim 2^{31}-1$   unsigned int 4 $0 \\sim 2^{32} - 1$   signed int 4 $-2^{31} \\sim 2^{31}-1$   short int 2 $-2^{15} \\sim 2^{15}-1$   unsigned short int 2 $0 \\sim 2^{16} - 1$   signed short int 2 $-2^{15} \\sim 2^{15}-1$   long int 8 $-2^{63} \\sim 2^{63} - 1$   unsigned long int 8 $0 \\sim 2^{64} - 1$   signed long int 8 $-2^{63} \\sim 2^{63} - 1$   float 4 $-3.40 \\times 10^{38} \\sim 3.40 \\times 10^{38}$   double 8 $-1.79\\times10^{308} \\sim 1.79 \\times 10^{308}$    Ê≥®Ôºö‰∏çÂêåÁ≥ªÁªüÂèØËÉΩ‰ºöÊúâÊâÄÂ∑ÆÂºÇ„ÄÇ\nÊµÆÁÇπÊï∞ÁöÑÊØîËæÉ  Âú®Â≠¶‰π†„ÄäÁÆóÊ≥ïÁ¨îËÆ∞„Äã‰∏Ä‰π¶Êó∂Ôºå‰ΩúËÄÖ‰ªãÁªç‰∫Ü‚ÄúÊûÅÂ∞èÊï∞‚ÄùÁî®‰ª•‰øÆÊ≠£ÊµÆÁÇπÊï∞Âú®ËÆ°ÁÆóÊú∫‰∏≠Â≠òÂÇ®‰∏éËÆ°ÁÆó‰∫ßÁîüÁöÑËØØÂ∑Æ„ÄÇËøô‰∏ÄÁÇπÊòØÊàëÂú®‰πãÂâçÂ≠¶‰π†‰∏≠ÊâÄÊ≤°ÊúâÊÑèËØÜÂà∞ÁöÑÔºå‰∫éÊòØÂú®Ê≠§ÂΩíÁ∫≥‰∏Ä‰∏ãÁõ∏ÂÖ≥ÁöÑÂÜÖÂÆπ„ÄÇ\n Áî±‰∫éËÆ°ÁÆóÊú∫‰∏≠ÈááÁî®ÊúâÈôê‰ΩçÁöÑ‰∫åËøõÂà∂ÁºñÁ†ÅÔºåÂõ†Ê≠§ÊµÆÁÇπÊï∞Âú®ËÆ°ÁÆóÊú∫‰∏≠ÁöÑÂ≠òÂÇ®Âπ∂‰∏çÊÄªÊòØÁ≤æÁ°ÆÁöÑ„ÄÇ‰æãÂ¶ÇÂú®ÁªèËøáÂ§ßÈáèËøêÁÆóÂêéÔºå‰∏Ä‰∏™ÊµÆÁÇπÂûãÁöÑÊï∞ 3.14 Âú®ËÆ°ÁÆóÊú∫‰∏≠Â∞±ÂèØËÉΩÂ≠òÂÇ®Êàê 3.1400000000001Ôºå‰πüÊúâÂèØËÉΩÂ≠òÂÇ®Êàê 3.1399999999999ÔºåËøôÁßçÊÉÖÂÜµ‰∏ã‰ºöÂØπÊØîËæÉÊìç‰ΩúÂ∏¶Êù•ÊûÅÂ§ßÁöÑÂπ≤Êâ∞ÔºàÂõ†‰∏∫C/C++‰∏≠ÁöÑ == Êìç‰ΩúÊòØÂÆåÂÖ®Áõ∏ÂêåÊâçËÉΩÂà§ÂÆö‰∏∫ trueÔºâ„ÄÇ‰∫éÊòØÂú®Êñá‰∏≠Ôºå‰ΩúËÄÖÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÊûÅÂ∞èÊï∞ eps Êù•ÂØπËøôÁßçËØØÂ∑ÆËøõË°å‰øÆÊ≠£„ÄÇ\nÁªèÈ™åË°®ÊòéÔºåeps Âèñ $1e-8$Ôºà$10^{-8}$Ôºâ Âú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÊó¢‰∏ç‰ºöÊºèÂà§Ôºå‰πü‰∏ç‰ºöËØØÂà§„ÄÇÂõ†Ê≠§ÂèØÂ∞Ü eps ÂÆö‰πâ‰∏∫Â∏∏Èáè $1e-8$Ôºö\nconst double eps = 1e-8; 1. Á≠â‰∫éËøêÁÆóÁ¨¶ == ‰∏ãÂõæ‰∏∫Á≠â‰∫éÂå∫Èó¥Á§∫ÊÑèÂõæÔºö\nÂ¶ÇÂõæÊâÄÁ§∫ÔºåÂ¶ÇÊûú‰∏Ä‰∏™Êï∞ a ËêΩÂú®‰∫Ü $[b - eps, b + eps]$ ÁöÑÂå∫Èó¥‰∏≠ÔºåÂ∞±Âà§Êñ≠ a == b ÊàêÁ´ã„ÄÇ‰∏∫‰∫Ü‰ΩøÊØîËæÉÊõ¥‰∏∫Êñπ‰æøÔºåÊääÊØîËæÉÊìç‰ΩúÂÜôÊàêÂÆèÂÆö‰πâÁöÑÂΩ¢ÂºèÔºö\n#include \u0026lt;math.h\u0026gt;#define Equ(a, b) ((fabs((a) - (b))) - (eps)) // Êã¨Âè∑ÊòØ‰∏∫Èò≤Ê≠¢ÂÆèÂÆö‰πâÂèØËÉΩÂ∏¶Êù•ÁöÑÈîôËØØ 2. Â§ß‰∫éËøêÁÆóÁ¨¶ \u0026gt; ‰∏ãÂõæ‰∏∫Â§ß‰∫éÂå∫Èó¥Á§∫ÊÑèÂõæÔºö\nÂ¶ÇÂõæÊâÄÁ§∫ÔºåÂ¶ÇÊûú‰∏Ä‰∏™Êï∞ a Ë¶ÅÂ§ß‰∫é bÔºåÈÇ£‰πàÂ∞±ÂøÖÈ°ªÂú®ËØØÂ∑Æ eps Êâ∞Âä®ËåÉÂõ¥‰πãÂ§ñÂ§ß‰∫é bÔºåÂõ†Ê≠§Âè™ÊúâÂ§ß‰∫é b + eps ÁöÑÊï∞ÊâçËÉΩÂà§ÂÆö‰∏∫Â§ß‰∫é bÔºà‰πüÂç≥ $a - b \u0026gt; eps$Ôºâ„ÄÇ\n#define More(a, b) (((a) - (b)) \u0026gt; (eps)) 3. Â∞è‰∫éËøêÁÆóÁ¨¶ \u0026lt; ‰∏ãÂõæ‰∏∫Â∞è‰∫éÂå∫Èó¥Á§∫ÊÑèÂõæÔºö\nÂ¶ÇÂõæÊâÄÁ§∫ÔºåÂ¶ÇÊûú‰∏Ä‰∏™Êï∞ a Ë¶ÅÂ∞è‰∫é bÔºåÈÇ£‰πàÂ∞±ÂøÖÈ°ªÂú®ËØØÂ∑Æ eps Êâ∞Âä®ËåÉÂõ¥‰πãÂ§ñÂ∞è‰∫é bÔºåÂõ†Ê≠§Âè™ÊúâÂ∞è‰∫é b - eps ÁöÑÊï∞ÊâçËÉΩÂà§ÂÆö‰∏∫Â∞è‰∫é bÔºà‰πüÂç≥ $a - b \u0026lt; -eps$Ôºâ„ÄÇ\n#define Less(a, b) (((a) - (b)) \u0026lt; (-eps)) 4. Â§ß‰∫éÁ≠â‰∫éËøêÁÆóÁ¨¶ \u0026gt;= ‰∏ãÂõæ‰∏∫Â§ß‰∫éÁ≠â‰∫éÂå∫Èó¥Á§∫ÊÑèÂõæÔºö\nÂ¶ÇÂõæÊâÄÁ§∫ÔºåÁî±‰∫éÂ§ß‰∫éÁ≠â‰∫éËøêÁÆóÁ¨¶ÂèØ‰ª•ÁêÜËß£‰∏∫Â§ß‰∫éËøêÁÆóÁ¨¶ÂíåÁ≠â‰∫éËøêÁÆóÁ¨¶ÁöÑÁªìÂêàÔºå‰∫éÊòØÈúÄË¶ÅËÆ©‰∏Ä‰∏™Êï∞ a Âú®ËØØÂ∑ÆÊâ∞Âä®ËåÉÂõ¥ÂÜÖÂ§ß‰∫éÊàñËÄÖÁ≠â‰∫é bÔºåÂõ†Ê≠§Â§ß‰∫é b - eps ÁöÑÊï∞ÈÉΩÂ∫îÂΩìÂà§ÂÆö‰∏∫Â§ß‰∫éÁ≠â‰∫é bÔºà‰πüÂç≥ $a - b \u0026gt; -eps$Ôºâ„ÄÇ\n#define MoreEqu(a, b) (((a) - (b)) \u0026gt; (-eps)) 5. Â∞è‰∫éÁ≠â‰∫éËøêÁÆóÁ¨¶ \u0026lt;= ‰∏ãÂõæ‰∏∫Â∞è‰∫éÁ≠â‰∫éÂå∫Èó¥Á§∫ÊÑèÂõæÔºö\nÂ¶ÇÂõæÊâÄÁ§∫ÔºåÁî±‰∫éÂ∞è‰∫éÁ≠â‰∫éËøêÁÆóÁ¨¶ÂèØ‰ª•ÁêÜËß£‰∏∫Â∞è‰∫éËøêÁÆóÁ¨¶ÂíåÁ≠â‰∫éËøêÁÆóÁ¨¶ÁöÑÁªìÂêàÔºå‰∫éÊòØÈúÄË¶ÅËÆ©‰∏Ä‰∏™Êï∞ a Âú®ËØØÂ∑ÆÊâ∞Âä®ËåÉÂõ¥ÂÜÖÂ∞è‰∫éÊàñËÄÖÁ≠â‰∫é bÔºåÂõ†Ê≠§Â∞è‰∫é b + eps ÁöÑÊï∞ÈÉΩÂ∫îÂΩìÂà§ÂÆö‰∏∫Â∞è‰∫éÁ≠â‰∫é bÔºà‰πüÂç≥ $a - b \u0026lt; eps$Ôºâ„ÄÇ\n#define LessEqu(a, b) (((a) - (b)) \u0026lt; (eps)) 6. ÂúÜÂë®Áéá œÄ ÂúÜÂë®Áéá œÄ ÂèØÁî± $arccos(-1)$ ËÆ°ÁÆóÂæóÂà∞Ôºö\n#include \u0026lt;math.h\u0026gt;const double PI = acos(-1.0); Ê±áÊÄªÁªìÊûú‰∏∫Â¶Ç‰∏ã‰ª£Á†ÅÔºö\n#include \u0026lt;math.h\u0026gt;const double eps = 1e-8; const double PI = acos(-1.0); #define Equ(a, b) ((fabs((a) - (b))) - (eps)) #define More(a, b) (((a) - (b)) \u0026gt; (eps)) #define Less(a, b) (((a) - (b)) \u0026lt; (-eps)) #define MoreEqu(a, b) (((a) - (b)) \u0026gt; (-eps)) #define LessEqu(a, b) (((a) - (b)) \u0026lt; (eps)) ËØªÂèñ‚ÄúÂ§ßÊï∞‚Äù ÊúâÁöÑÊó∂ÂÄôÔºåÊàëÈúÄË¶ÅËØªÂÖ•‰∏Ä‰∏™ÂæàÂ§ßÁöÑÊï¥Êï∞Ôºà‰æãÂ¶ÇÔºå1234567890987654321123456789ÔºâÔºåÂπ∂ËÆ°ÁÆóÂá∫ÂÆÉÁöÑÂêÑ‰ΩçÊï∞Â≠ó‰πãÂíå„ÄÇÁî® int ($-2^{31} \\sim 2^{31} - 1$) ÊàñËÄÖ long int ($-2^{63} \\sim 2^{63} - 1$) ÈÉΩ‰∏çÊª°Ë∂≥Ë¶ÅÊ±Ç„ÄÇËøôÊó∂ÂèØ‰ª•Â∞ÜËøôÊ†∑‰∏Ä‰∏™‚ÄúÂ§ßÊï∞‚Äù‰Ωú‰∏∫Â≠óÁ¨¶‰∏≤ÔºàÊàñÊòØÂ≠óÁ¨¶Êï∞ÁªÑÔºâËØªÂÖ•„ÄÇ\n Â≠óÁ¨¶‰∏≤ÂÆûÈôÖ‰∏äÊòØ‰ΩøÁî® null Â≠óÁ¨¶ '\\0' ÁªàÊ≠¢ÁöÑ‰∏ÄÁª¥Â≠óÁ¨¶Êï∞ÁªÑ„ÄÇÂõ†Ê≠§Ôºå‰∏Ä‰∏™‰ª• null ÁªìÂ∞æÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÂåÖÂê´‰∫ÜÁªÑÊàêÂ≠óÁ¨¶‰∏≤ÁöÑÂ≠óÁ¨¶„ÄÇ\n Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî® getchar() ÂáΩÊï∞ÔºåÂ∞ÜËØ•Êï∞Â≠óÁöÑÊØè‰∏Ä‰Ωç‰Ωú‰∏∫Â≠óÁ¨¶ËØªÂÖ•Âà∞Êàë‰ª¨ÁöÑÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑ‰∏≠„ÄÇ\n C Â∫ìÂáΩÊï∞ int getchar(void) ‰ªéÊ†áÂáÜËæìÂÖ• stdin Ëé∑Âèñ‰∏Ä‰∏™Â≠óÁ¨¶Ôºà‰∏Ä‰∏™Êó†Á¨¶Âè∑Â≠óÁ¨¶Ôºâ„ÄÇËøôÁ≠âÂêå‰∫é getc Â∏¶Êúâ stdin ‰Ωú‰∏∫ÂèÇÊï∞„ÄÇ\n Áî®Á®ãÂ∫èÂèØ‰ª•ÁÆÄÂçïÂú∞Ë°®Á§∫‰∏∫Ôºö\n#include \u0026lt;stdio.h\u0026gt; int main() { char n[100]; int i = 0; printf(\u0026#34;Input: \u0026#34;); while ((n[i] = getchar()) != \u0026#39;\\n\u0026#39;) { // ÈÄê‰ΩçËØªÂÖ•Êï∞Â≠ó  i++; } printf(\u0026#34;Output: \u0026#34;); for (int k = 0; k \u0026lt; i; k++) { printf(\u0026#34;%c\u0026#34;, n[k]); // ÈÄê‰ΩçËæìÂá∫Êï∞Â≠ó  } printf(\u0026#34;\\n\\n\u0026#34;); return 0; } ÂΩìÊàë‰ª¨ËæìÂÖ•Êï∞Â≠ó 1234567890987654321123456789Ôºå‰πü‰ºöÂæóÂà∞Áõ∏Â∫îÁöÑËæìÂá∫Ôºö\nÊ≠§Êó∂ÔºåÂ≠òÂÇ®Â≠óÁ¨¶ÁöÑÊï∞ÁªÑ n ‰∏∫Ôºö\n","permalink":"https://fang-lansheng.github.io/posts/2019-02-27-cpp-study-notes-1/","summary":"‰∏Ä‰∏™Â§öÊúà‰ª•Êù•ÔºåÁ°ÆÂÆöÂ•Ω‰∫ÜËÄÉÁ†îÁöÑËÆ°Âàí„ÄÇÁé∞Âú®ÂºÄÂßãÂáÜÂ§áÂà∑È¢òÂ≠¶‰π†‰∏Ä‰∫õÁÆóÊ≥ïÁöÑÂü∫Á°ÄÔºå‰πüÊòØ‰ªéÂ§¥ÂºÄÂßãÂ≠¶‰π†C/C++ÔºåÂΩìÁÑ∂Âú®Â≠¶‰π†ËøáÁ®ã‰∏≠Ë∏©‰∫Ü‰∏çÂ∞ëÂùëÔºåÂú®ËøôÈáåËøõË°å‰∏Ä‰∫õÊÄªÁªì„ÄÇ\n C++ Êï∞ÊçÆÁ±ªÂûãÂèäÂÖ∂Ë°®Á§∫ËåÉÂõ¥ Ëøô‰∏ÄÈÉ®ÂàÜÊòØÂü∫Á°ÄÔºå‰∏çËøáÊàë‰πãÂâçÈÉΩÊ≤°ÊúâÁ≥ªÁªüÊÄßÁöÑÊêûÊ∏ÖÊ•öÔºåÊâÄ‰ª•Áé∞Âú®ËøòÊòØÂÖàÊääËøô‰∫õÁü•ËØÜÁÇπÂàóÂá∫Êù•ÔºåËØ•ÈÉ®ÂàÜÂèÇËÄÉC++ Êï∞ÊçÆÁ±ªÂûã - ËèúÈ∏üÊïôÁ®ã„ÄÇ\n1. Âü∫Êú¨ÁöÑÂÜÖÁΩÆÁ±ªÂûã C++ ‰∏∫Á®ãÂ∫èÂëòÊèê‰æõ‰∫ÜÁßçÁ±ª‰∏∞ÂØåÁöÑÂÜÖÁΩÆÊï∞ÊçÆÁ±ªÂûãÂíåÁî®Êà∑Ëá™ÂÆö‰πâÁöÑÊï∞ÊçÆÁ±ªÂûã„ÄÇ‰∏ãË°®ÂàóÂá∫‰∫Ü‰∏ÉÁßçÂü∫Êú¨ÁöÑ C++ Êï∞ÊçÆÁ±ªÂûãÔºö\n   Á±ªÂûã ÂÖ≥ÈîÆÂ≠ó     Â∏ÉÂ∞îÂûã bool   Â≠óÁ¨¶Âûã char   Êï¥Âûã int   ÊµÆÁÇπÂûã float   ÂèåÊµÆÁÇπÂûã double   Êó†Á±ªÂûã void   ÂÆΩÂ≠óÁ¨¶Âûã wchar_t    ÂÖ∂‰∏≠Ôºåwchar_t ÂÆûÈôÖ‰∏äÊòØÔºö\ntypedef wchar_t short int; ÊâÄ‰ª• wchar_t ÂÆûÈôÖ‰∏äÁöÑÁ©∫Èó¥ÊòØÂíå short int ‰∏ÄÊ†∑„ÄÇ ‰∏Ä‰∫õÂü∫Êú¨Á±ªÂûãÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∏™ÊàñÂ§ö‰∏™Á±ªÂûã‰øÆÈ•∞Á¨¶ËøõË°å‰øÆÈ•∞Ôºö\n signed unsigned short long  2.","title":"C++ Â≠¶‰π†Á¨îËÆ∞Ôºà‰∏ÄÔºâÔºöÂàùÂ≠¶ËÄÖÁöÑÂ∞èÁ™çÈó®"},{"content":" Êú¨ÁØáÂ≠¶‰π†Á¨îËÆ∞ÊòØÂú®ÊÖïËØæÁΩë‰∏äÂ≠¶‰π†JavaScriptÊ∑±ÂÖ•ÊµÖÂá∫Êó∂ÊâÄÂÅöÁ¨îËÆ∞ÔºåÂÜÖÂÆπ‰∏ªË¶ÅÊù•Ëá™ËØæÁ®ãËØæ‰ª∂ÂèäËÄÅÂ∏àËÆ≤Ëß£ÔºåÂêåÊó∂Á©øÊèíËá™Â∑±ÁöÑ‰∏Ä‰∫õÁêÜËß£ÂíåÂ∞ùËØï„ÄÇÈÉ®ÂàÜËµÑÊñôÊï¥ÁêÜËá™ MDN Web Docs„ÄÇ\n Êï∞ÁªÑÊòØÂÄºÁöÑÊúâÂ∫èÈõÜÂêà„ÄÇÊØè‰∏™ÂÄºÂè´ÂÅöÂÖÉÁ¥†ÔºåÊØè‰∏™ÂÖÉÁ¥†Âú®Êï∞ÁªÑ‰∏≠ÈÉΩÊúâÊï∞Â≠ó‰ΩçÁΩÆÁºñÂè∑Ôºå‰πüÂ∞±ÊòØÁ¥¢Âºï„ÄÇJavaScript ‰∏≠ÁöÑÊï∞ÁªÑÊòØÂº±Á±ªÂûãÁöÑÔºåÊï∞ÁªÑ‰∏≠ÂèØ‰ª•Âê´Êúâ‰∏çÂêåÁ±ªÂûãÁöÑÂÖÉÁ¥†„ÄÇÊï∞ÁªÑÂÖÉÁ¥†ÁîöËá≥ÂèØ‰ª•ÊòØÂØπË±°ÊàñÂÖ∂‰ªñÊï∞ÁªÑ„ÄÇ\n// ‰æãÂ¶Ç var arr = [1, true, null, undefined, {x: 1}, [1, 2, 3]]; arr[0];\t// 1 arr[3];\t// undefined arr[4].x;\t// 1 arr[5][1];\t// 2 5.1 ÂàõÂª∫Êï∞ÁªÑ \u0026amp; Êï∞ÁªÑÊìç‰Ωú ÂàõÂª∫Êï∞ÁªÑ - Â≠óÈù¢Èáè var BAT = [\u0026#39;Baidu\u0026#39;, \u0026#39;Alibaba\u0026#39;, \u0026#39;Tencent\u0026#39;]; var students = [{name: \u0026#39;Bosn\u0026#39;, age: 27}, {name: \u0026#39;Nunnly\u0026#39;, age: 3}]; var arr = [\u0026#39;Nunnly\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;big\u0026#39;, \u0026#39;keng\u0026#39;, \u0026#39;B\u0026#39;, 123, true, null]; var commasArr1 = [1, , 2];\t// 1, undefined, 2 var commasArr2 = [,,];\t// undefined * 2Ôºå‰∏çËµûÂêåËøôÁßçÂÆö‰πâÊñπÂºè Êï∞ÁªÑÁöÑÂ§ßÂ∞èÊòØÊúâÈôêÂà∂ÁöÑÔºösize from 0 to 4,294,967,295 ($2^{23} - 1$)\nÂàõÂª∫Êï∞ÁªÑ - new Array var arr = new Array(); var arrWithLength = new Array(100);\t// undefined * 100 var arrLikesLiteral = new Array(true, false, null, 1, 2, \u0026#39;hi\u0026#39;);\t// Á≠â‰ª∑‰∫é [true, false, null, 1, 2, \u0026#39;hi\u0026#39;] Ê≥®ÊÑèÔºöArray ÁöÑÊûÑÈÄ†Âô®Â¶ÇÊûú‰∏ç‰ΩøÁî® new ‰πüÊòØÁ≠â‰ª∑ÁöÑ\nÊï∞ÁªÑÊìç‰Ωú - Êï∞ÁªÑÂÖÉÁ¥†ËØªÂÜô var arr = [1, 2, 3, 4, 5]; arr[1];\t// 2 arr.length;\t// 5  arr[5] = 6;\t// Âä®ÊÄÅÊ∑ªÂä†Êï∞ÁªÑÂÖÉÁ¥† arr.length;\t// 6 delete arr[0]; arr[0];\t// undefined arr.length;\t// 6 Êï∞ÁªÑÊìç‰Ωú - Êï∞ÁªÑÂÖÉÁ¥†Â¢ûÂà† JavaScript ‰∏≠ÁöÑÊï∞ÁªÑÊòØÂä®ÊÄÅÁöÑÔºå‰∏çÈúÄË¶ÅÊåáÂÆöÂ§ßÂ∞è„ÄÇÊï∞ÁªÑÂØπË±°ÁöÑ lenght Â±ûÊÄß‰πü‰ºöÊ†πÊçÆÊï∞ÁªÑÁöÑÊÉÖÂÜµÊõ¥Êñ∞„ÄÇ\nvar arr = []; arr[0] = 1; arr[1] = 2; arr.push(3); arr;\t// [1, 2, 3]  arr[arr.length] = 4;\t// equals to arr.push(4) arr;\t// [1, 2, 3, 4]  arr.unshift(0);\t// Â∞Ü‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÂÖÉÁ¥†Ê∑ªÂä†Âà∞Êï∞ÁªÑÁöÑÂºÄÂ§¥ÔºåÂπ∂ËøîÂõûËØ•Êï∞ÁªÑÁöÑÊñ∞ÈïøÂ∫¶ arr;\t// [0, 1, 2, 3, 4]  delete arr[2]; arr;\t// [0, 1, undefined, 3, 4] arr.length;\t// 5 2 in arr;\t// false  arr.length -= 1; arr;\t// [0, 1, undefined, 3], 4 is removed  arr.pop();\t// 3 returned by pop arr;\t// [0, 1, undefined], 3 is removed  arr.shift;\t// 0 returned by shift arr;\t// [1, undefined] Êï∞ÁªÑÊìç‰Ωú - Êï∞ÁªÑËø≠‰ª£ var i = 0, n = 10; var arr = [1, 2, 3, 4, 5]; for (; i \u0026lt; n; i++) { console.log(arr[i]);\t// 1, 2, 3, 4, 5 } for (i in arr) { console.log(arr[i]);\t// 1, 2, 3, 4, 5 } Array.prototype.x = \u0026#39;inherited\u0026#39;;\t// Âú®Êï∞ÁªÑÂØπË±°ÁöÑÂéüÂûã‰∏≠Â¢ûÂä†‰∏Ä‰∏™Â±ûÊÄß x for (i in arr) { console.log(arr[i]);\t// 1, 2, 3, 4, 5, inheritedÔºàÈ°∫Â∫è‰∏çÁ°ÆÂÆöÔºâ } for (i in arr) { if (arr.hasOwnProperty(i)) {\t// ËøáÊª§ÊéâÂéüÂûãÈìæ‰∏äÁöÑÂ±ûÊÄß  console.log(arr[i]);\t// 1, 2, 3, 4, 5ÔºàÈ°∫Â∫è‰∏çÁ°ÆÂÆöÔºâ  } } 5.2 ‰∫åÁª¥Êï∞ÁªÑ \u0026amp; Á®ÄÁñèÊï∞ÁªÑ ‰∫åÁª¥Êï∞ÁªÑ JavaScript ‰∏≠ÁöÑÊï∞ÁªÑÁöÑÂÖÉÁ¥†‰πüÂèØ‰ª•ÊòØÊï∞ÁªÑÔºåÂèØ‰ª•ÂÆö‰πâ‰∏Ä‰∏™Êï∞ÁªÑÔºåÂÆÉÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÂèàÊòØ‰∏Ä‰∏™Êï∞ÁªÑÔºåËøôÊ†∑Â∞±ÂΩ¢Êàê‰∫Ü‰∫åÁª¥Êï∞ÁªÑ„ÄÇ\nvar arr = [[0, 1], [2, 3], [4, 5]]; var i = 0, j = 0; var row; for (; i \u0026lt; arr.length; i++) { row = arr[i]; console.log(\u0026#39;row \u0026#39; + i); for (j = 0; j \u0026lt; row.length; j++) { console.log(row[j]); } } // RESULT: // row 0 // 0 // 1 // row 1 // 2 // 3 // row 2 // 4 // 5 Á®ÄÁñèÊï∞ÁªÑ Á®ÄÁñèÊï∞ÁªÑÔºàsparse arrayÔºâÂπ∂‰∏çÂê´Êúâ‰ªé 0 ÂºÄÂßãÁöÑËøûÁª≠Á¥¢Âºï„ÄÇ‰∏ÄËà¨ length Â±ûÊÄßÂÄºÊØîÂÆûÈôÖÂÖÉÁ¥†‰∏™Êï∞Â§ß„ÄÇÂú® JavaÔºåC++ Á≠â‰∏≠ÁöÑÊï∞ÁªÑÊòØ‰∏ÄÊÆµËøûÁª≠ÁöÑÂ≠òÂÇ®Á©∫Èó¥ÔºåÂÖÉÁ¥†‰∏éÂÖÉÁ¥†‰πãÈó¥Ê≤°ÊúâÁ©∫ÈöôÔºå‰ΩÜÊòØÂú® JavaScript ‰∏≠ÂÖÅËÆ∏Â≠òÂú®ÊúâÁ©∫ÈöôÁöÑÊï∞ÁªÑ„ÄÇ\nvar arr1 = [undefined]; var arr2 = new Array(1); 0 in arr1;\t// true 0 in arr2;\t// false ËøôÊòØÂõ†‰∏∫ÔºåÂ¶ÇÊéßÂà∂Âè∞ÊâÄÁ§∫Ôºà‰∏ãÂõæÔºâÔºåÊï∞ÁªÑ arr1 ÊòØÊúâ‰∏Ä‰∏™ÂÖÉÁ¥†ÔºàÂÄº‰∏∫ undefinedÔºâÁöÑÔºõËÄåÊï∞ÁªÑ arr2 Âç¥ÊòØ‰∏Ä‰∏™Áî± new Array ÊñπÊ≥ïÊûÑÈÄ†ÁöÑ‰∏Ä‰∏™ÈïøÂ∫¶‰∏∫ 1 ÁöÑÊï∞ÁªÑÔºåÁ¨¨ 0 ÂÖÉÁ¥†ÁöÑ key ÊòØ‰∏çÂ≠òÂú®ÁöÑ„ÄÇ\nÊàë‰ª¨ÁªßÁª≠ÂØπÊï∞ÁªÑËøõË°åÊìç‰ΩúÔºåÊéßÂà∂Âè∞‰ºöÂá∫Áé∞‰ª•‰∏ãÁªìÊûúÔºö\narr1.length = 100; arr1[99] = 123; 99 in arr1;\t// true 98 in arr2;\t// false Á±ª‰ººÂú∞Ôºå‰πüÂèØ‰ª•ËøôÊ†∑ÁöÑÊñπÂºèÂàõÂª∫Á®ÄÁñèÊï∞ÁªÑÔºö\nvar arr = [, ,]; 0 in arr;\t// false Á®ÄÁñèÊï∞ÁªÑÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Âπ∂‰∏çÂ∏∏ËßÅ„ÄÇÂú®ÈÅçÂéÜÁ®ÄÁñèÊï∞ÁªÑÊó∂ÔºåÈúÄË¶ÅÁî® in Êìç‰ΩúÁ¨¶ÊàñËÄÖ‚ÄúÊòØÂê¶‰∏∫ undefined‚ÄùÁöÑÊñπÂºè‰ΩúÁõ∏Â∫îÁöÑÂà§Êñ≠„ÄÇ\nÈÄöËøáÊü•ÈòÖÁõ∏ÂÖ≥ËµÑÊñôÔºåÁ¨îËÄÖ‰∫ÜËß£Âà∞ÔºåJavaScript ÂÆûÈôÖ‰∏äÂπ∂Ê≤°ÊúâÂ∏∏ËßÑÁöÑÊï∞ÁªÑÔºåÊâÄÊúâÁöÑÊï∞ÁªÑÂÖ∂ÂÆûÂ∞±ÊòØ‰∏™ÂØπË±°ÔºåÂè™‰∏çËøá‰ºöËá™Âä®ÁÆ°ÁêÜ‰∏Ä‰∫õ ‚ÄúÊï∞Â≠ó‚Äù Â±ûÊÄßÂíå length Â±ûÊÄßÁΩ¢‰∫Ü„ÄÇËØ¥ÁöÑÊõ¥Áõ¥Êé•‰∏ÄÁÇπÔºåJavaScript ‰∏≠ÁöÑÊï∞ÁªÑÊ†πÊú¨Ê≤°ÊúâÁ¥¢ÂºïÔºåÂõ†‰∏∫Á¥¢ÂºïÂ∫îËØ•ÊòØÊï∞Â≠óÔºåËÄå JavaScript ‰∏≠Êï∞ÁªÑÁöÑÁ¥¢ÂºïÂÖ∂ÂÆûÊòØÂ≠óÁ¨¶‰∏≤„ÄÇarr[1] ÂÖ∂ÂÆûÂ∞±ÊòØ arr[\u0026quot;1\u0026quot;]ÔºåËã•‰ª§ arr[\u0026quot;1000\u0026quot;] = 1Ôºåarr.length ‰πü‰ºöËá™Âä®Âèò‰∏∫ 1001„ÄÇËøô‰∫õË°®Áé∞ÁöÑÊ†πÊú¨ÂéüÂõ†Â∞±ÊòØÔºåJavaScript ‰∏≠ÁöÑÂØπË±°Â∞±ÊòØÂ≠óÁ¨¶‰∏≤Âà∞‰ªªÊÑèÂÄºÁöÑÈîÆÂÄºÂØπ„ÄÇÊ≥®ÊÑèÈîÆÂè™ËÉΩÊòØÂ≠óÁ¨¶‰∏≤„ÄÇ\n5.3 Êï∞ÁªÑÊñπÊ≥ï ‰∏ÄËà¨ÁöÑÂØπË±°ÈÉΩÊúâ‰∏Ä‰∏™ÂéüÂûãÔºåÊàñËÄÖÂÖ∂ÂéüÂûãÈìæÊúÄÁªàÈÉΩÊåáÂêë Object.prototypeÔºåÂõ†Ê≠§Êàë‰ª¨ÊâçËÉΩ‰ΩøÁî® hasOwnProperty Á≠âÊù•Ëá™ Object.prototype ÁöÑÊñπÊ≥ï„ÄÇÊï∞ÁªÑ‰πüÊòØ‰∏ÄÊ†∑ÁöÑÔºåÂÆÉÁöÑÂéüÂûãÊù•Ëá™ Array.prototype„ÄÇ‰ªé MDN ‰∏äÔºåÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ Array.prototype Êèê‰æõ‰∫ÜÂ§ßÈáèÁöÑÊï∞ÁªÑÊìç‰ΩúÊñπÊ≥ï„ÄÇ\nÊé•‰∏ãÊù•Â∞±ÈíàÂØπÂÖ∂‰∏≠‰∏Ä‰∫õÊñπÊ≥ïËøõË°å‰∫ÜËß£„ÄÇ\nArray.isArray() Array.isArray() Áî®‰∫éÁ°ÆÂÆö‰º†ÈÄíÁöÑÂÄºÊòØÂê¶ÊòØ‰∏Ä‰∏™ Array„ÄÇ\nArray.isArray(obj);  ÂèÇÊï∞Ôºö  objÔºöÈúÄË¶ÅÊ£ÄÊµãÁöÑÂÄº   ËøîÂõûÂÄºÔºöÂ¶ÇÊûúÂØπË±°ÊòØ ArrayÔºåÂàô‰∏∫ trueÔºõÂê¶ÂàôËøîÂõû false  Array.isArray([]);\t// true  // ÂÖ∂‰ªñÂà§Êñ≠ÊñπÂºè [].instanceof Array;\t// true ({}).toString.apply([]) === \u0026#39;[object Array]\u0026#39;;\t// true [].constructor === Array;\t// true Array.prototype.concat() concat() ÊñπÊ≥ïÁî®‰∫éÂêàÂπ∂‰∏§‰∏™ÊàñÂ§ö‰∏™Êï∞ÁªÑ„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰ºöÊõ¥ÊîπÁé∞ÊúâÊï∞ÁªÑÔºåËÄåÊòØËøîÂõû‰∏Ä‰∏™Êñ∞Êï∞ÁªÑ„ÄÇ\nvar new_array = old_array.concat([value1[, value2[, ...[, valueN]]]])  ÂèÇÊï∞Ôºö  valueN ÔºöÁî®‰∫éËøûÊé•Âà∞Êñ∞Êï∞ÁªÑÁöÑÊï∞ÁªÑ / ÂÄº   ËøîÂõûÂÄºÔºö‰∏Ä‰∏™Êñ∞ÁöÑ Array ÂÆû‰æã ÊèèËø∞Ôºö  concat ÊñπÊ≥ïÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÁªÑÔºåÂÆÉÁî±Ë¢´Ë∞ÉÁî®ÁöÑÂØπË±°‰∏≠ÁöÑÂÖÉÁ¥†ÁªÑÊàêÔºåÊØè‰∏™ÂèÇÊï∞ÁöÑÈ°∫Â∫è‰æùÊ¨°ÊòØËØ•ÂèÇÊï∞ÁöÑÂÖÉÁ¥†ÔºàÂ¶ÇÊûúÂèÇÊï∞ÊòØÊï∞ÁªÑÔºâÊàñËÄÖÂèÇÊï∞Êú¨Ë∫´ÔºàÂ¶ÇÊûúÂèÇÊï∞‰∏çÊòØÊï∞ÁªÑÔºâ„ÄÇÂÆÉ‰∏ç‰ºöÈÄíÂΩíÂà∞ÂµåÂ•óÊï∞ÁªÑÂèÇÊï∞‰∏≠ concat ÊñπÊ≥ï‰∏ç‰ºöÊîπÂèò this Êàñ‰ªª‰Ωï‰Ωú‰∏∫ÂèÇÊï∞Êèê‰æõÁöÑÊï∞ÁªÑÔºåËÄåÊòØËøîÂõû‰∏Ä‰∏™ÊµÖÊã∑Ë¥ùÔºàshallow copyÔºâÔºåÂÆÉÂåÖÂê´‰∏éÂÖÉÊòØÊï∞ÁªÑÁõ∏ÁªìÂêàÁöÑÁõ∏ÂêåÂÖÉÁ¥†ÁöÑÂâØÊú¨„ÄÇÂéüÂßãÊï∞ÁªÑÁöÑÂÖÉÁ¥†Â∞ÜÂ§çÂà∂Âà∞Êñ∞Êï∞ÁªÑ‰∏≠ÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö  ÂØπË±°ÂºïÁî®ÔºàËÄå‰∏çÊòØÂÆûÈôÖÂØπË±°ÔºâÔºöcontac Â∞ÜÂØπË±°ÂºïÁî®Â§çÂà∂Âà∞Êñ∞Êï∞ÁªÑ‰∏≠„ÄÇÂéüÂßãÊï∞ÁªÑÂíåÊñ∞Êï∞ÁªÑÈÉΩÂºïÁî®Áõ∏ÂêåÁöÑÂØπË±°„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂ¶ÇÊûúÂºïÁî®ÁöÑÂØπË±°Ë¢´‰øÆÊîπÔºåÂàôÊõ¥ÊîπÂØπ‰∫éÊñ∞Êï∞ÁªÑÂíåÂéüÂßãÊï∞ÁªÑÈÉΩÊòØÂèØËßÅÁöÑ„ÄÇÂÖ∂‰∏≠ÂåÖÊã¨Êï∞ÁªÑÂèÇÊï∞‰∏≠ÈÇ£‰∫õ‰πüÊòØÊï∞ÁªÑÁöÑÂÖÉÁ¥†„ÄÇ Êï∞ÊçÆÁ±ªÂûãÂ¶ÇÂ≠óÁ¨¶‰∏≤„ÄÅÊï∞ÁªÑÂíåÂ∏ÉÂ∞îÔºà‰∏çÊòØ String„ÄÅNumber Êàñ Boolean ÂØπË±°ÔºâÔºöconcat Â∞ÜÂ≠óÁ¨¶‰∏≤ÂíåÊï∞Â≠óÁöÑÂÄºÂ§çÂà∂Âà∞Êñ∞Êï∞ÁªÑ‰∏≠     Êï∞ÁªÑ / ÂÄºÂú®ËøûÊé•Êó∂‰øùÊåÅ‰∏çÂèò„ÄÇÊ≠§Â§ñÔºåÂØπ‰∫éÊñ∞Êï∞ÁªÑÁöÑ‰ªª‰ΩïÊìç‰ΩúÔºàÂΩì‰∏î‰ªÖÂΩìÂÖÉÁ¥†‰∏çÊòØÂØπË±°ÂºïÁî®Êó∂ÔºâÈÉΩ‰∏ç‰ºöÂØπÂéüÂßãÊï∞ÁªÑ‰∫ßÁîüÂΩ±ÂìçÔºåÂèç‰πã‰∫¶ÁÑ∂  var arr = [1, 2, 3]; arr.concat(4, 5);\t// [1, 2, 3, 4, 5] arr;\t// [1, 2, 3]ÔºåÂéüÊï∞ÁªÑÊú™Ë¢´‰øÆÊîπ  arr.concat([10, 11], 13);\t// [1, 2, 3, 10, 11, 13]Ôºå‚ÄúÂèÇÊï∞ÊòØÊï∞ÁªÑ‚Äù arr.concat([1, [2, 3]]);\t// [1, 2, 3, 1, [2, 3]]Ôºå‚ÄúÂèÇÊï∞ÂÄºÊï∞ÁªÑ‚Äù \u0026amp; ‚Äú‰∏ç‰ºöÈÄíÂΩíÂà∞ÂµåÂ•óÊï∞ÁªÑÂèÇÊï∞‰∏≠‚Äù Array.prototype.every() every() Ê£ÄÊü•Êï∞ÁªÑÁöÑÊØè‰∏™ÂÖÉÁ¥†ÊòØÂê¶ÈÉΩÈÄöËøá‰∫ÜÊåáÂÆöÂáΩÊï∞ÁöÑÊµãËØï„ÄÇ\narr.every(function callback(currentValue, index, array), thisArg);  ÂèÇÊï∞Ôºö  callbackÔºöÁî®Êù•ÊµãËØïÊï∞ÁªÑÊØè‰∏™ÂÖÉÁ¥†ÁöÑÂáΩÊï∞„ÄÇÂÆÉÊé•Âèó‰∏â‰∏™ÂèÇÊï∞Ôºö  currentValueÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº index ÂèØÈÄâ ÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâ ÔºöË∞ÉÁî®‰∫Ü every ÁöÑÊï∞ÁªÑ   thisArg ÂèØÈÄâ ÔºöÊâßË°å callback ÂáΩÊï∞Êó∂‰ΩøÁî®ÁöÑ this ÂÄº   ÊèèËø∞Ôºö  every ÊñπÊ≥ï‰∏∫Êï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÊâßË°å‰∏ÄÊ¨° callback ÂáΩÊï∞ÔºåÁõ¥Âà∞ÂÆÉÊâæÂà∞‰∏Ä‰∏™‰Ωø callback ËøîÂõû falseÔºàË°®Á§∫ÂèØËΩ¨Êç¢‰∏∫Â∏ÉÂ∞îÂÄº false ÁöÑÂÄºÔºâÁöÑÂÖÉÁ¥†„ÄÇÂ¶ÇÊûúÂèëÁé∞‰∫Ü‰∏Ä‰∏™ËøôÊ†∑ÁöÑÂÖÉÁ¥†Ôºåevery ÊñπÊ≥ïÂ∞Ü‰ºöÁ´ãÂç≥ËøîÂõû false„ÄÇÂê¶ÂàôÔºåcallback ‰∏∫ÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ËøîÂõû trueÔºåevery Â∞±‰ºöËøîÂõû true„ÄÇcallback Âè™‰ºö‰∏∫ÈÇ£‰∫õÂ∑≤ÁªèË¢´ËµãÂÄºÁöÑÁ¥¢ÂºïË∞ÉÁî®„ÄÇ‰∏ç‰ºö‰∏∫ÈÇ£‰∫õË¢´Âà†Èô§Êàñ‰ªéÊù•Ê≤°Ë¢´ËµãÂÄºÁöÑÁ¥¢ÂºïË∞ÉÁî® callback Ë¢´Ë∞ÉÁî®Êó∂‰º†ÂÖ•‰∏â‰∏™ÂèÇÊï∞ÔºöÂÖÉÁ¥†ÂÄºÔºåÂÖÉÁ¥†ÁöÑÁ¥¢ÂºïÔºåÂéüÊï∞ÁªÑ Â¶ÇÊûú‰∏∫ every Êèê‰æõ‰∏Ä‰∏™ thisArg ÂèÇÊï∞ÔºåÂàôËØ•ÂèÇÊï∞‰∏∫Ë∞ÉÁî® callback Êó∂ÁöÑ this ÂÄº„ÄÇÂ¶ÇÊûúÁúÅÁï•ËØ•ÂèÇÊï∞ÔºåÂàô callback Ë¢´Ë∞ÉÁî®Êó∂ÁöÑ this ÂÄºÔºåÂú®Èùû‰∏•Ê†ºÊ®°Âºè‰∏ã‰∏∫ÂÖ®Â±ÄÂØπË±°ÔºåÂú®‰∏•Ê†ºÊ®°Âºè‰∏ã‰º†ÂÖ• undefined every ÈÅçÂéÜÁöÑÂÖÉÁ¥†ËåÉÂõ¥Âú®Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî® callback ‰πãÂâçÂ∞±Â∑≤Á°ÆÂÆö‰∫Ü„ÄÇÂú®Ë∞ÉÁî® every ‰πãÂêéÊ∑ªÂä†Âà∞Êï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏ç‰ºöË¢´ callback ËÆøÈóÆÂà∞„ÄÇÂ¶ÇÊûúÊï∞ÁªÑ‰∏≠Â≠òÂú®ÁöÑÂÖÉÁ¥†Ë¢´Êõ¥ÊîπÔºåÂàô‰ªñ‰ª¨‰º†ÂÖ• callbackÁöÑÂÄºÊòØ every ËÆøÈóÆÂà∞‰ªñ‰ª¨ÈÇ£‰∏ÄÂàªÁöÑÂÄº„ÄÇÈÇ£‰∫õË¢´Âà†Èô§ÁöÑÂÖÉÁ¥†Êàñ‰ªéÊù•Êú™Ë¢´ËµãÂÄºÁöÑÂÖÉÁ¥†Â∞Ü‰∏ç‰ºöË¢´ËÆøÈóÆÂà∞ every ÂíåÊï∞Â≠¶‰∏≠ÁöÑ\u0026quot;ÊâÄÊúâ\u0026quot;Á±ª‰ººÔºåÂΩìÊâÄÊúâÁöÑÂÖÉÁ¥†ÈÉΩÁ¨¶ÂêàÊù°‰ª∂ÊâçËøîÂõû true„ÄÇÂè¶Â§ñÔºåÁ©∫Êï∞ÁªÑ‰πüÊòØËøîÂõû true„ÄÇÔºàÁ©∫Êï∞ÁªÑ‰∏≠ÊâÄÊúâÂÖÉÁ¥†ÈÉΩÁ¨¶ÂêàÁªôÂÆöÁöÑÊù°‰ª∂ÔºåÊ≥®ÔºöÂõ†‰∏∫Á©∫Êï∞ÁªÑÊ≤°ÊúâÂÖÉÁ¥†Ôºâ every ‰∏ç‰ºöÊîπÂèòÂéüÊï∞ÁªÑ    var arr = [1, 2, 3, 4, 5]; arr.every(function(x){ return x \u0026lt; 10; });\t// true arr.every(function(x){ return x \u0026lt; 3; });\t// false Array.prototype.filter() filter() ÊñπÊ≥ïÊ£ÄÊµãÊï∞ÁªÑÂÖÉÁ¥†ÔºåÂπ∂ËøîÂõûÁ¨¶ÂêàÊù°‰ª∂ÁöÑÊâÄÊúâÂÖÉÁ¥†ÁöÑÊï∞ÁªÑ„ÄÇ\nvar new_array = arr.filter(function callback(currentValue, index, arr), thisValue);  ÂèÇÊï∞Ôºö  callbackÔºöÁî®Êù•ÊµãËØïÊï∞ÁªÑÊØè‰∏™ÂÖÉÁ¥†ÁöÑÂáΩÊï∞„ÄÇËøîÂõû true Ë°®Á§∫‰øùÁïôËØ•ÂÖÉÁ¥†ÔºàÈÄöËøáÊµãËØïÔºâÔºå‰∏∫ false Âàô‰∏ç‰øùÁïô„ÄÇÂÆÉÊé•Âèó‰∏â‰∏™ÂèÇÊï∞Ôºö  currentValueÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº index ÂèØÈÄâ ÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâ ÔºöË∞ÉÁî®‰∫Ü filter ÁöÑÊï∞ÁªÑ   thisArg ÂèØÈÄâ ÔºöÊâßË°å callback ÂáΩÊï∞Êó∂‰ΩøÁî®ÁöÑ this ÂÄº   ËøîÂõûÂÄºÔºö‰∏Ä‰∏™Êñ∞ÁöÑÈÄöËøáÊµãËØïÁöÑÂÖÉÁ¥†ÈõÜÂêàÁöÑÊï∞ÁªÑÔºåÂ¶ÇÊûúÊ≤°ÊúâÂÖÉÁ¥†ÈÄöËøáÊµãËØïÂàôËøîÂõûÁ©∫Êï∞ÁªÑ ÊèèËø∞Ôºö  filter ‰∏∫Êï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†Ë∞ÉÁî®‰∏ÄÊ¨° callback ÂáΩÊï∞ÔºåÂπ∂Âà©Áî®ÊâÄÊúâ‰ΩøÂæó callback ËøîÂõû true Êàñ Á≠â‰ª∑‰∫é true ÁöÑÂÄº ÁöÑÂÖÉÁ¥†ÂàõÂª∫‰∏Ä‰∏™Êñ∞Êï∞ÁªÑ„ÄÇcallback Âè™‰ºöÂú®Â∑≤ÁªèËµãÂÄºÁöÑÁ¥¢Âºï‰∏äË¢´Ë∞ÉÁî®ÔºåÂØπ‰∫éÈÇ£‰∫õÂ∑≤ÁªèË¢´Âà†Èô§ÊàñËÄÖ‰ªéÊú™Ë¢´ËµãÂÄºÁöÑÁ¥¢Âºï‰∏ç‰ºöË¢´Ë∞ÉÁî®„ÄÇÈÇ£‰∫õÊ≤°ÊúâÈÄöËøá callback ÊµãËØïÁöÑÂÖÉÁ¥†‰ºöË¢´Ë∑≥ËøáÔºå‰∏ç‰ºöË¢´ÂåÖÂê´Âú®Êñ∞Êï∞ÁªÑ‰∏≠ callback Ë¢´Ë∞ÉÁî®Êó∂‰º†ÂÖ•‰∏â‰∏™ÂèÇÊï∞ÔºöÂÖÉÁ¥†ÁöÑÂÄºÔºåÂÖÉÁ¥†Á¥¢ÂºïÔºå‰ª•ÂèäË¢´ÈÅçÂéÜÁöÑÊï∞ÁªÑ Â¶ÇÊûú‰∏∫ filter Êèê‰æõ‰∏Ä‰∏™ thisArg ÂèÇÊï∞ÔºåÂàôÂÆÉ‰ºöË¢´‰Ωú‰∏∫ callback Ë¢´Ë∞ÉÁî®Êó∂ÁöÑ this ÂÄº„ÄÇÂê¶ÂàôÔºåcallback ÁöÑ this ÂÄºÂú®Èùû‰∏•Ê†ºÊ®°Âºè‰∏ãÂ∞ÜÊòØÂÖ®Â±ÄÂØπË±°Ôºå‰∏•Ê†ºÊ®°Âºè‰∏ã‰∏∫ undefined filter ÈÅçÂéÜÁöÑÂÖÉÁ¥†ËåÉÂõ¥Âú®Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî® callback ‰πãÂâçÂ∞±Â∑≤ÁªèÁ°ÆÂÆö‰∫Ü„ÄÇÂú®Ë∞ÉÁî® filter ‰πãÂêéË¢´Ê∑ªÂä†Âà∞Êï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏ç‰ºöË¢´ filter ÈÅçÂéÜÂà∞„ÄÇÂ¶ÇÊûúÂ∑≤ÁªèÂ≠òÂú®ÁöÑÂÖÉÁ¥†Ë¢´ÊîπÂèò‰∫ÜÔºåÂàô‰ªñ‰ª¨‰º†ÂÖ• callback ÁöÑÂÄºÊòØ filter ÈÅçÂéÜÂà∞ÂÆÉ‰ª¨ÈÇ£‰∏ÄÂàªÁöÑÂÄº„ÄÇË¢´Âà†Èô§Êàñ‰ªéÊù•Êú™Ë¢´ËµãÂÄºÁöÑÂÖÉÁ¥†‰∏ç‰ºöË¢´ÈÅçÂéÜÂà∞ filter ‰∏ç‰ºöÊîπÂèòÂéüÊï∞ÁªÑÔºåÂÆÉËøîÂõûËøáÊª§ÂêéÁöÑÊñ∞Êï∞ÁªÑ    var arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; arr.filter(function(x, index){ return index % 3 === 0 || x \u0026gt;= 8; });\t// returns [1, 4, 7, 8, 9, 10] arr;\t// [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Array.prototype.forEach() forEach() ÊñπÊ≥ïÂØπÊï∞ÁªÑÁöÑÊØè‰∏™ÂÖÉÁ¥†ÊâßË°åÊâßË°å‰∏ÄÊ¨°ÊåáÂÆöÂáΩÊï∞„ÄÇÂèØÁî®‰∫éÊï∞ÁªÑÈÅçÂéÜ„ÄÇ\narray.forEach(callback[, thisArg]); array.forEach(callback(currentValue, index, array){ // do something }, this)  ÂèÇÊï∞Ôºö  callbackÔºö‰∏∫Êï∞ÁªÑ‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÊâßË°åÁöÑÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞Êé•Êî∂‰∏â‰∏™ÂèÇÊï∞Ôºö  currentValueÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº index ÂèØÈÄâ ÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâ ÔºöforEach() ÊñπÊ≥ïÊ≠£Âú®Êìç‰ΩúÁöÑÊï∞ÁªÑ   thisArg ÂèØÈÄâ ÔºöÊâßË°åÂõûË∞ÉÂáΩÊï∞Êó∂Áî®‰Ωú this ÁöÑÂÄºÔºàÂèÇËÄÉÂØπË±°Ôºâ   ËøîÂõûÂÄºÔºöundefined ÊèèËø∞Ôºö  forEach ÊñπÊ≥ïÊåâÂçáÂ∫è‰∏∫Êï∞ÁªÑ‰∏≠Âê´ÊúâÊúâÊïàÂÄºÁöÑÊØè‰∏ÄÈ°πÊâßË°å‰∏ÄÊ¨° callback ÂáΩÊï∞ÔºåÈÇ£‰∫õÂ∑≤Âà†Èô§ÊàñËÄÖÊú™ÂàùÂßãÂåñÁöÑÈ°πÂ∞ÜË¢´Ë∑≥ËøáÔºàÂ¶ÇÔºåÊìç‰ΩúÁ®ÄÁñèÊï∞ÁªÑÊó∂Ôºâ callback ÂáΩÊï∞‰ºöË¢´‰æùÊ¨°‰º†ÂÖ•‰∏â‰∏™ÂèÇÊï∞Ôºö  Êï∞ÁªÑÂΩìÂâçÈ°πÁöÑÂÄº Êï∞ÁªÑÂΩìÂâçÈ°πÁöÑÁ¥¢Âºï Êï∞ÁªÑÂØπË±°Êú¨Ë∫´   Â¶ÇÊûúÁªô forEach ‰º†ÈÄí‰∫Ü thisArg ÂèÇÊï∞ÔºåÂΩìË∞ÉÁî®Êó∂ÔºåÂÆÉÂ∞ÜË¢´‰º†Áªô callback ÂáΩÊï∞Ôºå‰Ωú‰∏∫ÂÆÉÁöÑ this ÂÄº„ÄÇÂê¶ÂàôÔºåÂ∞Ü‰ºö‰º†ÂÖ• undefined ‰Ωú‰∏∫ÂÆÉÁöÑ this ÂÄº„ÄÇ forEach ÈÅçÂéÜÁöÑËåÉÂõ¥Âú®Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî® callback ÂâçÂ∞±‰ºöÁ°ÆÂÆö„ÄÇË∞ÉÁî® forEach ÂêéÊ∑ªÂä†Âà∞Êï∞ÁªÑ‰∏≠ÁöÑÈ°π‰∏ç‰ºöË¢´ callback ËÆøÈóÆÂà∞„ÄÇÂ¶ÇÊûúÂ∑≤ÁªèÂ≠òÂú®ÁöÑÂÄºË¢´ÊîπÂèòÔºåÂàô‰º†ÈÄíÁªô callback ÁöÑÂÄºÊòØ forEach ÈÅçÂéÜÂà∞ÂÆÉ‰ª¨ÈÇ£‰∏ÄÂàªÁöÑÂÄº„ÄÇÂ∑≤Âà†Èô§ÁöÑÈ°π‰∏ç‰ºöË¢´ÈÅçÂéÜÂà∞„ÄÇÂ¶ÇÊûúÂ∑≤ËÆøÈóÆÁöÑÂÖÉÁ¥†Âú®Ëø≠‰ª£Êó∂Ë¢´Âà†Èô§‰∫ÜÔºå‰πãÂêéÁöÑÂÖÉÁ¥†Â∞ÜË¢´Ë∑≥Ëøá„ÄÇ forEach() ‰∏∫ÊØè‰∏™Êï∞ÁªÑÂÖÉÁ¥†ÊâßË°å callback ÂáΩÊï∞Ôºå‰∏çÂêå‰∫é map() Êàñ reduce()ÔºåÂÆÉÊÄªÊòØËøîÂõû undefined ÂÄºÔºåÂπ∂‰∏î‰∏çÂèØÈìæÂºèË∞ÉÁî®    var arr = [1, 2, 3, 4, 5]; arr.forEach(function(x, index, a){ console.log(x + \u0026#39;|\u0026#39; + index + \u0026#39;|\u0026#39; + (a === arr)); }); // 1|0|true // 2|1|true // 3|2|true // 4|3|true // 5|4|true Array.prototype.indexOf() indexOfÔºàÔºâ ÊñπÊ≥ïËøîÂõûÂú®Êï∞ÁªÑ‰∏≠ÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™ÁªôÂÆöÂÖÉÁ¥†ÁöÑÁ¨¨‰∏Ä‰∏™Á¥¢ÂºïÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÔºåÂàôËøîÂõû -1„ÄÇ\narr.indexOf(searchElement); arr.indexOf(searchElement[, fromIndex=0])  ÂèÇÊï∞Ôºö  searchElementÔºöË¶ÅÊü•ÊâæÁöÑÂÖÉÁ¥† fromIndexÔºöÂºÄÂßãÊü•ÊâæÁöÑ‰ΩçÁΩÆ  Â¶ÇÊûúËØ•Á¥¢ÂºïÂÄºÂ§ß‰∫éÊàñÁ≠â‰∫éÊï∞ÁªÑÈïøÂ∫¶ÔºåÊÑèÂë≥ÁùÄ‰∏ç‰ºöÂú®Êï∞ÁªÑÈáåÊü•ÊâæÔºåËøîÂõû -1 Â¶ÇÊûúÂèÇÊï∞‰∏≠Êèê‰æõÁöÑÁ¥¢ÂºïÂÄºÊòØ‰∏Ä‰∏™Ë¥üÂÄºÔºåÂàôÂ∞ÜÂÖ∂‰Ωú‰∏∫Êï∞ÁªÑÊú´Â∞æÁöÑ‰∏Ä‰∏™ÊäµÊ∂àÔºåÂç≥ -1 Ë°®Á§∫‰ªéÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†ÂºÄÂßãÊü•ÊâæÔºå-2 Ë°®Á§∫‰ªéÂÄíÊï∞Á¨¨‰∫å‰∏™ÂÖÉÁ¥†ÂºÄÂßãÊü•Êâæ Ôºå‰ª•Ê≠§Á±ªÊé® Ê≥®ÊÑèÔºöÂ¶ÇÊûúÂèÇÊï∞‰∏≠Êèê‰æõÁöÑÁ¥¢ÂºïÂÄºÊòØ‰∏Ä‰∏™Ë¥üÂÄºÔºåÂπ∂‰∏çÊîπÂèòÂÖ∂Êü•ÊâæÈ°∫Â∫èÔºåÊü•ÊâæÈ°∫Â∫è‰ªçÁÑ∂ÊòØ‰ªéÂâçÂêëÂêéÊü•ËØ¢Êï∞ÁªÑ„ÄÇÂ¶ÇÊûúÊäµÊ∂àÂêéÁöÑÁ¥¢ÂºïÂÄº‰ªçÂ∞è‰∫é0ÔºåÂàôÊï¥‰∏™Êï∞ÁªÑÈÉΩÂ∞Ü‰ºöË¢´Êü•ËØ¢„ÄÇÂÖ∂ÈªòËÆ§ÂÄº‰∏∫0     ËøîÂõûÂÄºÔºöÈ¶ñ‰∏™Ë¢´ÊâæÂà∞ÁöÑÂÖÉÁ¥†Âú®Êï∞ÁªÑ‰∏≠ÁöÑÁ¥¢Âºï‰ΩçÁΩÆÔºåËã•Ê≤°ÊúâÊâæÂà∞ÂàôËøîÂõû -1 ÊèèËø∞Ôºö  indexOf ‰ΩøÁî® strict equalityÔºàÊó†ËÆ∫ÊòØ ===, ËøòÊòØ triple-equals Êìç‰ΩúÁ¨¶ÈÉΩÂü∫‰∫éÂêåÊ†∑ÁöÑÊñπÊ≥ïÔºâËøõË°åÂà§Êñ≠ searchElement ‰∏éÊï∞ÁªÑ‰∏≠ÂåÖÂê´ÁöÑÂÖÉÁ¥†‰πãÈó¥ÁöÑÂÖ≥Á≥ª    var arr = [1, 2, 3, 2, 1]; arr.indexOf(2);\t// 1 arr.indexOf(99);\t// -1 arr.indexOf(1, 1);\t// 4 arr.indexOf(1, -3);\t// 4 arr.indexOf(2, -1);\t// -1 Array.prototype.join() join() ÊñπÊ≥ïÂ∞Ü‰∏Ä‰∏™Êï∞ÁªÑÔºàÊàñ‰∏Ä‰∏™Á±ªÊï∞ÁªÑÂØπË±°ÔºâÁöÑÊâÄÊúâÂÖÉÁ¥†ËøûÊé•Êàê‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤Âπ∂ËøîÂõûËøô‰∏™Â≠óÁ¨¶„ÄÇ\nstr = arr.join(); // ÈªòËÆ§‰∏∫ \u0026#34;,\u0026#34;  str = arr.join(\u0026#34;\u0026#34;); // ÂàÜÈöîÁ¨¶ === Á©∫Â≠óÁ¨¶‰∏≤ \u0026#34;\u0026#34;  str = arr.join(separator); // ÂàÜÈöîÁ¨¶  ÂèÇÊï∞Ôºö  separatorÔºöÊåáÂÆö‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤Êù•ÂàÜÂâ≤Êï∞ÁªÑÁöÑÊØè‰∏™ÂÖÉÁ¥†  ÂøÖË¶ÅÊó∂ÔºåÂàÜÈöîÁ¨¶ separator Â∞ÜË¢´ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤ Â¶ÇÊûúÁúÅÁï• separatorÔºåÊï∞ÁªÑÊï∞ÁªÑÂÖÉÁ¥†Áî®ÈÄóÂè∑ÂàÜÈöî„ÄÇÈªòËÆ§‰∏∫ , Â¶ÇÊûú separator ÊòØÁ©∫Â≠óÁ¨¶‰∏≤ (‚Äú‚Äù)ÔºåÂàôÊâÄÊúâÂÖÉÁ¥†Â∞ÜËøûÊé•Âú®‰∏ÄËµ∑Ôºå‰∏îÂÆÉ‰ª¨‰πãÈó¥Ê≤°Êúâ‰ªª‰ΩïÂ≠óÁ¨¶     ËøîÂõûÂÄºÔºö‰∏Ä‰∏™ÊâÄÊúâÊï∞ÁªÑÂÖÉÁ¥†ËøûÊé•ÁöÑÂ≠óÁ¨¶‰∏≤„ÄÇ  Â¶ÇÊûú arr.length ‰∏∫ 0ÔºåÂàôËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤   Ê≥®ÊÑèÔºöËØ•ÊñπÊ≥ï‰∏ç‰ºöÊîπÂèòÊï∞ÁªÑ  Array.prototype.lastIndexOf() lastIndexOf() ÊñπÊ≥ïËøîÂõûÊåáÂÆöÂÖÉÁ¥†Ôºà‰πüÂç≥ÊúâÊïàÁöÑ JavaScript ÂÄºÊàñÂèòÈáèÔºâÂú®Êï∞ÁªÑ‰∏≠ÁöÑÊúÄÂêé‰∏Ä‰∏™ÁöÑÁ¥¢ÂºïÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôËøîÂõû -1„ÄÇ\narr.lastIndexOf(searchElement[, fromIndex])  ÂèÇÊï∞Ôºö  searchElementÔºöË¢´Êü•ÊâæÁöÑÂÖÉÁ¥† fromIndexÔºö‰ªéÊ≠§‰ΩçÁΩÆÂºÄÂßãÔºåÈÄÜÂêëÊü•Êâæ  ÈªòËÆ§‰∏∫Êï∞ÁªÑÈïøÂ∫¶Âáè‰∏Ä arr.length - 1ÔºåÂç≥Êï¥‰∏™Êï∞ÁªÑÈÉΩË¢´Êü•Êâæ Â¶ÇÊûúËØ•ÂÄºÂ§ß‰∫éÊàñÁ≠â‰∫éÊï∞ÁªÑÁöÑÈïøÂ∫¶ÔºåÂàôÊï¥‰∏™Êï∞ÁªÑ‰ºöË¢´Êü•Êâæ Â¶ÇÊûú‰∏∫Ë¥üÂÄºÔºåÂ∞ÜÂÖ∂ËßÜ‰∏∫‰ªéÊï∞ÁªÑÊú´Â∞æÂêëÂâçÁöÑÂÅèÁßª„ÄÇÂç≥‰ΩøËØ•ÂÄº‰∏∫Ë¥üÔºåÊï∞ÁªÑ‰ªçÁÑ∂‰ºöË¢´‰ªéÂêéÂêëÂâçÊü•Êâæ„ÄÇÂ¶ÇÊûúËØ•ÂÄº‰∏∫Ë¥üÔºå‰∏îÂÖ∂ÁªùÂØπÂÄºÂ§ß‰∫éÊï∞ÁªÑÈïøÂ∫¶ÔºåÂàôËøîÂõû -1ÔºåÂç≥Êï∞ÁªÑ‰∏ç‰ºöË¢´Êü•Êâæ     ËøîÂõûÂÄºÔºöÊï∞ÁªÑ‰∏≠ÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑÁ¥¢ÂºïÔºåÂ¶ÇÊú™ÊâæÂà∞ËøîÂõû -1 ÊèèËø∞ÔºölastIndexOf ‰ΩøÁî®‰∏•Ê†ºÁõ∏Á≠âÔºàstrict equalityÔºåÂç≥ ===ÔºâÊØîËæÉ searchElement ÂíåÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†  var arr = [1, 2, 3, 2, 1]; arr.lastIndexOf(2);\t// 3 arr.lastIndexOf(2, -2);\t// 3 arr.lastIndexOf(2, -3);\t// 1 Array.prototype.map() map() ÊñπÊ≥ïÈÄöËøáÊåáÂÆöÂáΩÊï∞Â§ÑÁêÜÊï∞‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÔºåÂπ∂ËøîÂõûÂ§ÑÁêÜÂêéÁöÑÊï∞ÁªÑ„ÄÇ\nvar new_array = array.map(function callback(currentValue, index, array), thisArg);  ÂèÇÊï∞Ôºö  callback ÔºöÁîüÊàêÊñ∞Êï∞ÁªÑÂÖÉÁ¥†ÁöÑÂáΩÊï∞Ôºå‰ΩøÁî®‰∏â‰∏™ÂèÇÊï∞Ôºö  currentValueÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº index ÂèØÈÄâ ÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâ Ôºömap ÊñπÊ≥ïË¢´Ë∞ÉÁî®ÁöÑÊï∞ÁªÑ   thisArg ÂèØÈÄâ ÔºöÊâßË°å callback ÂáΩÊï∞Êó∂‰ΩøÁî®ÁöÑ this ÂÄº   ËøîÂõûÂÄºÔºö‰∏Ä‰∏™Êñ∞Êï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÈÉΩÊòØÂõûË∞ÉÂáΩÊï∞ÁöÑÁªìÊûú ÊèèËø∞Ôºö  map ÊñπÊ≥ï‰ºöÂØπÂéüÊï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÈÉΩÊåâÈ°∫Â∫èË∞ÉÁî®‰∏ÄÊ¨° callback ÂáΩÊï∞„ÄÇcallback ÊØèÊ¨°ÊâßË°åÂêéÁöÑËøîÂõûÂÄºÔºàÂåÖÊã¨ undefinedÔºâÁªÑÂêàËµ∑Êù•ÂΩ¢Êàê‰∏Ä‰∏™Êñ∞Êï∞ÁªÑ„ÄÇcallback ÂáΩÊï∞Âè™‰ºöÂú®ÊúâÂÄºÁöÑÁ¥¢Âºï‰∏äË¢´Ë∞ÉÁî®ÔºåÈÇ£‰∫õÊú™Ë¢´ËµãÂÄºÊàñ‰ΩøÁî® delete Âà†Èô§ËøáÁöÑÁ¥¢ÂºïÂàô‰∏ç‰ºöË∞ÉÁî® callback ÂáΩÊï∞‰º†ÂÖ•‰∏â‰∏™ÂèÇÊï∞ÔºöÊï∞ÁªÑÂÖÉÁ¥†ÔºåÂÖÉÁ¥†Á¥¢ÂºïÔºå‰ª•ÂèäÂéüÊï∞ÁªÑÊú¨Ë∫´ Â¶ÇÊûú thisArg ÂèÇÊï∞ÊúâÂÄºÔºåÂàôÊØèÊ¨° callback ÂáΩÊï∞Ë¢´Ë∞ÉÁî®ÁöÑÊó∂ÂÄôÔºåthis ÈÉΩ‰ºöÊåáÂêë thisArg ÂèÇÊï∞‰∏äÁöÑËøô‰∏™ÂØπË±°„ÄÇÂ¶ÇÊûúÁúÅÁï•‰∫Ü thisArg ÂèÇÊï∞ÔºåÊàñËÄÖËµãÂÄº‰∏∫ null Êàñ undefinedÔºåÂàô this ÊåáÂêëÂÖ®Â±ÄÂØπË±° map() Êú¨Ë∫´‰∏ç‰øÆÊîπË∞ÉÁî®ÂÆÉÁöÑÂéüÊï∞ÁªÑÔºà‰ΩÜÊòØÂèØ‰ª•Âú® callback ÊâßË°åÊó∂ÊîπÂèòÂéüÊï∞ÁªÑÔºâ ‰ΩøÁî® map() ÊñπÊ≥ïÂ§ÑÁêÜÊï∞ÁªÑÊó∂ÔºåÊï∞ÁªÑÂÖÉÁ¥†ÁöÑËåÉÂõ¥ÊòØÂú® callback ÊñπÊ≥ïÁ¨¨‰∏ÄÊ¨°Ë∞ÉÁî®‰πãÂâçÂ∞±Â∑≤ÁªèÁ°ÆÂÆö‰∫ÜÁöÑ„ÄÇÂú® map ÊñπÊ≥ïÊâßË°åÁöÑËøáÁ®ã‰∏≠ÔºåÂéüÊï∞ÁªÑ‰∏≠Êñ∞Â¢ûÂä†ÁöÑÂÖÉÁ¥†Â∞Ü‰∏ç‰ºöË¢´ callback ËÆøÈóÆÂà∞ÔºõËã•Â∑≤ÁªèÂ≠òÂú®ÁöÑÂÖÉÁ¥†Ë¢´ÊîπÂèò‰∫ÜÊàñÂà†Èô§‰∫ÜÔºåÂàôÂÆÉ‰ª¨‰º†ÈÄíÂà∞ callback ÁöÑÂÄºÊòØ map ÊñπÊ≥ïÈÅçÂéÜÂà∞ÂÆÉ‰ª¨ÈÇ£‰∏ÄÂàªÁöÑÂÄºÔºõËÄåË¢´Âà†Èô§ÁöÑÂÖÉÁ¥†Â∞Ü‰∏ç‰ºöË¢´ËÆøÈóÆÂà∞    var arr = [1, 2, 3]; arr.map(function(x){ return x + 10; });\t// [11, 12, 13] arr;\t// [1, 2, 3] Array.prototype.reduce() reduce() ÊñπÊ≥ïÂØπÊï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÊâßË°å‰∏Ä‰∏™Áî±ÊÇ®Êèê‰æõÁöÑ reducer ÂáΩÊï∞ÔºàÂçáÂ∫èÊâßË°åÔºâÔºåÂ∞ÜÂÖ∂ÁªìÊûúÊ±áÊÄª‰∏∫Âçï‰∏™ËøîÂõûÂÄº„ÄÇ\narray.reduce(function callback(accumulator, currentValue, currentIndex, array), initialValue);  ÂèÇÊï∞Ôºö  callbackÔºö‰∏∫Êï∞ÁªÑ‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÊâßË°åÁöÑÂáΩÊï∞ÔºåÂåÖÂê´Âõõ‰∏™ÂèÇÊï∞Ôºö  accumulatorÔºöÁ¥ØËÆ°Âô®Á¥ØËÆ°ÂõûË∞ÉÁöÑËøîÂõûÂÄº„ÄÇÂÆÉÊòØÂÖàÂâçÂú®ÂõûË∞É‰∏≠ËøîÂõûÁöÑÁ¥ØÁßØÂÄºÔºåÊàñ initialValue currentValueÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº currentIndex ÂèØÈÄâÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâÔºöË∞ÉÁî®‰∫Ü reduce() ÁöÑÊï∞ÁªÑ   initialValue ÂèØÈÄâÔºö‰Ωú‰∏∫Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî® callback ÂáΩÊï∞Êó∂Á¨¨‰∏Ä‰∏™ÂèÇÊï∞ÁöÑÂÄº„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÊèê‰æõÂàùÂßãÂÄºÔºåÂàôÂ∞Ü‰ΩøÁî®   ËøîÂõûÂÄºÔºöÂáΩÊï∞Á¥ØÁßØÂ§ÑÁêÜÁöÑÁªìÊûú ÊèèËø∞Ôºö  reduce‰∏∫Êï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†‰æùÊ¨°ÊâßË°åcallbackÂáΩÊï∞Ôºå‰∏çÂåÖÊã¨Êï∞ÁªÑ‰∏≠Ë¢´Âà†Èô§Êàñ‰ªéÊú™Ë¢´ËµãÂÄºÁöÑÂÖÉÁ¥†ÔºåÊé•ÂèóÂõõ‰∏™ÂèÇÊï∞Ôºö  accumulator Á¥ØËÆ°Âô® currentValue ÂΩìÂâçÂÄº currentIndex ÂΩìÂâçÁ¥¢Âºï array Êï∞ÁªÑ   ÂõûË∞ÉÂáΩÊï∞Á¨¨‰∏ÄÊ¨°ÊâßË°åÊó∂Ôºåaccumulator Âíå currentValueÁöÑÂèñÂÄºÊúâ‰∏§ÁßçÊÉÖÂÜµÔºöÂ¶ÇÊûúË∞ÉÁî®reduce() Êó∂Êèê‰æõ‰∫Ü initialValueÔºåaccumulator ÂèñÂÄº‰∏∫ initialValueÔºåcurrentValue ÂèñÊï∞ÁªÑ‰∏≠ÁöÑÁ¨¨‰∏Ä‰∏™ÂÄºÔºõÂ¶ÇÊûúÊ≤°ÊúâÊèê‰æõ initialValueÔºåÈÇ£‰πà accumulator ÂèñÊï∞ÁªÑ‰∏≠ÁöÑÁ¨¨‰∏Ä‰∏™ÂÄºÔºåcurrentValue ÂèñÊï∞ÁªÑ‰∏≠ÁöÑÁ¨¨‰∫å‰∏™ÂÄº**Ê≥®ÊÑèÔºö**Â¶ÇÊûúÊ≤°ÊúâÊèê‰æõinitialValueÔºåreduce ‰ºö‰ªéÁ¥¢Âºï 1 ÂºÄÂßãÊâßË°å callback ÊñπÊ≥ïÔºåË∑≥ËøáÁ¨¨‰∏Ä‰∏™Á¥¢ÂºïÔºàÁ¥¢Âºï 0Ôºâ„ÄÇÂ¶ÇÊûúÊèê‰æõ initialValueÔºåÂàô‰ªéÁ¥¢Âºï 0 ÂºÄÂßã Â¶ÇÊûúÊï∞ÁªÑ‰∏∫Á©∫‰∏îÊ≤°ÊúâÊèê‰æõ initialValueÔºå‰ºöÊäõÂá∫ TypeError „ÄÇÂ¶ÇÊûúÊï∞ÁªÑ‰ªÖÊúâ‰∏Ä‰∏™ÂÖÉÁ¥†ÔºàÊó†ËÆ∫‰ΩçÁΩÆÂ¶Ç‰ΩïÔºâÂπ∂‰∏îÊ≤°ÊúâÊèê‰æõ initialValueÔºå ÊàñËÄÖÊúâÊèê‰æõ initialValue ‰ΩÜÊòØÊï∞ÁªÑ‰∏∫Á©∫ÔºåÈÇ£‰πàÊ≠§ÂîØ‰∏ÄÂÄºÂ∞ÜË¢´ËøîÂõûÂπ∂‰∏î callback ‰∏ç‰ºöË¢´ÊâßË°å reduce ‰∏ç‰ºöÊîπÂèòÂéüÊï∞ÁªÑ    var arr = [1, 2, 3]; var sum = arr.reduce(function(x, y){ return x + y; }, 0);\t// 6 arr;\t// [1, 2, 3]  arr = [3, 9, 6]; var max = arr.reduce(function(x, y){ console.log(x + \u0026#39;|\u0026#39; + y); return x \u0026gt; y ? x : y; }); // 3|9 // 9|6 max;\t// 9 Array.prototype.reduceRight() reduceRight() ÊñπÊ≥ïÁ±ª‰ºº‰∫é reduce() ÊñπÊ≥ïÔºå‰∏çËøáÊòØÈôçÂ∫èÊâßË°åÁöÑÔºà‰ªéÂè≥Âà∞Â∑¶Ôºâ„ÄÇ\narray.reduceRight(function callback(previousValue, currentValue, currentIndex, array), initialValue);  ÂèÇÊï∞Ôºö  callbackÔºö‰∏∫Êï∞ÁªÑ‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÊâßË°åÁöÑÂáΩÊï∞ÔºåÂåÖÂê´Âõõ‰∏™ÂèÇÊï∞Ôºö  previousValueÔºöÁ¥ØËÆ°Âô®Á¥ØËÆ°ÂõûË∞ÉÁöÑËøîÂõûÂÄº„ÄÇÂÆÉÊòØÂÖàÂâçÂú®ÂõûË∞É‰∏≠ËøîÂõûÁöÑÁ¥ØÁßØÂÄºÔºåÊàñ initialValue currentValueÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº currentIndex ÂèØÈÄâÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâÔºöË∞ÉÁî®‰∫Ü reduce() ÁöÑÊï∞ÁªÑ   initialValue ÂèØÈÄâÔºö‰Ωú‰∏∫Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî® callback ÂáΩÊï∞Êó∂Á¨¨‰∏Ä‰∏™ÂèÇÊï∞ÁöÑÂÄº„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÊèê‰æõÂàùÂßãÂÄºÔºåÂàôÂ∞Ü‰ΩøÁî®   ËøîÂõûÂÄºÔºöÂáΩÊï∞Á¥ØÁßØÂ§ÑÁêÜÁöÑÁªìÊûú ÊèèËø∞Ôºö  reduceRight ‰∏∫Êï∞ÁªÑ‰∏≠ÊØè‰∏™ÂÖÉÁ¥†Ë∞ÉÁî®‰∏ÄÊ¨° callback ÂõûË∞ÉÂáΩÊï∞Ôºå‰ΩÜÊòØÊï∞ÁªÑ‰∏≠Ë¢´Âà†Èô§ÁöÑÁ¥¢ÂºïÊàñ‰ªéÊú™Ë¢´ËµãÂÄºÁöÑÁ¥¢Âºï‰ºöË∑≥Ëøá„ÄÇÂõûË∞ÉÂáΩÊï∞Êé•ÂèóÂõõ‰∏™ÂèÇÊï∞ÔºöÂàùÂßãÂÄºÔºàÊàñ‰∏äÊ¨°Ë∞ÉÁî®ÂõûË∞ÉÁöÑËøîÂõûÂÄºÔºâ„ÄÅÂΩìÂâçÂÖÉÁ¥†ÂÄº„ÄÅÂΩìÂâçÁ¥¢ÂºïÔºå‰ª•ÂèäË∞ÉÁî® reduceRight ÁöÑÊï∞ÁªÑ„ÄÇ ÂõûË∞ÉÂáΩÊï∞Á¨¨‰∏ÄÊ¨°ÊâßË°åÊó∂ÔºåpreviousValue Âíå currentValueÁöÑÂèñÂÄºÊúâ‰∏§ÁßçÊÉÖÂÜµÔºöÂ¶ÇÊûúË∞ÉÁî®reduceRight() Êó∂Êèê‰æõ‰∫Ü initialValueÔºåÂàô previousValue ÂèñÂÄº‰∏∫ initialValueÔºåcurrentValue ÂèñÊï∞ÁªÑ‰∏≠ÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑÂÄºÔºõÂ¶ÇÊûúÊ≤°ÊúâÊèê‰æõ initialValueÔºåÈÇ£‰πà previousValue ÂèñÊï∞ÁªÑ‰∏≠ÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÄºÔºåcurrentValue ÂèñÊï∞ÁªÑ‰∏≠ÁöÑÂÄíÊï∞Á¨¨‰∫å‰∏™ÂÄº Â¶ÇÊûúÊï∞ÁªÑ‰∏∫Á©∫‰∏îÊ≤°ÊúâÊèê‰æõ initialValueÔºå‰ºöÊäõÂá∫ TypeError „ÄÇÂ¶ÇÊûúÊï∞ÁªÑ‰ªÖÊúâ‰∏Ä‰∏™ÂÖÉÁ¥†ÔºàÊó†ËÆ∫‰ΩçÁΩÆÂ¶Ç‰ΩïÔºâÂπ∂‰∏îÊ≤°ÊúâÊèê‰æõ initialValueÔºå ÊàñËÄÖÊúâÊèê‰æõ initialValue ‰ΩÜÊòØÊï∞ÁªÑ‰∏∫Á©∫ÔºåÈÇ£‰πàÊ≠§ÂîØ‰∏ÄÂÄºÂ∞ÜË¢´ËøîÂõûÂπ∂‰∏î callback ‰∏ç‰ºöË¢´ÊâßË°å reduceRight ‰∏ç‰ºöÊîπÂèòÂéüÊï∞ÁªÑ    var arr = [3, 9, 6]; var max = arr.reduceRight(function(x, y){ console.log(x + \u0026#39;|\u0026#39; + y); return x \u0026gt; y ? x : y; }); // 6|9 // 9|3 max;\t// 9 Array.prototype.reverse() reverse() ÊñπÊ≥ïÂ∞ÜÊï∞ÁªÑ‰∏≠ÂÖÉÁ¥†ÁöÑ‰ΩçÁΩÆÈ¢†ÂÄí\narr.reverse();  ÂèÇÊï∞ÔºöÊó† ÊèèËø∞Ôºöreverse() ÊñπÊ≥ïÈ¢†ÂÄíÊï∞ÁªÑ‰∏≠ÂÖÉÁ¥†ÁöÑ‰ΩçÁΩÆÔºåÂπ∂ËøîÂõûËØ•Êï∞ÁªÑÁöÑÂºïÁî®  Array.prototype.slice() slice() ÊñπÊ≥ïËøîÂõû‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÁªÑÂØπË±°ÔºåËøô‰∏ÄÂØπË±°ÊòØ‰∏Ä‰∏™Áî± begin Âíå endÔºà‰∏çÂåÖÊã¨ endÔºâÂÜ≥ÂÆöÁöÑÂéüÂßãÊï∞ÁªÑÁöÑÊµÖÊã∑Ë¥ù„ÄÇÂéüÂßãÊï∞ÁªÑ‰∏ç‰ºöÊîπÂèò„ÄÇ\narr.slice();\t// [0, end]  arr.slice(begin);\t// [begin, end]  arr.slice(begin, end); // [begin, end)  ÂèÇÊï∞Ôºö  begin ÂèØÈÄâÔºö‰ªéÁ¥¢ÂºïÂ§ÑÂºÄÂßãÊèêÂèñÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†  Â¶ÇÊûúËØ•ÂèÇÊï∞‰∏∫Â§çÊï∞ÔºåÂàôË°®Á§∫‰ªéÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÄíÊï∞Á¨¨Âá†‰∏™ÂÖÉÁ¥†ÂºÄÂßãÊèêÂèñ„ÄÇÂ¶ÇÔºöslice(-2) Ë°®Á§∫ÊèêÂèñÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÄíÊï∞Á¨¨‰∫å‰∏™ÂÖÉÁ¥†Âà∞ÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†ÔºàÂåÖÂê´ÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†Ôºâ Â¶ÇÊûúÁúÅÁï• beginÔºåÂàô slice ‰ªéÁ¥¢Âºï 0 ÂºÄÂßã   end ÂèØÈÄâÔºöÂú®ËØ•Á¥¢ÂºïÂ§ÑÁªìÊùüÊèêÂèñÂéüÊï∞ÁªÑÂÖÉÁ¥†  slice ‰ºöÊèêÂèñÂéüÊï∞ÁªÑ‰∏≠‰ªé begin Âà∞ end ÁöÑÊâÄÊúâÂÖÉÁ¥†ÔºàÂåÖÂê´ begin ‰ΩÜ‰∏çÂåÖÂê´ endÔºâ Â¶ÇÊûúËØ•ÂèÇÊï∞‰∏∫Ë¥üÊï∞ÔºåÂàô‰ªñË°®Á§∫Âú®ÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÄíÊï∞Á¨¨Âá†‰∏™ÂÖÉÁ¥†ÁªìÊùüÊäΩÂèñ„ÄÇÂ¶ÇÔºöslice(-2, -1) Ë°®Á§∫ÊäΩÂèñ‰∫ÜÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÄíÊï∞Á¨¨‰∫å‰∏™ÂÖÉÁ¥†Âà∞ÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†Ôºà‰∏çÂåÖÂê´ÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÂç≥Âè™ÊúâÂÄíÊï∞Á¨¨‰∫å‰∏™ÂÖÉÁ¥†Ôºâ Â¶ÇÊûú end Ë¢´ÁúÅÁï•ÔºåÂàô slice ‰ºö‰∏ÄÁõ¥ÊèêÂèñÂà∞ÂéüÊï∞ÁªÑÊú´Â∞æ Â¶ÇÊûú end Â§ß‰∫éÊï∞ÁªÑÈïøÂ∫¶Ôºåslice ‰πü‰ºö‰∏ÄÁõ¥ÊèêÂèñÂà∞ÂéüÊï∞ÁªÑÊú´Â∞æ     ËøîÂõûÂÄºÔºö‰∏Ä‰∏™Âê´ÊúâÊèêÂèñÂÖÉÁ¥†ÁöÑÊñ∞Êï∞ÁªÑ ÊèèËø∞Ôºö  slice ‰∏ç‰øÆÊîπÂéüÊï∞ÁªÑÔºåÂè™‰ºöËøîÂõû‰∏Ä‰∏™ÊµÖÂ§çÂà∂‰∫ÜÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†ÁöÑ‰∏Ä‰∏™Êñ∞Êï∞ÁªÑ„ÄÇÂéüÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰ºöÊåâÁÖß‰∏ãËø∞ËßÑÂàôÊã∑Ë¥ùÔºö  Â¶ÇÊûúËØ•ÂÖÉÁ¥†ÊòØ‰∏Ä‰∏™ÂØπË±°ÂºïÁî®Ôºà‰∏çÊòØÂÆûÈôÖÁöÑÂØπË±°ÔºâÔºåslice ‰ºöÊã∑Ë¥ùËøô‰∏™ÂØπË±°ÂºïÁî®Âà∞Êñ∞ÁöÑÊï∞ÁªÑÈáå„ÄÇ‰∏§‰∏™ÂØπË±°ÂºïÁî®ÈÉΩÂºïÁî®‰∫ÜÂêå‰∏Ä‰∏™ÂØπË±°„ÄÇÂ¶ÇÊûúË¢´ÂºïÁî®ÁöÑÂØπË±°ÂèëÁîüÊîπÂèòÔºåÂàôÊñ∞ÁöÑÂíåÂéüÊù•ÁöÑÊï∞ÁªÑ‰∏≠Ëøô‰∏™ÂÖÉÁ¥†‰πü‰ºöÂèëÁîüÊîπÂèò ÂØπ‰∫éÂ≠óÁ¨¶‰∏≤„ÄÅÊï∞Â≠óÂèäÂ∏ÉÂ∞îÂÄºÊù•ËØ¥Ôºà‰∏çÊòØ String„ÄÅNumber Êàñ Boolean ÂØπË±°ÔºâÔºåslice‰ºöÊã∑Ë¥ùËøô‰∫õÂÄºÂà∞Êñ∞ÁöÑÊï∞ÁªÑÈáå„ÄÇÂú®Âà´ÁöÑÊï∞ÁªÑÈáå‰øÆÊîπËøô‰∫õÂ≠óÁ¨¶‰∏≤„ÄÅÊï∞Â≠óÊàñÂ∏ÉÂ∞îÂÄºÔºå‰∏ç‰ºöÂΩ±ÂìçÂè¶‰∏Ä‰∏™Êï∞ÁªÑ   ÂêëÊñ∞Êï∞ÁªÑ‰∏≠Ê∑ªÂä†ÊàñÂà†Èô§ÂÖÉÁ¥†ÔºåÂéüÊï∞ÁªÑ‰∏ç‰ºöÂèóÂà∞ÂΩ±ÂìçÔºåÂèç‰πã‰∫¶ÁÑ∂    var arr = [1, 2, 3, 4, 5]; arr.slice(1, 3);\t// [2, 3] arr.slice(1);\t// [2, 3, 4, 5] arr.slice(1, -1);\t// [2, 3, 4] arr.slice(-4, -3);\t// [2] Array.prototype.some() some() ÊñπÊ≥ïÊ£ÄÊµãÊï∞ÁªÑÂÖÉÁ¥†‰∏≠ÊòØÂê¶ÊúâÂÖÉÁ¥†Á¨¶ÂêàÊåáÂÆöÊù°‰ª∂„ÄÇ\narray.some(function callback(currentValue, index, array), thisArg);  ÂèÇÊï∞Ôºö  callbackÔºöÁî®Êù•ÊµãËØïÊØè‰∏™ÂÖÉÁ¥†ÁöÑÂáΩÊï∞ÔºåÊé•Âèó‰∏â‰∏™ÂèÇÊï∞Ôºö  elementÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÂÄº index ÂèØÈÄâ ÔºöÊï∞ÁªÑ‰∏≠ÂΩìÂâçÊ≠£Âú®Â§ÑÁêÜÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï array ÂèØÈÄâ ÔºöË∞ÉÁî®‰∫Ü some() ÁöÑÊï∞ÁªÑ   thisArg ÂèØÈÄâ ÔºöÊâßË°å callback ÂáΩÊï∞Êó∂‰ΩøÁî®ÁöÑ this ÂÄº   ËøîÂõûÂÄºÔºöÂ∏ÉÂ∞îÂÄº„ÄÇÂ¶ÇÊûúÊï∞ÁªÑ‰∏≠ÊúâÂÖÉÁ¥†Êª°Ë∂≥Êù°‰ª∂ËøîÂõû trueÔºåÂê¶ÂàôËøîÂõû false ÊèèËø∞Ôºö  some() ‰∏∫Êï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÊâßË°å‰∏ÄÊ¨° callback ÂáΩÊï∞ÔºåÁõ¥Âà∞ÊâæÂà∞‰∏Ä‰∏™‰ΩøÂæó callback ËøîÂõû‰∏Ä‰∏™‚ÄúÁúüÂÄº‚ÄùÔºàÂç≥ÂèØËΩ¨Êç¢‰∏∫Â∏ÉÂ∞îÂÄº true ÁöÑÂÄºÔºâ„ÄÇÂ¶ÇÊûúÊâæÂà∞‰∫ÜËøôÊ†∑‰∏Ä‰∏™ÂÄºÔºåsome() Â∞Ü‰ºöÁ´ãÂç≥ËøîÂõû true„ÄÇÂê¶ÂàôÔºåsome() ËøîÂõû false„ÄÇcallback Âè™‰ºöÂú®ÈÇ£‰∫õ‚ÄùÊúâÂÄº‚ÄúÁöÑÁ¥¢Âºï‰∏äË¢´Ë∞ÉÁî®Ôºå‰∏ç‰ºöÂú®ÈÇ£‰∫õË¢´Âà†Èô§Êàñ‰ªéÊú™Ë¢´ËµãÂÄºÁöÑÁ¥¢Âºï‰∏äË∞ÉÁî® callback Ë¢´Ë∞ÉÁî®Êó∂‰º†ÂÖ•‰∏â‰∏™ÂèÇÊï∞ÔºöÂÖÉÁ¥†ÁöÑÂÄºÔºåÂÖÉÁ¥†ÁöÑÁ¥¢ÂºïÔºåË¢´ÈÅçÂéÜÁöÑÊï∞ÁªÑ Â¶ÇÊûú‰∏∫ every Êèê‰æõ‰∏Ä‰∏™ thisArg ÂèÇÊï∞ÔºåÂàôËØ•ÂèÇÊï∞‰∏∫Ë∞ÉÁî® callback Êó∂ÁöÑ this ÂÄº„ÄÇÂ¶ÇÊûúÁúÅÁï•ËØ•ÂèÇÊï∞ÔºåÂàô callback Ë¢´Ë∞ÉÁî®Êó∂ÁöÑ this ÂÄºÔºåÂú®Èùû‰∏•Ê†ºÊ®°Âºè‰∏ã‰∏∫ÂÖ®Â±ÄÂØπË±°ÔºåÂú®‰∏•Ê†ºÊ®°Âºè‰∏ã‰º†ÂÖ• undefined some() ÈÅçÂéÜÁöÑÂÖÉÁ¥†ÁöÑËåÉÂõ¥Âú®Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî® callback. Êó∂Â∞±Â∑≤ÁªèÁ°ÆÂÆö‰∫Ü„ÄÇÂú®Ë∞ÉÁî® some() ÂêéË¢´Ê∑ªÂä†Âà∞Êï∞ÁªÑ‰∏≠ÁöÑÂÄº‰∏ç‰ºöË¢´ callback ËÆøÈóÆÂà∞„ÄÇÂ¶ÇÊûúÊï∞ÁªÑ‰∏≠Â≠òÂú®‰∏îËøòÊú™Ë¢´ËÆøÈóÆÂà∞ÁöÑÂÖÉÁ¥†Ë¢´ callbackÊîπÂèò‰∫ÜÔºåÂàôÂÖ∂‰º†ÈÄíÁªô callback ÁöÑÂÄºÊòØ some() ËÆøÈóÆÂà∞ÂÆÉÈÇ£‰∏ÄÂàªÁöÑÂÄº some() Ë¢´Ë∞ÉÁî®Êó∂‰∏ç‰ºöÊîπÂèòÊï∞ÁªÑ    var arr = [1, 2, 3, 4, 5]; arr.some(function(x){ return x === 3; });\t// true arr.some(function(x){ return x === 100; });\t// false Array.prototype.sort() sort() ÊñπÊ≥ïÁî®ÂéüÂú∞ÁÆóÊ≥ïÂØπÊï∞ÁªÑÁöÑÂÖÉÁ¥†ËøõË°åÊéíÂ∫èÔºåÂπ∂ËøîÂõûÊï∞ÁªÑÔºàÂéüÊï∞ÁªÑË¢´‰øÆÊîπÔºâ„ÄÇ\narr.sort(compareFunction);  ÂèÇÊï∞Ôºö  compareFunction ÂèØÈÄâÔºöÁî®‰∫éÊåáÂÆöÊåâÊüêÁßçÈ°∫Â∫èËøõË°åÊéíÂàóÁöÑÂáΩÊï∞„ÄÇÂ¶ÇÊûúÁúÅÁï•ÔºåÂàôÂ∞ÜÊØè‰∏™ÂÖÉÁ¥†ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤ÔºåÊ†πÊçÆÂÖ∂ Unicode ‰ΩçÁÇπÂÄºËøõË°åÊéíÂ∫è„ÄÇ   ËøîÂõûÂÄºÔºöÊéíÂ∫èÂêéÁöÑÊï∞ÁªÑÔºàÊï∞ÁªÑÂ∑≤ÂéüÂú∞ÊéíÂ∫èÔºåÂπ∂‰∏î‰∏çËøõË°åÂ§çÂà∂Ôºâ„ÄÇ Ê≥®ÊÑèÔºö  Â¶ÇÊûúÊ≤°ÊúâÊåáÊòé compareFunction ÔºåÈÇ£‰πàÂÖÉÁ¥†‰ºöÊåâÁÖßËΩ¨Êç¢‰∏∫ÁöÑÂ≠óÁ¨¶‰∏≤ÁöÑËØ∏‰∏™Â≠óÁ¨¶ÁöÑUnicode‰ΩçÁÇπËøõË°åÊéíÂ∫è„ÄÇ‰æãÂ¶Ç \u0026ldquo;Banana\u0026rdquo; ‰ºöË¢´ÊéíÂàóÂà∞ \u0026ldquo;cherry\u0026rdquo; ‰πãÂâç„ÄÇÂΩìÊï∞Â≠óÊåâÁî±Â∞èÂà∞Â§ßÊéíÂ∫èÊó∂Ôºå9 Âá∫Áé∞Âú® 80 ‰πãÂâçÔºå‰ΩÜÂõ†‰∏∫ÔºàÊ≤°ÊúâÊåáÊòé compareFunctionÔºâÔºåÊØîËæÉÁöÑÊï∞Â≠ó‰ºöÂÖàË¢´ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤ÔºåÊâÄ‰ª•Âú®UnicodeÈ°∫Â∫è‰∏ä \u0026ldquo;80\u0026rdquo; Ë¶ÅÊØî \u0026ldquo;9\u0026rdquo; Ë¶ÅÈù†Ââç Â¶ÇÊûúÊåáÊòé‰∫Ü compareFunction ÔºåÂàôÊ†πÊçÆËØ•ÂáΩÊï∞ÁöÑËøîÂõûÂÄºÂØπÊâÄÊúâ ‚ÄúÈùû undefined‚Äù ÁöÑÊï∞ÁªÑÂÖÉÁ¥†ËøõË°åÊéíÂ∫èÔºàÊâÄÊúâÁöÑ undefined ÂÖÉÁ¥†ÈÉΩÊéíÂà∞Êï∞ÁªÑÁöÑÊú´Â∞æÔºå‰∏î‰∏çË∞ÉÁî® compareFunctionÔºâ„ÄÇÂ¶ÇÊûú a Âíå b ÊòØ‰∏§‰∏™Ë¢´ÊØîËæÉÁöÑÂÖÉÁ¥†ÔºåÈÇ£‰πàÔºö  ÂΩì compareFunction(a, b) ËøîÂõûÂÄºÂ∞è‰∫é 0 Êó∂Ôºåa ÊéíÂú® b ‰πãÂâçÔºàa ÁöÑÁ¥¢Âºï‰Ωé‰∫é bÔºâ ÂΩì compareFunction(a, b) ËøîÂõûÂÄºÁ≠â‰∫é 0 Êó∂Ôºåa Âíå b Áõ∏ÂØπ‰ΩçÁΩÆ‰∏çÂèò ÂΩì compareFunction(a, b) ËøîÂõûÂÄºÂ§ß‰∫é 0 Êó∂Ôºåa ÊéíÂú® b ‰πãÂêé   sort() ÊñπÊ≥ïÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÂÆûÁé∞ÔºåÂõ†Ê≠§Êó†Ê≥ï‰øùËØÅÊéíÂ∫èÁöÑÊó∂Èó¥ÂíåÁ©∫Èó¥Â§çÊùÇÊÄß    var arr = [13, 24, 51, 3]; arr.sort();\t// [13, 24, 3, 51] arr;\t// [13, 24, 3, 51]  arr.sort(function(a, b){ return a - b; });\t// [3, 13, 24, 51]  var arr1 = [{age: 25}, {age: 39}, {age: 99}]; arr.sort(function(a, b){ return a.age - b.age; }); arr.forEach(function(item){\t// forEach() ÈÅçÂéÜ  console.log(\u0026#39;age\u0026#39;, item.age); }); // RESULT: // age 25 // age 39 // age 99 Array.prototype.splice() splice() ÊñπÊ≥ïÈÄöËøáÂà†Èô§Áé∞ÊúâÂÖÉÁ¥† / Ê∑ªÂä†Êñ∞ÂÖÉÁ¥†Êù•‰øÆÊîπÊï∞ÁªÑ„ÄÇÂπ∂‰ª•Êï∞ÁªÑÂΩ¢ÂºèËøîÂõûÂéüÊï∞ÁªÑ‰∏≠Ë¢´‰øÆÊîπÁöÑÂÜÖÂÆπ\narray.splice(start[, deleteCount[, item1[, item2[, ...]]]])  ÂèÇÊï∞Ôºö  startÔºöÊåáÂÆö‰øÆÊîπÁöÑÂºÄÂßã‰ΩçÁΩÆÔºà‰ªé 0 ËÆ°Êï∞Ôºâ  Â¶ÇÊûúË∂ÖËøá‰∫ÜÊï∞ÁªÑÁöÑÈïøÂ∫¶ÔºåÂàô‰ªéÊï∞ÁªÑÊú´Â∞æÂºÄÂßãÊ∑ªÂä†ÂÜÖÂÆπ Â¶ÇÊûúÊòØË¥üÂÄºÔºåÂàôË°®Á§∫‰ªéÊï∞ÁªÑÂÄíÊï∞Á¨¨Âá†‰∏™ÂÖÉÁ¥†ÂºÄÂßã Â¶ÇÊûúË¥üÊï∞ÁöÑÁªùÂØπÂÄºÂ§ß‰∫éÊï∞ÁªÑÁöÑÈïøÂ∫¶ÔºåÂàôË°®Á§∫ÂºÄÂßã‰ΩçÁΩÆ‰∏∫Á¨¨ 0 ‰Ωç   deleteCount ÂèØÈÄâÔºöÊï¥Êï∞ÔºåË°®Á§∫Ë¶ÅÁßªÈô§ÁöÑÊï∞ÁªÑÂÖÉÁ¥†ÁöÑ‰∏™Êï∞  Â¶ÇÊûú deleteCount ÊòØ 0 ÊàñË¥üÊï∞ÔºåÂàô‰∏çÁßªÈô§ÂÖÉÁ¥†„ÄÇËøôÁßçÊÉÖÂÜµ‰∏ãÔºåËá≥Â∞ëÂ∫îÔºàÂú® start ‰Ωç‰πãÂâçÔºâÊ∑ªÂä†‰∏Ä‰∏™Êñ∞ÂÖÉÁ¥† Â¶ÇÊûú deleteCount Â§ß‰∫é start ‰πãÂêéÁöÑÂÖÉÁ¥†ÁöÑÊÄªÊï∞ÔºåÂàô‰ªé start ÂºÄÂßãÔºå‰πãÂêéÁöÑÂÖÉÁ¥†Ë±ÜÊµÜÊùØÂà†Èô§ÔºàÂê´Á¨¨ start ‰ΩçÔºâ Â¶ÇÊûúÁúÅÁï• deleteCountÔºåÂàôÂÖ∂Áõ∏ÂΩì‰∫é arr.length - start   item1, item2, ... ÂèØÈÄâÔºöË¶ÅÊ∑ªÂä†ËøõÊï∞ÁªÑÁöÑÂÖÉÁ¥†Ôºå‰ªé start ÁöÑ‰ΩçÁΩÆÂºÄÂßã„ÄÇÂ¶ÇÊûúÊú™ÊåáÂÆöÔºåÂàô splice() Â∞ÜÂè™Âà†Èô§Êï∞ÁªÑÂÖÉÁ¥†   ËøîÂõûÂÄºÔºöÁî±Ë¢´Âà†Èô§ÁöÑÂÖÉÁ¥†ÁªÑÊàêÁöÑ‰∏Ä‰∏™Êï∞ÁªÑ  Â¶ÇÊûúÂè™Âà†Èô§‰∫Ü‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÂàôËøîÂõûÂè™ÂåÖÂê´‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑÊï∞ÁªÑ Â¶ÇÊûúÊ≤°ÊúâÂà†Èô§ÂÖÉÁ¥†ÔºåÂàôËøîÂõûÁ©∫Êï∞ÁªÑ   ÊèèËø∞Ôºö  splice() ÊñπÊ≥ï‰ΩøÁî® deleteCount ÂèÇÊï∞Êù•ÊéßÂà∂ÊòØÂà†Èô§ËøòÊòØÊ∑ªÂä†ÂÖÉÁ¥† Â¶ÇÊûúÊ∑ªÂä†ËøõÊï∞ÁªÑÁöÑÂÖÉÁ¥†‰∏™Êï∞‰∏çÁ≠â‰∫éË¢´Âà†Èô§ÁöÑÂÖÉÁ¥†‰∏™Êï∞ÔºåÊï∞ÁªÑÁöÑÈïøÂ∫¶‰ºöÂèëÁîüÁõ∏Â∫îÁöÑÊîπÂèò    var arr = [1, 2, 3, 4, 5]; arr.splice(2);\t// returns [3, 4, 5] arr;\t// [1, 2]ÔºåÂéüÊï∞ÁªÑË¢´‰øÆÊîπ  arr = [1, 2, 3, 4, 5]; arr.splice(2, 2);\t// returns [3, 4] arr;\t// [1, 2, 5]  arr = [1, 2, 3, 4, 5]; arr.splice(1, 1, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;);\t// returns [2] arr;\t// [1, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, 3, 4, 5] arr.splice(1, 0, \u0026#39;a\u0026#39;);\t// returns [] arr;\t// [1, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, 3, 4, 5] 5.4 Êï∞ÁªÑÂ∞èÁªì Êï∞ÁªÑ ‰∏é ‰∏ÄËà¨ÂØπË±°\n   Áõ∏Âêå ‰∏çÂêå     ÈÉΩÂèØ‰ª•ÁªßÊâø Êï∞ÁªÑËá™Âä®Êõ¥Êñ∞ length   Êï∞ÁªÑÊòØÂØπË±°ÔºåÂØπË±°‰∏ç‰∏ÄÂÆöÊòØÊï∞ÁªÑ ÊåâÁ¥¢ÂºïËÆøÈóÆÊï∞ÁªÑÂ∏∏Â∏∏ÊØîËÆøÈóÆ‰∏ÄËà¨ÂØπË±°Â±ûÊÄßÊòéÊòæËøÖÈÄü   ÈÉΩÂèØ‰ª•ÂΩìÂÅöÂØπË±°Ê∑ªÂä†„ÄÅÂà†Èô§Â±ûÊÄß Êï∞ÁªÑÂØπË±°ÁªßÊâø Array.prototype ‰∏äÁöÑÂ§ßÈáèÊï∞ÁªÑÊìç‰ΩúÊñπÊ≥ï    ","permalink":"https://fang-lansheng.github.io/posts/2019-01-20-javascript-study-notes-3/","summary":"Êú¨ÁØáÂ≠¶‰π†Á¨îËÆ∞ÊòØÂú®ÊÖïËØæÁΩë‰∏äÂ≠¶‰π†JavaScriptÊ∑±ÂÖ•ÊµÖÂá∫Êó∂ÊâÄÂÅöÁ¨îËÆ∞ÔºåÂÜÖÂÆπ‰∏ªË¶ÅÊù•Ëá™ËØæÁ®ãËØæ‰ª∂ÂèäËÄÅÂ∏àËÆ≤Ëß£ÔºåÂêåÊó∂Á©øÊèíËá™Â∑±ÁöÑ‰∏Ä‰∫õÁêÜËß£ÂíåÂ∞ùËØï„ÄÇÈÉ®ÂàÜËµÑÊñôÊï¥ÁêÜËá™ MDN Web Docs„ÄÇ\n Êï∞ÁªÑÊòØÂÄºÁöÑÊúâÂ∫èÈõÜÂêà„ÄÇÊØè‰∏™ÂÄºÂè´ÂÅöÂÖÉÁ¥†ÔºåÊØè‰∏™ÂÖÉÁ¥†Âú®Êï∞ÁªÑ‰∏≠ÈÉΩÊúâÊï∞Â≠ó‰ΩçÁΩÆÁºñÂè∑Ôºå‰πüÂ∞±ÊòØÁ¥¢Âºï„ÄÇJavaScript ‰∏≠ÁöÑÊï∞ÁªÑÊòØÂº±Á±ªÂûãÁöÑÔºåÊï∞ÁªÑ‰∏≠ÂèØ‰ª•Âê´Êúâ‰∏çÂêåÁ±ªÂûãÁöÑÂÖÉÁ¥†„ÄÇÊï∞ÁªÑÂÖÉÁ¥†ÁîöËá≥ÂèØ‰ª•ÊòØÂØπË±°ÊàñÂÖ∂‰ªñÊï∞ÁªÑ„ÄÇ\n// ‰æãÂ¶Ç var arr = [1, true, null, undefined, {x: 1}, [1, 2, 3]]; arr[0];\t// 1 arr[3];\t// undefined arr[4].x;\t// 1 arr[5][1];\t// 2 5.1 ÂàõÂª∫Êï∞ÁªÑ \u0026amp; Êï∞ÁªÑÊìç‰Ωú ÂàõÂª∫Êï∞ÁªÑ - Â≠óÈù¢Èáè var BAT = [\u0026#39;Baidu\u0026#39;, \u0026#39;Alibaba\u0026#39;, \u0026#39;Tencent\u0026#39;]; var students = [{name: \u0026#39;Bosn\u0026#39;, age: 27}, {name: \u0026#39;Nunnly\u0026#39;, age: 3}]; var arr = [\u0026#39;Nunnly\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;big\u0026#39;, \u0026#39;keng\u0026#39;, \u0026#39;B\u0026#39;, 123, true, null]; var commasArr1 = [1, , 2];\t// 1, undefined, 2 var commasArr2 = [,,];\t// undefined * 2Ôºå‰∏çËµûÂêåËøôÁßçÂÆö‰πâÊñπÂºè Êï∞ÁªÑÁöÑÂ§ßÂ∞èÊòØÊúâÈôêÂà∂ÁöÑÔºösize from 0 to 4,294,967,295 ($2^{23} - 1$)","title":"JavaScript Â≠¶‰π†Á¨îËÆ∞Ôºà‰∏âÔºâÔºöÊï∞ÁªÑ„ÄÅÊï∞ÁªÑÊìç‰Ωú‰ª•ÂèäÊï∞ÁªÑÊñπÊ≥ï"},{"content":" Êú¨ÁØáÂ≠¶‰π†Á¨îËÆ∞ÊòØÂú®ÊÖïËØæÁΩë‰∏äÂ≠¶‰π†JavaScriptÊ∑±ÂÖ•ÊµÖÂá∫Êó∂ÊâÄÂÅöÁ¨îËÆ∞ÔºåÂÜÖÂÆπ‰∏ªË¶ÅÊù•Ëá™ËØæÁ®ãËØæ‰ª∂ÂèäËÄÅÂ∏àËÆ≤Ëß£ÔºåÂêåÊó∂Á©øÊèíËá™Â∑±ÁöÑ‰∏Ä‰∫õÁêÜËß£ÂíåÂ∞ùËØï„ÄÇÈÉ®ÂàÜËµÑÊñôÊï¥ÁêÜËá™ MDN Web Docs„ÄÇ\n 4.1 ÂØπË±°Ê¶ÇËø∞ ÂØπË±°‰∏≠ÂåÖÂê´‰∏ÄÁ≥ªÂàóÂ±ûÊÄßÔºåËøô‰∫õÂ±ûÊÄßÊòØÊó†Â∫èÁöÑ„ÄÇÊØè‰∏™Â±ûÊÄßÈÉΩÊúâ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ key ÂíåÂØπÂ∫îÁöÑ value„ÄÇ\nvar obj_0 = { x : 1, y : 2 }; obj_0.x;\t// 1 obj_0.y;\t// 2  var obj = {}; obj[1] = 1; obj[\u0026#39;1\u0026#39;] = 2; obj; // Object {1: 2}  obj[{}] = true; obj[{x: 1}] = true; obj; // Object {1: 2, [object Object]: true} ÂáΩÊï∞ function„ÄÅÊï∞ÁªÑ Array Á≠âÈÉΩÊòØÂØπË±°„ÄÇ\n4.2 ÂàõÂª∫ÂØπË±°„ÄÅÂéüÂûãÈìæ 1. Â≠óÈù¢Èáè\nvar obj1 = { x : 1, y : 2 }; var obj2 = { x: 1, y: 2, o: {\t// ÂµåÂ•ó  z: 3, n: 4 } }; 2. new / ÂéüÂûãÈìæ\nfunction foo() {}; foo.prototype.z = 3; var obj = new foo(); obj.x = 1; obj.y = 2; ÂéüÂûãÈìæÔºö\nobj.x;\t// 1 obj.y;\t// 2 obj.z;\t// 3 (ÈÄöËøáÂéüÂûãÈìæÂêë‰∏äÊü•ÊâæÔºåÊúâÂ±ûÊÄß\u0026#39;z\u0026#39;) typeof obj.toString;\t// \u0026#39;function\u0026#39; (toString Âú® Object.prototype ‰∏ä) \u0026#39;z\u0026#39; in obj;\t// true (z ÊòØ obj ‰ªé foo ‰∏≠ÁªßÊâøËøáÊù•ÁöÑ) obj.hasOwnProperty(\u0026#39;z\u0026#39;);\t// false (z Âπ∂‰∏çÂú®ÂØπË±°‰∏≠ÔºåËÄåÊòØÂú®ÂØπË±°ÁöÑÂéüÂûãÈìæ‰∏ä) Â¶ÇÊûúËµãÂÄºÔºö\nobj.z = 5; ÂéüÂûãÈìæÔºö\nobj.hasOwnProperty(\u0026#39;z\u0026#39;);\t// true foo.prototype.z;\t// (still) 3 obj.z;\t// 5  delete obj.z;\t// true (Âú®ÂØπË±° obj ‰∏≠Âà†Èô§Â±ûÊÄß z) obj.z;\t// 3  delete obj.z;\t// true (ÂÜçÊ¨°Âà†Èô§) obj.z;\t// still 3 (ÂéüÂûãÈìæ‰∏äÁöÑÂ±ûÊÄß‰∏ç‰ºöË¢´‰øÆÊîπÔºÅ) 3. Object.create\nObject.create() ÊòØ‰∏Ä‰∏™Á≥ªÁªüÂÜÖÁΩÆÁöÑÂáΩÊï∞ÔºåËøô‰∏™ÂáΩÊï∞‰ºöÊé•Âèó‰∏Ä‰∏™ÂèÇÊï∞Ôºà‰∏ÄËà¨ÊòØ‰∏Ä‰∏™ÂØπË±°ÔºâÔºåÂÆÉ‰ºöËøîÂõû‰∏Ä‰∏™Êñ∞ÂàõÂª∫ÁöÑÂØπË±°ÔºåÂπ∂ËÆ©Ëøô‰∏™ÂØπË±°ÁöÑÂéüÂûãÔºåÊåáÂêëËøô‰∏™ÂèÇÊï∞„ÄÇ\nvar obj = Object.create({ x : 1 }); obj.x;\t// 1 typeof obj.toString;\t// function obj.hasOwnProperty(\u0026#39;x\u0026#39;)\t// false var obj = Object.create(null); obj.toString;\t// undefined 4.3 Â±ûÊÄßÊìç‰Ωú 4.3.1 Â±ûÊÄßËØªÂÜô var obj = { x : 1, y : 2 }; // ËÆøÈóÆÂØπË±°Â±ûÊÄß obj.x;\t// 1 obj[\u0026#39;y\u0026#39;];\t// 2 // obj[\u0026#39;x\u0026#39;] = 3; obj.y = 4; Â±ûÊÄßËØªÂÜô - ÈÅçÂéÜÔºö\nvar obj = { x1: 1, x2 : 2, x3 : 3 }; var i = 1, n = 3; for (; i \u0026lt;= n; i++) { console.log(obj[\u0026#39;x\u0026#39; + i]);\t// ËæìÂá∫ 1, 2, 3 } var p; for (p in obj) {\t// Áî® for ‚Ä¶ in ÊñπÊ≥ïÂèØËÉΩ‰ºöÊääÂéüÂûãÈìæ‰∏äÁöÑ‰∏Ä‰∫õÂ±ûÊÄßÈÅçÂéÜÂá∫Êù•  console.log(p);\t// ËæìÂá∫ x1, x2, x3ÔºàÈ°∫Â∫è‰∏çÁ°ÆÂÆöÔºâ  console.log(obj[p]);\t// ËæìÂá∫ 1, 2, 3ÔºàÈ°∫Â∫è‰∏çÁ°ÆÂÆö) } Â±ûÊÄßËØªÂÜô - ÂºÇÂ∏∏Ôºö\nvar obj = { x : 1 }; obj.y;\t// undefined var yz = obj.y.z;\t// TypeError: Cannot read property \u0026#39;z\u0026#39; of undefined obj.y.z = 2;\t// TypeError: Cannot set property \u0026#39;z\u0026#39; of undefined  // ËØªÂèñÊó∂ÁöÑÂà§Êñ≠ var yz; if (obj.y) { yz = obj.y.z; } // or var yz = obj \u0026amp;\u0026amp; obj.y \u0026amp;\u0026amp; obj.y.z; 4.3.2 Â±ûÊÄßÂà†Èô§ var person = { age: 28, title: \u0026#39;fe\u0026#39; }; delete person.age;\t// true delete person[\u0026#39;title\u0026#39;];\t// true person.age;\t// undefined delete person.age;\t// true ÔºàÈáçÂ§çÂà†Èô§Ôºâ  delete Object.prototype;\t// false Ôºà‰∏çÂÖÅËÆ∏Âà†Èô§Ôºâ var descriptor = Object.getOwnPropertyDescriptor(Object, \u0026#39;prototype\u0026#39;);\t// Ëé∑ÂèñÂØπË±°ÁöÑ‰∏Ä‰∏™Â±ûÊÄß‰∏≠ÁöÑÊâÄÊúâÊ†áÁ≠æ descriptor.configurable;\t// false 4.3.3 Â±ûÊÄßÊ£ÄÊµã var cat = new Object; cat.legs = 4; cat.name = \u0026#39;Kitty\u0026#39;; \u0026#39;legs\u0026#39; in cat;\t// true \u0026#39;abc\u0026#39; in cat;\t// false \u0026#39;toString\u0026#39; in cat;\t// true (inherited property!)(in Êìç‰ΩúÁ¨¶‰ºöÂú®ÂéüÂûãÈìæ‰∏≠Âêë‰∏äÊü•Êâæ)  cat.hasOwnProperty(\u0026#39;legs\u0026#39;);\t// true cat.hasOwnProperty(\u0026#39;toString\u0026#39;);\t// false (ËØ•ÂØπË±°‰∏äÊ≤°ÊúâËØ•Â±ûÊÄßÔºåÂ∞ΩÁÆ°ÂéüÂûãÈìæ‰∏äÊúâ)  cat.propertyIsEnumerable(\u0026#39;legs\u0026#39;);\t// true (Âà§Êñ≠ËØ•Â±ûÊÄßÊòØÂê¶ÂèØÊûö‰∏æ) cat.propertyIsEnumerable(\u0026#39;toString\u0026#39;);\t// false  // Ëá™ÂÆö‰πâÂØπË±°Â±ûÊÄß Object.defineProperty(cat, \u0026#39;price\u0026#39;, {enumerable: false, value: 1000}); cat.propertyIsEnumerable(\u0026#39;price\u0026#39;);\t// false cat.hasOwnProperty(\u0026#39;price\u0026#39;);\t// true  if (cat \u0026amp;\u0026amp; cat.legs) { // cat Â≠òÂú®Ôºåcat.legs Â≠òÂú®  cat.legs *= 2; } if (cat.legs != undefined) { // !== undefined, or, !== null } if (cat.legs !== undefined) { // only if cat.legs is not undefined } 4.3.4 Â±ûÊÄßÊûö‰∏æ var o = { x: 1, y: 2, z: 3 }; \u0026#39;toString\u0026#39; in o;\t// true o.propertyIsEnumerable(\u0026#39;toString\u0026#39;);\t// falseÔºàÈªòËÆ§ falseÔºâ var key; for (key in o) {\t// Êûö‰∏æ o ‰∏≠Ôºàenumerable / ÂèØÊûö‰∏æÁöÑÔºâÂ±ûÊÄß  console.log(key);\t// x, y, z Ôºà‰∏ç‰ºöÂá∫Áé∞ toString Á≠âÂ±ûÊÄßÔºâ } var obj = Object.create(o); obj.a = 4; var key; for (key in obj) { console.log(key);\t// a, x, y, z } var obj = Object.create(o); obj.a = 4; var key; for (key in obj) { if (obj.hasOwnProperty(key)) {\t// ËøáÊª§ÊéâÂéüÂûãÈìæ‰∏äÁöÑÂ±ûÊÄß  console.log(key);\t// a  } } 4.4 get / set ÊñπÊ≥ï Âè¶‰∏ÄÁßçËØªÂÜôÂ±ûÊÄßÁöÑÊñπÂºè\nvar man = { name: \u0026#39;Bosn\u0026#39;,\t// ËØæÁ®ãËÄÅÂ∏àÁöÑÂêçÂ≠ó :)  weibo: \u0026#39;@Bosn\u0026#39;, get age() {\t// get ÊñπÊ≥ï  return new Date().getFullYear() - 1988; }, set age(val) {\t// setÊñπÊ≥ï  console.log(\u0026#39;Age can\\\u0026#39;t be set to \u0026#39; + val); } } console.log(man.age);\t// 31ÔºàËÆøÈóÆ man.age Êó∂ÔºåËá™Âä®Ë∞ÉÁî® get ÊñπÊ≥ïÔºâ man.age = 100;\t// Age cat\u0026#39;t be set to 100Ôºà‰∏∫ man.age ËµãÂÄºÊó∂ÔºåËá™Âä®Ë∞ÉÁî® set ÊñπÊ≥ïÔºâ console.log(man.age);\t// still 31ÔºàË∞ÉÁî® get ÊñπÊ≥ïÔºâ Êõ¥Â§çÊùÇÁöÑ‰æãÂ≠êÔºö\nvar man = { weibo: \u0026#39;@Bosn\u0026#39;, $age: null,\t// $age ‰∏é age ÊòØ‰∏çÂêåÁöÑÔºåËøôÈáåÁöÑ $ Á¨¶Âè∑Ë°®Á§∫‰∏çÂú®Â§ñÈÉ®Êö¥Èú≤Ëøô‰∏™Â±ûÊÄß*  get age() { if (this.$age == undefined) { return new Date().getFullYear() - 1988; } else { return this.$age; } }, set age(val) { val = +val;\t// ‰∏ÄÂÖÉ + Êìç‰ΩúÁ¨¶ÔºåÁî®‰ª•Â∞Ü val ËΩ¨Âåñ‰∏∫Êï∞Â≠ó  // ‰æãÂ¶ÇÔºö+123 =\u0026gt; 123, +\u0026#39;123\u0026#39; =\u0026gt; 123, +\u0026#39;abc\u0026#39; =\u0026gt; NaN  if (!isNaN(val) \u0026amp;\u0026amp; val \u0026gt; 0 \u0026amp;\u0026amp; val \u0026lt; 150) { this.$age = +val; } else { throw new Error(\u0026#39;Incorrect val = \u0026#39; + val); } } } console.log(man.age);\t// 31 man.age = 100; console.log(man.age);\t// 100 man.age = \u0026#39;abc\u0026#39;;\t// error: Incorrect val = NaN get / set ‰∏éÂéüÂûãÈìæ\nfunction foo() {} Object.defineProperty(foo.prototype, \u0026#39;z\u0026#39;, { get: function() { return 1; } }); var obj = new foo(); obj.z;\t// 1 obj.z;\t// 10 obj.z;\t// still 1 ËøôÊòØÂõ†‰∏∫ÂΩì obj ÂØπË±°‰∏äÊ≤°ÊúâËØ•Â±ûÊÄßÔºåÂπ∂‰∏îÂú®ÂéüÂûãÈìæ‰∏äÊü•ÊâæÊó∂ÂèëÁé∞ÊúâÂØπÂ∫îÁöÑ get Êàñ set ÊñπÊ≥ïÊó∂ÔºåÂ∞ùËØïÂØπËØ•Â±ûÊÄßËµãÂÄºÂ∞±‰ºöË∞ÉÁî®ÂéüÂûãÈìæ‰∏äÁöÑ get / set ÊñπÊ≥ïÔºåËÄå‰∏ç‰ºöÂÜçÂéªÁªôÂΩìÂâçÂØπË±°Ê∑ªÂä†Êñ∞Â±ûÊÄß„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÁªô obj ÂØπË±°Ê∑ªÂä†ËøôÊ†∑‰∏Ä‰∏™Â±ûÊÄßÂπ∂ÂÆûÁé∞‰øÆÊîπÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÊñπÊ≥ïÔºö\nObject.defineProperty(obj, \u0026#39;z\u0026#39;, { value: 100, configurable: true\t// ÈªòËÆ§‰∏∫ false }); obj.z;\t// 100 delete obj.z;\t// true obj.z;\t// back to 1 var o = {}; Object.defineProperty(o, \u0026#39;x\u0026#39;, { value: 1 });\t// writable = false, configurable = false; var obj = Object.create(o); obj.x;\t// 1 obj.x = 200; obj.x;\t// still 1, can\u0026#39;t change it  Object.defineProperty(obj, \u0026#39;x\u0026#39;, { writable: true, configurable: true, value: 100 }); obj.x;\t// 100 obj.x = 500; obj.x;\t// 500 4.5 Â±ûÊÄßÊ†áÁ≠æ  Â±ûÊÄßÊ†áÁ≠æÂç≥ÊåáÂ±ûÊÄßÊèèËø∞Á¨¶\n 4.5.1 Ëé∑ÂèñÂ±ûÊÄßÊ†áÁ≠æ Object.getOwnPropertyDescriptor(obj, prop)\n  ËØ•ÊñπÊ≥ïËøîÂõûÊåáÂÆöÂØπË±°‰∏ä‰∏Ä‰∏™Ëá™ÊúâÂ±ûÊÄßÂØπÂ∫îÁöÑÂ±ûÊÄßÊèèËø∞Á¨¶\nÔºàËá™ÊúâÂ±ûÊÄßÊåáÁöÑÊòØÁõ¥Êé•Ëµã‰∫àËØ•ÂØπË±°ÁöÑÂ±ûÊÄßÔºå‰∏çÈúÄË¶Å‰ªéÂéüÂûãÈìæ‰∏äËøõË°åÊü•ÊâæÁöÑÂ±ûÊÄßÔºâ\n  ÂèÇÊï∞Ôºö\n objÔºöÈúÄË¶ÅÊü•ÊâæÁöÑÁõÆÊ†áÂØπË±° propÔºöÁõÆÊ†áÂØπË±°ÂÜÖÂ±ûÊÄßÂêçÁß∞ ÔºàString Á±ªÂûãÔºâ    ËøîÂõûÂÄºÔºö\n Â¶ÇÊûúÊåáÂÆöÁöÑÂ±ûÊÄßÂ≠òÂú®‰∫éÂØπË±°‰∏äÔºåÂàôËøîÂõûÂÖ∂Â±ûÊÄßÊèèËø∞Á¨¶ÂØπË±°Ôºàproperty descriptorÔºâÔºåÂê¶ÂàôËøîÂõû undefined    ‰∏Ä‰∏™Â±ûÊÄßÊèèËø∞Á¨¶ÊòØ‰∏Ä‰∏™ËÆ∞ÂΩïÔºåÁî±‰∏ãÈù¢Â±ûÊÄßÂΩì‰∏≠ÁöÑÊüê‰∫õÁªÑÊàêÔºö\n valueÔºöËØ•Â±ûÊÄßÁöÑÂÄºÔºà‰ªÖÈíàÂØπÊï∞ÊçÆÂ±ûÊÄßÊèèËø∞ÊúâÊïàÔºâ writableÔºöÂΩì‰∏î‰ªÖÂΩìÂ±ûÊÄßÁöÑÂÄºÂèØ‰ª•ÊîπÂèòÊó∂‰∏∫ trueÔºà‰ªÖÈíàÂØπÊï∞ÊçÆÂ±ûÊÄßÊèèËø∞ÊúâÊïàÔºâ getÔºöËé∑ÂèñËØ•Â±ûÊÄßÁöÑËÆøÈóÆÂô®ÂáΩÊï∞ÔºàgetterÔºâÔºåÂ¶ÇÊûúÊ≤°ÊúâËÆøÈóÆÂô®ÔºåËØ•ÂÄº‰∏∫ undefinedÔºà‰ªÖÈíàÂØπÂåÖÂê´ËÆøÈóÆÂô®ÊàñËÆæÁΩÆÂô®ÁöÑÂ±ûÊÄßÊèèËø∞ÊúâÊïàÔºâ setÔºöËé∑ÂèñËØ•Â±ûÊÄßÁöÑËÆæÁΩÆÂô®ÂáΩÊï∞ÔºàsetterÔºâÔºåÂ¶ÇÊûúÊ≤°ÊúâËÆæÁΩÆÂô®ÔºåËØ•ÂÄº‰∏∫ undefinedÔºà‰ªÖÈíàÂØπÂåÖÂê´ËÆøÈóÆÂô®ÊàñËÆæÁΩÆÂô®ÁöÑÂ±ûÊÄßÊèèËø∞ÊúâÊïàÔºâ configurableÔºöÂΩì‰∏î‰ªÖÂΩìÊåáÂÆöÂØπË±°ÁöÑÂ±ûÊÄßÊèèËø∞ÂèØ‰ª•Ë¢´ÊîπÂèòÊàñËÄÖÂ±ûÊÄßÂèØË¢´Âà†Èô§Êó∂Ôºå‰∏∫ true enumerableÔºöÂΩì‰∏î‰ªÖÂΩìÊåáÂÆöÂØπË±°ÁöÑÂ±ûÊÄßÂèØ‰ª•Ë¢´Êûö‰∏æÂá∫Êó∂Ôºå‰∏∫ true    Á§∫‰æãÔºö\nObject.getOwnPropertyDescriptor({pro: true}, \u0026#39;pro\u0026#39;); // Object {value: true, writable: true, enumerable: true, configurable: true} Object.getOwnPropertyDescriptor({pro: true}, \u0026#39;pro\u0026#39;); // undefined 4.5.2 ËÆæÁΩÆÂ±ûÊÄßÊ†áÁ≠æ Object.defineProperty(obj, prop, descriptor)\n ËØ•ÊñπÊ≥ï‰ºöÁõ¥Êé•Âú®‰∏Ä‰∏™ÂØπË±°‰∏äÂÆö‰πâ‰∏Ä‰∏™Êñ∞Â±ûÊÄßÔºåÊàñËÄÖ‰øÆÊîπ‰∏Ä‰∏™ÂØπË±°ÁöÑÁé∞ÊúâÂ±ûÊÄßÔºå Âπ∂ËøîÂõûËøô‰∏™ÂØπË±°„ÄÇ ÂèÇÊï∞Ôºö  objÔºöË¶ÅÂú®ÂÖ∂‰∏äÂÆö‰πâÂ±ûÊÄßÁöÑÂØπË±° propÔºöË¶ÅÂÆö‰πâÊàñ‰øÆÊîπÁöÑÂ±ûÊÄßÁöÑÂêçÊ¨°ÔºàString Á±ªÂûãÔºâ descriptorÔºöÂ∞ÜË¢´ÂÆö‰πâÊàñ‰øÆÊîπÁöÑÂ±ûÊÄßÊèèËø∞Á¨¶   ËøîÂõûÂÄºÔºöË¢´‰º†ÈÄíÁªôÂáΩÊï∞ÁöÑÂØπË±° ËØ•ÊñπÊ≥ïÂÖÅËÆ∏Á≤æÁ°ÆÊ∑ªÂä†Êàñ‰øÆÊîπÂØπË±°ÁöÑÂ±ûÊÄß„ÄÇÈÄöËøáËµãÂÄºÊìç‰ΩúÊ∑ªÂä†ÁöÑÊôÆÈÄöÂ±ûÊÄßÊòØÂèØÊûö‰∏æÁöÑÔºåËÉΩÂ§üÂú®Â±ûÊÄßÊûö‰∏æÊúüÈó¥ÂëàÁé∞Âá∫Êù•Ôºàfor...in Êàñ Object.keys ÊñπÊ≥ïÔºâÔºå Ëøô‰∫õÂ±ûÊÄßÁöÑÂÄºÂèØ‰ª•Ë¢´ÊîπÂèòÔºå‰πüÂèØ‰ª•Ë¢´Âà†Èô§„ÄÇËøô‰∏™ÊñπÊ≥ïÂÖÅËÆ∏‰øÆÊîπÈªòËÆ§ÁöÑÈ¢ùÂ§ñÈÄâÈ°πÔºàÊàñÈÖçÁΩÆÔºâ„ÄÇ**Ê≥®ÊÑèÔºö**ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî® Object.defineProperty() Ê∑ªÂä†ÁöÑÂ±ûÊÄßÂÄºÊòØ‰∏çÂèØ‰øÆÊîπÁöÑ„ÄÇ  Á§∫‰æã 1Ôºö\nvar person = {}; Object.defineProperty(person, \u0026#39;name\u0026#39;, { configurable: false, writable: false, enumerable: true, value: \u0026#39;Bosn Ma\u0026#39; }); person.name;\t// Bosn Ma person.name = 1; person.name;\t// still Bosn Ma delete person.name;\t//false  Object.defineProperty(person, \u0026#39;type\u0026#39;, { configurable: true, writable: true, enumerable: false, value: \u0026#39;Object\u0026#39; }); Object.keys(person);\t// [\u0026#34;name\u0026#34;] (Âõ†‰∏∫Ôºåname ÂèØÊûö‰∏æÔºåtype ‰∏çÂèØÊûö‰∏æ) Á§∫‰æã 2Ôºö\nObject.defineProperties(person, {\t// Á±ª‰ºº‰∫é Object.defineProperty()ÔºåÂèØ‰ª•ÂêåÊó∂ÂÆö‰πâÂ§ö‰∏™Â±ûÊÄß  title: {value: \u0026#39;fe\u0026#39;, enumerable: true},\t// Êú™Â£∞ÊòéÁöÑÂ±ûÊÄßÊ†áÁ≠æÈªòËÆ§ÈÉΩ‰∏∫ false  corp: {value: \u0026#39;BABA\u0026#39;, enumerable: true}, salary: {value: 50000, enumerable: true, writable: true} }); Object.getOwnPropertyDescriptor(person, \u0026#39;salary\u0026#39;); // Object {value: 50000, writable: true, enumerable: true, configurable: false} Object.getOwnPropertyDescriptor(person, \u0026#39;corp\u0026#39;); // Object {value: \u0026#34;BABA\u0026#34;, writable: false, enumerable: true, configurable: false} Á§∫‰æã 3Ôºö\nObject.defineProperties(person, {\t// Êõ¥Âä†Â§çÊùÇÂåñ  title: {value: \u0026#39;fe\u0026#39;, enumerable: true},\tcorp: {value: \u0026#39;BABA\u0026#39;, enumerable: true}, salary: {value: 50000, enumerable: true, writable: true}, luck: { get: function() { return Math.random() \u0026gt; 0.5 ? \u0026#39;good\u0026#39; : \u0026#39;bad\u0026#39;; } }, promote: { set: function(level) { this.salary *= 1 + level * 0.1; } } }); Object.getOwnPropertyDescriptor(person, \u0026#39;salary\u0026#39;); // Object {value: 50000, writable: true, enumerable: true, configurable: false} Object.getOwnPropertyDescriptor(person, \u0026#39;corp\u0026#39;); // Object {value: \u0026#34;BABA\u0026#34;, writable: false, enumerable: true, configurable: false} person.salary;\t// 50000 person.promote = 2; person.salary;\t// 60000 4.5.3 configurable ‰∏é writalbe ÁöÑÈôêÂà∂ 4.6 ÂØπË±°Ê†áÁ≠æ„ÄÅÂØπË±°Â∫èÂàóÂåñ 4.6.1 ÂØπË±°Ê†áÁ≠æ 1. ÂéüÂûãÊ†áÁ≠æ __proto__\n Object.prototype ÁöÑ __proto__ Â±ûÊÄßÊòØ‰∏Ä‰∏™ËÆøÈóÆÂô®Â±ûÊÄßÔºà‰∏Ä‰∏™ getter ÂáΩÊï∞Âíå‰∏Ä‰∏™ setter ÂáΩÊï∞ÔºâÔºå Êö¥Èú≤‰∫ÜÈÄöËøáÂÆÉËÆøÈóÆÁöÑÂØπË±°ÁöÑÂÜÖÈÉ® [[Prototype]] (‰∏Ä‰∏™ÂØπË±°Êàñ null)Ôºå‰ªª‰Ωï‰∏Ä‰∏™ __proto__ ÁöÑÂ≠òÂèñÂ±ûÊÄßÈÉΩÁªßÊâø‰∫éObject.prototype„ÄÇ ‰ΩøÁî® __proto__ ÊòØÊúâ‰∫âËÆÆÁöÑÔºå‰πü‰∏çÈºìÂä±‰ΩøÁî®ÂÆÉ„ÄÇÂõ†‰∏∫ÂÆÉ‰ªéÊù•Ê≤°ÊúâË¢´ÂåÖÊã¨Âú® EcmaScript ËØ≠Ë®ÄËßÑËåÉ‰∏≠Ôºå‰ΩÜÊòØÁé∞‰ª£ÊµèËßàÂô®ÈÉΩÂÆûÁé∞‰∫ÜÂÆÉ„ÄÇ  2. class Ê†áÁ≠æ\n Ë°®Á§∫ÂØπË±°ÁöÑÁ±ªÂûã Ê≤°ÊúâÁõ¥Êé•Êü•ÁúãÊàñ‰øÆÊîπÁöÑÊñπÂºèÔºåÂèØ‰ª•Èó¥Êé•ÈÄöËøá Object.prototype.toString ÁöÑÊñπÂºèÂéªËé∑Âèñ  var toString = Object.prototype.toString;\t// Áî®‰ª•Ëé∑ÂèñÊØè‰∏™ÂØπË±°ÁöÑÁ±ªÂûã function getType(o) {return toString.call(o).slice(8, -1);} toString.call(null);\t// \u0026#34;[object Null]\u0026#34; getType(null);\t// \u0026#34;Null\u0026#34; getType(undefined);\t// \u0026#34;Number\u0026#34; getType(new Number(1));\t// \u0026#34;Number\u0026#34; typeof new Number(1);\t// \u0026#34;object\u0026#34; getType(true);\t// \u0026#34;Boolean\u0026#34; getType(new Boolean(true));\t// \u0026#34;Boolean\u0026#34; typeof new Number(true);\t// \u0026#34;object\u0026#34; 3. extensible Ê†áÁ≠æ\nObject.isExtensible() ÊñπÊ≥ïÂà§Êñ≠‰∏Ä‰∏™ÂØπË±°ÊòØÂê¶ÊòØÂèØÊâ©Â±ïÁöÑÔºàÊòØÂê¶ÂèØ‰ª•Âú®ÂÆÉ‰∏äÈù¢Ê∑ªÂä†Êñ∞ÁöÑÂ±ûÊÄßÔºâ\n ÂèÇÊï∞Ôºö  objÔºöÈúÄË¶ÅÊ£ÄÊµãÁöÑÈòüÂΩ¢   ËøîÂõûÂÄºÔºöË°®Á§∫ÁªôÂÆöÂØπË±°ÊòØÂê¶ÂèØÊâ©Â±ïÁöÑ‰∏Ä‰∏™ Boolean ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÂØπË±°ÊòØÂèØÊâ©Â±ïÁöÑÔºöÂç≥ÂèØ‰ª•‰∏∫‰ªñ‰ª¨Ê∑ªÂä†Êñ∞ÁöÑÂ±ûÊÄß„ÄÇ‰ª•ÂèäÂÆÉ‰ª¨ÁöÑ __proto__ Â±ûÊÄßÂèØ‰ª•Ë¢´Êõ¥Êîπ„ÄÇObject.preventExtensionsÔºåObject.seal Êàñ Object.freeze ÊñπÊ≥ïÈÉΩÂèØ‰ª•Ê†áËÆ∞‰∏Ä‰∏™ÂØπË±°‰∏∫‰∏çÂèØÊâ©Â±ïÔºànon-extensibleÔºâ„ÄÇ  Object.seal()ÊñπÊ≥ïÂ∞ÅÈó≠‰∏Ä‰∏™ÂØπË±°ÔºåÈòªÊ≠¢Ê∑ªÂä†Êñ∞Â±ûÊÄßÂπ∂Â∞ÜÊâÄÊúâÁé∞ÊúâÂ±ûÊÄßÊ†áËÆ∞‰∏∫‰∏çÂèØÈÖçÁΩÆ„ÄÇÂΩìÂâçÂ±ûÊÄßÁöÑÂÄºÂè™Ë¶ÅÂèØÂÜôÂ∞±ÂèØ‰ª•ÊîπÂèò„ÄÇ\n  ÂèÇÊï∞Ôºö\n objÔºöÂ∞ÜË¶ÅË¢´ÂØÜÂ∞ÅÁöÑÂØπË±°    ËøîÂõûÂÄºÔºöË¢´ÂØÜÂ∞ÅÁöÑÂØπË±°\n  Â∞ùËØïÂà†Èô§‰∏Ä‰∏™ÂØÜÂ∞ÅÂØπË±°ÁöÑÂ±ûÊÄßÊàñËÄÖÂ∞ÜÊüê‰∏™ÂØÜÂ∞ÅÂØπË±°ÁöÑÂ±ûÊÄß‰ªéÊï∞ÊçÆÂ±ûÊÄßËΩ¨Êç¢ÊàêËÆøÈóÆÂô®Â±ûÊÄßÔºåÁªìÊûú‰ºöÈùôÈªòÂ§±Ë¥•ÊàñÊäõÂá∫TypeErrorÔºàÂú® ‰∏•Ê†ºÊ®°Âºè ‰∏≠ÊúÄÂ∏∏ËßÅÁöÑÔºå‰ΩÜ‰∏çÂîØ‰∏ÄÔºâ„ÄÇ\n‰∏ç‰ºöÂΩ±Âìç‰ªéÂéüÂûãÈìæ‰∏äÁªßÊâøÁöÑÂ±ûÊÄß„ÄÇ‰ΩÜ __proto__ Â±ûÊÄßÁöÑÂÄº‰πü‰ºö‰∏çËÉΩ‰øÆÊîπ„ÄÇ\n  Object.freeze() ÊñπÊ≥ïÂèØ‰ª•ÂÜªÁªì‰∏Ä‰∏™ÂØπË±°ÔºåÂÜªÁªìÊåáÁöÑÊòØ‰∏çËÉΩÂêëËøô‰∏™ÂØπË±°Ê∑ªÂä†Êñ∞ÁöÑÂ±ûÊÄßÔºå‰∏çËÉΩ‰øÆÊîπÂÖ∂Â∑≤ÊúâÂ±ûÊÄßÁöÑÂÄºÔºå‰∏çËÉΩÂà†Èô§Â∑≤ÊúâÂ±ûÊÄßÔºå‰ª•Âèä‰∏çËÉΩ‰øÆÊîπËØ•ÂØπË±°Â∑≤ÊúâÂ±ûÊÄßÁöÑÂèØÊûö‰∏æÊÄß„ÄÅÂèØÈÖçÁΩÆÊÄß„ÄÅÂèØÂÜôÊÄß„ÄÇ\n  ÂèÇÊï∞Ôºö\n objÔºöË¶ÅË¢´ÂÜªÁªìÁöÑÂØπË±°    ËøîÂõûÂÄºÔºöË¢´ÂÜªÁªìÁöÑÂØπË±°ÔºàËÄå‰∏çÊòØÂàõÂª∫‰∏Ä‰∏™Ë¢´ÂÜªÁªìÁöÑÂâØÊú¨Ôºâ\n  Ë¢´ÂÜªÁªìÂØπË±°Ëá™Ë∫´ÁöÑÊâÄÊúâÂ±ûÊÄßÈÉΩ‰∏çÂèØËÉΩ‰ª•‰ªª‰ΩïÊñπÂºèË¢´‰øÆÊîπ„ÄÇ‰ªª‰Ωï‰øÆÊîπÂ∞ùËØïÈÉΩ‰ºöÂ§±Ë¥•ÔºåÊó†ËÆ∫ÊòØÈùôÈªòÂú∞ËøòÊòØÈÄöËøáÊäõÂá∫TypeErrorÂºÇÂ∏∏ÔºàÊúÄÂ∏∏ËßÅ‰ΩÜ‰∏ç‰ªÖÈôê‰∫é ‰∏•Ê†ºÊ®°ÂºèÔºâ„ÄÇ\nÊï∞ÊçÆÂ±ûÊÄßÁöÑÂÄº‰∏çÂèØÊõ¥ÊîπÔºåËÆøÈóÆÂô®Â±ûÊÄßÔºàÊúâgetterÂíåsetterÔºâ‰πüÂêåÊ†∑Ôºà‰ΩÜÁî±‰∫éÊòØÂáΩÊï∞Ë∞ÉÁî®ÔºåÁªô‰∫∫ÁöÑÈîôËßâÊòØËøòÊòØÂèØ‰ª•‰øÆÊîπËøô‰∏™Â±ûÊÄßÔºâ„ÄÇÂ¶ÇÊûú‰∏Ä‰∏™Â±ûÊÄßÁöÑÂÄºÊòØ‰∏™ÂØπË±°ÔºåÂàôËøô‰∏™ÂØπË±°‰∏≠ÁöÑÂ±ûÊÄßÊòØÂèØ‰ª•‰øÆÊîπÁöÑÔºåÈô§ÈùûÂÆÉ‰πüÊòØ‰∏™ÂÜªÁªìÂØπË±°„ÄÇÊï∞ÁªÑ‰Ωú‰∏∫‰∏ÄÁßçÂØπË±°ÔºåË¢´ÂÜªÁªìÔºåÂÖ∂ÂÖÉÁ¥†‰∏çËÉΩË¢´‰øÆÊîπ„ÄÇÊ≤°ÊúâÊï∞ÁªÑÂÖÉÁ¥†ÂèØ‰ª•Ë¢´Ê∑ªÂä†ÊàñÁßªÈô§„ÄÇ\n  var obj = {x: 1, y: 2}; Object.isExtensible(obj);\t// true Object.preventExtensions(obj); Object.isExtensible(obj);\t// false obj.z = 1; obj.z;\t// undefined (add new property failed) Object.getOwnPropertyDescriptor(obj, \u0026#39;x\u0026#39;); // Object {value: 1, writable: true, enumerable: true, configurable: true} // ÂéüÊúâÂ±ûÊÄß‰ªçÁÑ∂ÊòØÂèØ‰ª•‰øÆÊîπ„ÄÅÂà†Èô§ÁöÑ  Object.seal(obj); Object.getOwnPropertyDescriptor(obj, \u0026#39;x\u0026#39;); // Object {value: 1, writable: true, enumerable: true, configurable: false} Object.isSealed(obj);\t// true  Object.freeze(obj); Object.getOwnPropertyDescriptor(obj, \u0026#39;x\u0026#39;); // Object {value: 1, writable: false, enumerable: true, configurable: false} Object.isFrozen(obj);\t// true  // [CAUTION] Do not affects prototype chain! 4.6.2 Â∫èÂàóÂåñ„ÄÅÂÖ∂‰ªñÂØπË±°ÊñπÊ≥ï 1. Â∫èÂàóÂåñ\nJSON.stringify() ÊñπÊ≥ïÊòØÂ∞Ü‰∏Ä‰∏™ JavaScript ÂÄºÔºàÂØπË±°ÊàñËÄÖÊï∞ÁªÑÔºâËΩ¨Êç¢‰∏∫‰∏Ä‰∏™ JSON Â≠óÁ¨¶‰∏≤ÔºåÂ¶ÇÊûúÊåáÂÆö‰∫Ü replacer ÊòØ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂàôÂèØ‰ª•ÊõøÊç¢ÂÄºÔºåÊàñËÄÖÂ¶ÇÊûúÊåáÂÆö‰∫Ü replacer ÊòØ‰∏Ä‰∏™Êï∞ÁªÑÔºåÂèØÈÄâÁöÑ‰ªÖÂåÖÊã¨ÊåáÂÆöÁöÑÂ±ûÊÄß„ÄÇ\nJSON.stringify(value[, replacer[, space]])  ÂèÇÊï∞Ôºö  valueÔºöÂ∞ÜË¶ÅÂ∫èÂàóÂåñÊàê‰∏Ä‰∏™ JSON Â≠óÁ¨¶‰∏≤ÁöÑÂÄº replacer ÂèØÈÄâÔºöÂ¶ÇÊûúËØ•ÂèÇÊï∞ÊòØ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂàôÂú®Â∫èÂàóÂåñËøáÁ®ã‰∏≠ÔºåË¢´Â∫èÂàóÂåñÁöÑÂÄºÁöÑÊØè‰∏™Â±ûÊÄßÈÉΩ‰ºöÁªèËøáËØ•ÂáΩÊï∞ÁöÑËΩ¨Êç¢ÂíåÂ§ÑÁêÜÔºõÂ¶ÇÊûúËØ•ÂèÇÊï∞ÊòØ‰∏Ä‰∏™Êï∞ÁªÑÔºåÂàôÂè™ÊúâÂåÖÂê´Âú®Ëøô‰∏™Êï∞ÁªÑ‰∏≠ÁöÑÂ±ûÊÄßÂêçÊâç‰ºöË¢´Â∫èÂàóÂåñÂà∞ÊúÄÁªàÁöÑ JSON Â≠óÁ¨¶‰∏≤‰∏≠ÔºõÂ¶ÇÊûúËØ•ÂèÇÊï∞‰∏∫nullÊàñËÄÖÊú™Êèê‰æõÔºåÂàôÂØπË±°ÊâÄÊúâÁöÑÂ±ûÊÄßÈÉΩ‰ºöË¢´Â∫èÂàóÂåñÔºõÂÖ≥‰∫éËØ•ÂèÇÊï∞Êõ¥ËØ¶ÁªÜÁöÑËß£ÈáäÂíåÁ§∫‰æãÔºåËØ∑ÂèÇËÄÉ‰ΩøÁî®ÂéüÁîüÁöÑ JSON ÂØπË±°‰∏ÄÊñá„ÄÇ space ÂèØÈÄâÔºöÊåáÂÆöÁº©ËøõÁî®ÁöÑÁ©∫ÁôΩÂ≠óÁ¨¶‰∏≤ÔºåÁî®‰∫éÁæéÂåñËæìÂá∫Ôºàpretty-printÔºâÔºõÂ¶ÇÊûúÂèÇÊï∞ÊòØ‰∏Ä‰∏™Êï∞Â≠óÔºåÂÆÉ‰ª£Ë°®ÊúâÂ§öÂ∞ëÁöÑÁ©∫Ê†ºÔºõ‰∏äÈôê‰∏∫ 10„ÄÇËØ•ÂÄºËã•Â∞è‰∫é 1ÔºåÂàôÊÑèÂë≥ÁùÄÊ≤°ÊúâÁ©∫Ê†ºÔºõÂ¶ÇÊûúËØ•ÂèÇÊï∞‰∏∫Â≠óÁ¨¶‰∏≤ÔºàÂ≠óÁ¨¶‰∏≤ÁöÑÂâçÂçÅ‰∏™Â≠óÊØçÔºâÔºåËØ•Â≠óÁ¨¶‰∏≤Â∞ÜË¢´‰Ωú‰∏∫Á©∫Ê†ºÔºõÂ¶ÇÊûúËØ•ÂèÇÊï∞Ê≤°ÊúâÊèê‰æõÔºàÊàñËÄÖ‰∏∫ nullÔºâÂ∞ÜÊ≤°ÊúâÁ©∫Ê†º„ÄÇ   ËøîÂõûÂÄºÔºö‰∏Ä‰∏™Ë°®Á§∫ÁªôÂÆöÂÄºÁöÑ JSON Â≠óÁ¨¶‰∏≤ ÂÖ≥‰∫éÂ∫èÂàóÂåñÔºå‰∏ãÈù¢Êúâ‰∫îÁÇπÊ≥®ÊÑè‰∫ãÈ°πÔºö  ÈùûÊï∞ÁªÑÂØπË±°ÁöÑÂ±ûÊÄß‰∏çËÉΩ‰øùËØÅ‰ª•ÁâπÂÆöÁöÑÈ°∫Â∫èÂá∫Áé∞Âú®Â∫èÂàóÂåñÂêéÁöÑÂ≠óÁ¨¶‰∏≤‰∏≠ Â∏ÉÂ∞îÂÄº„ÄÅÊï∞Â≠ó„ÄÅÂ≠óÁ¨¶‰∏≤ÁöÑÂåÖË£ÖÂØπË±°Âú®Â∫èÂàóÂåñËøáÁ®ã‰∏≠‰ºöËá™Âä®ËΩ¨Êç¢‰∏∫ÂØπÂ∫îÁöÑÂéüÂßãÂÄº undefined„ÄÅ‰ªªÊÑèÁöÑÂáΩÊï∞‰ª•Âèä symbol ÂÄºÔºåÂú®Â∫èÂàóÂåñ‰∏≠‰ºöË¢´ÂøΩÁï•ÔºàÂá∫Áé∞Âú®ÈùûÊï∞ÁªÑÂØπË±°ÁöÑÂ±ûÊÄßÂÄº‰∏≠ÔºâÊàñËÄÖË¢´ËΩ¨Êç¢Êàê null ÔºàÂá∫Áé∞Âú®Êï∞ÁªÑ‰∏≠Êó∂Ôºâ ÊâÄÊúâ‰ª• symbol ÂÄº‰∏∫Â±ûÊÄßÈîÆÁöÑÂ±ûÊÄßÈÉΩ‰ºöË¢´ÂÆåÂÖ®ÂøΩÁï•ÊéâÔºåÂç≥‰Ωø replacer ÂèÇÊï∞‰∏≠Âº∫Âà∂ÊåáÂÆöÂåÖÂê´‰∫ÜÂÆÉ‰ª¨ ‰∏çÂèØÊûö‰∏æÁöÑÂ±ûÊÄß‰ºöË¢´ÂøΩÁï•    JSON.parse() ÊñπÊ≥ïÁî®Êù•Ëß£ÊûêJSONÂ≠óÁ¨¶‰∏≤ÔºåÊûÑÈÄ†Áî±Â≠óÁ¨¶‰∏≤ÊèèËø∞ÁöÑJavaScriptÂÄºÊàñÂØπË±°„ÄÇÊèê‰æõÂèØÈÄâÁöÑ reviver ÂáΩÊï∞Áî®‰ª•Âú®ËøîÂõû‰πãÂâçÂØπÊâÄÂæóÂà∞ÁöÑÂØπË±°ÊâßË°åÂèòÊç¢ÔºàÊìç‰ΩúÔºâ„ÄÇ\nJSON.parse(text[, reviver])  ÂèÇÊï∞Ôºö  textÔºöÂ∞ÜË¶ÅË¢´Ëß£ÊûêÊàê JavaScript ÂÄºÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÂÖ≥‰∫é JSON ÁöÑËØ≠Ê≥ïÊ†ºÂºèÔºåËØ∑ÂèÇËÄÉÔºöJSON„ÄÇ reviver ÂèØÈÄâÔºöËΩ¨Êç¢Âô®ÔºåÂ¶ÇÊûú‰º†ÂÖ•ËØ•ÂèÇÊï∞ÔºàÂáΩÊï∞ÔºâÔºåÂèØ‰ª•Áî®Êù•‰øÆÊîπËß£ÊûêÁîüÊàêÁöÑÂéüÂßãÂÄºÔºåË∞ÉÁî®Êó∂Êú∫Âú® parser ÂáΩÊï∞ËøîÂõû‰πãÂâç   ËøîÂõûÂÄºÔºöObject Á±ªÂûãÔºåÂØπÂ∫îÁªôÂÆöÁöÑ JSON ÊñáÊú¨ÁöÑÂØπË±° / ÂÄº ÂºÇÂ∏∏ÔºöËã•‰º†ÂÖ•ÁöÑÂ≠óÁ¨¶‰∏≤‰∏çÁ¨¶Âêà JSON ËßÑËåÉÔºåÂàô‰ºöÊäõÂá∫ SyntaxError ÂºÇÂ∏∏  var obj = {x: 1, y: true, z: [1, 2, 3], nullVal: null}; JSON.stringify(obj);\t// \u0026#34;{\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: true, \u0026#34;z\u0026#34;: [1, 2, 3], \u0026#34;nullVal\u0026#34;: null}\u0026#34;  obj = {val: undefined, a: NaN, b: Infinity, c: new Date()}; JSON.stringify(obj);\t// \u0026#34;{\u0026#34;a\u0026#34;: null, \u0026#34;b\u0026#34;: null, \u0026#34;c\u0026#34;: \u0026#34;2019-01-09T14:15:43.910Z\u0026#34;}\u0026#34;  obj = JSON.parse(\u0026#39;{\u0026#34;x\u0026#34;: 1}\u0026#39;); obj.x;\t// 1 2. Â∫èÂàóÂåñ - Ëá™ÂÆö‰πâ\nvar obj = { x: 1, y: 2, o: { o1: 1, o2: 2, toJSON: function() { return this.o1 + this.o2; } } }; JSON.stringify(obj);\t// \u0026#34;{\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 2, \u0026#34;o\u0026#34;: 3}\u0026#34; 3. ÂÖ∂‰ªñÂØπË±°ÊñπÊ≥ï\nvar obj = {x: 1, y: 2}; obj.toString();\t// \u0026#34;[object Object]\u0026#34; obj.toString = function() {return this.x + this.y;};\t// Ëá™ÂÆö‰πâ toString  \u0026#39;Result \u0026#39; + obj;\t// \u0026#34;Result 3\u0026#34;, by toString +obj;\t// 3, from toString  obj.valueOf = function() {return this.x + this.y + 100;};\t// Ëá™ÂÆö‰πâ valueOf +obj;\t// 103, from valueOf \u0026#39;Result \u0026#39; + obj;\t// still \u0026#34;Result 103\u0026#34; Êó†ËÆ∫‰∏ÄÂÖÉÊìç‰ΩúÁ¨¶ + ËøòÊòØ‰Ωú‰∏∫Â≠óÁ¨¶‰∏≤ÊãºÊé•ÁöÑ‰∫åÂÖÉÊìç‰ΩúÁ¨¶ +ÔºåÂú®ÂÖ∑‰ΩìÊìç‰ΩúÊó∂ÈÉΩ‰ºöÂ∞ùËØïÊääÂØπË±°ËΩ¨Êç¢‰∏∫Âü∫Êú¨Á±ªÂûã„ÄÇÂÆÉ‰ºöÂÖàÂéªÊâæ valueOf ÔºåÂ¶ÇÊûúÂÖ∂ËøîÂõûÂü∫Êú¨Á±ªÂûãÔºåÂàô‰ºö‰ª•ËØ•ÂÄº‰Ωú‰∏∫ÁªìÊûúÁπÅÊÆñÔºåÂΩì valueOf ‰∏çÂ≠òÂú®ÊàñËøîÂõûÂØπË±°Êó∂ÔºåÂ∞±‰ºöÂéªÊâæ toString„ÄÇËã•‰∫åËÄÖÈÉΩÊ≤°Â≠òÂú®ÊàñÈÉΩËøîÂõûÂØπË±°ÔºåÂàô‰ºöÊä•Èîô„ÄÇ\n","permalink":"https://fang-lansheng.github.io/posts/2019-01-10-javascript-study-notes-2/","summary":"Êú¨ÁØáÂ≠¶‰π†Á¨îËÆ∞ÊòØÂú®ÊÖïËØæÁΩë‰∏äÂ≠¶‰π†JavaScriptÊ∑±ÂÖ•ÊµÖÂá∫Êó∂ÊâÄÂÅöÁ¨îËÆ∞ÔºåÂÜÖÂÆπ‰∏ªË¶ÅÊù•Ëá™ËØæÁ®ãËØæ‰ª∂ÂèäËÄÅÂ∏àËÆ≤Ëß£ÔºåÂêåÊó∂Á©øÊèíËá™Â∑±ÁöÑ‰∏Ä‰∫õÁêÜËß£ÂíåÂ∞ùËØï„ÄÇÈÉ®ÂàÜËµÑÊñôÊï¥ÁêÜËá™ MDN Web Docs„ÄÇ\n 4.1 ÂØπË±°Ê¶ÇËø∞ ÂØπË±°‰∏≠ÂåÖÂê´‰∏ÄÁ≥ªÂàóÂ±ûÊÄßÔºåËøô‰∫õÂ±ûÊÄßÊòØÊó†Â∫èÁöÑ„ÄÇÊØè‰∏™Â±ûÊÄßÈÉΩÊúâ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ key ÂíåÂØπÂ∫îÁöÑ value„ÄÇ\nvar obj_0 = { x : 1, y : 2 }; obj_0.x;\t// 1 obj_0.y;\t// 2  var obj = {}; obj[1] = 1; obj[\u0026#39;1\u0026#39;] = 2; obj; // Object {1: 2}  obj[{}] = true; obj[{x: 1}] = true; obj; // Object {1: 2, [object Object]: true} ÂáΩÊï∞ function„ÄÅÊï∞ÁªÑ Array Á≠âÈÉΩÊòØÂØπË±°„ÄÇ\n4.2 ÂàõÂª∫ÂØπË±°„ÄÅÂéüÂûãÈìæ 1. Â≠óÈù¢Èáè\nvar obj1 = { x : 1, y : 2 }; var obj2 = { x: 1, y: 2, o: {\t// ÂµåÂ•ó  z: 3, n: 4 } }; 2.","title":"JavaScript Â≠¶‰π†Á¨îËÆ∞Ôºà‰∫åÔºâÔºöÂØπË±°„ÄÅÂØπË±°ÁöÑÂ±ûÊÄß‰ª•ÂèäÂØπË±°ÁöÑÊñπÊ≥ï"},{"content":" Êú¨ÁØáÂ≠¶‰π†Á¨îËÆ∞ÊòØÂú®ÊÖïËØæÁΩë‰∏äÂ≠¶‰π†JavaScriptÊ∑±ÂÖ•ÊµÖÂá∫Êó∂ÊâÄÂÅöÁ¨îËÆ∞ÔºåÂÜÖÂÆπ‰∏ªË¶ÅÊù•Ëá™ËØæÁ®ãËØæ‰ª∂ÂèäËÄÅÂ∏àËÆ≤Ëß£ÔºåÂêåÊó∂Á©øÊèíËá™Â∑±ÁöÑ‰∏Ä‰∫õÁêÜËß£ÂíåÂ∞ùËØï„ÄÇÈÉ®ÂàÜËµÑÊñôÊï¥ÁêÜËá™ MDN Web Docs„ÄÇ\n ‰∏Ä„ÄÅÊï∞ÊçÆÁ±ªÂûã ÊúâÊÑèÊÄùÁöÑÁâπÊÄßÔºö\n\n1.1 ÂÖ≠ÁßçÊï∞ÊçÆÁ±ªÂûã  ‰∫îÁßçÂéüÂßãÁ±ªÂûãÔºö  number string Boolean null undefined   ‰∏ÄÁßçÂØπË±°Á±ªÂûã  objectÔºö  function Array Date ‚Ä¶      1.2 ÈöêÂºèËΩ¨Êç¢  Â≠óÁ¨¶‰∏≤ÊãºÊé•Ôºö  var x = \u0026#34;The answer is \u0026#34; + 42;\t// \u0026#34;The answer is 42\u0026#34; var y = 42 + \u0026#39; is the answer\u0026#39;;\t// \u0026#34;42 is the answer\u0026#34;  Â∑ßÁî® + / - ËßÑÂàôËΩ¨Êç¢Á±ªÂûãÔºö  num - 0Ôºönum ËΩ¨Êç¢‰∏∫Êï∞Â≠óÁ±ªÂûã num + ''Ôºönum ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤Á±ªÂûã   a == b  Á±ªÂûãÁõ∏ÂêåÔºåÂêå === Á±ªÂûã‰∏çÂêåÊó∂ÔºåÂ∞ùËØïÁ±ªÂûãËΩ¨Êç¢ÂíåÊØîËæÉ  null == undifined Áõ∏Á≠â number == string ËΩ¨ number boolean = ? ËΩ¨ number object == number | string Â∞ùËØïÂ∞ÜÂØπË±°ËΩ¨‰∏∫Âü∫Êú¨Á±ªÂûã     a === bÔºà‰∏•Ê†ºÁ≠â‰∫éÔºâ  È¶ñÂÖàÂà§Êñ≠ = ‰∏§ËæπÁöÑÁ±ªÂûã„ÄÇÁ±ªÂûã‰∏çÂêåÊó∂ÔºåËøîÂõû false  null === null undefined === undefined NaN ‚â† NaN new Object() ‚â† new Object()      1.3 ÂåÖË£ÖÂØπË±° var str = \u0026#39;string\u0026#39;\t// str ‰∏∫Â≠óÁ¨¶‰∏≤ÂØπË±° var strObj = new String(\u0026#39;string\u0026#39;)\t// strObj ‰∏∫ÂØπË±°Á±ªÂûãÔºàStringÁ±ªÂûãÂØπÂ∫îÁöÑ‰∏Ä‰∏™ÂåÖË£ÖÁ±ªÔºâ 1.4 Á±ªÂûãÊ£ÄÊµã   typeof\n ËøîÂõû‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ ÈÄÇÂêàÂáΩÊï∞ÂØπË±°ÂíåÂü∫Êú¨Á±ªÂûãÁöÑÂà§Êñ≠ÔºåÈÅáÂà∞ null Â§±ÊïàÔºàËøîÂõû ObjectÔºâ„ÄÇ    instanceof\n  instanceof ÊòØ JavaScript ÁöÑ‰∏Ä‰∏™‰∫åÂÖÉÊìç‰ΩúÁ¨¶ÔºåÁ±ª‰ºº‰∫é ==Ôºå\u0026gt;Ôºå\u0026lt; Á≠âÊìç‰ΩúÁ¨¶\n  instanceof ÊòØ JavaScript ÁöÑ‰øùÁïôÂÖ≥ÈîÆÂ≠ó\n  ÂÆÉÁöÑ‰ΩúÁî®ÊòØÊµãËØïÂÆÉÂ∑¶ËæπÁöÑÂØπË±°ÊòØÂê¶ÊòØÂÆÉÂè≥ËæπÁöÑÁ±ªÁöÑÂÆû‰æãÔºåËøîÂõû boolean ÁöÑÊï∞ÊçÆÁ±ªÂûã\n  ÈÄÇÂêà‰∫éËá™ÂÆö‰πâÂØπË±°Ôºå‰πüÂèØ‰ª•Áî®Êù•Ê£ÄÊµãÂéüÁîüÂØπË±°\n  Ê≥®ÊÑèÔºö‰∏çÂêå window Êàñ iframe ‰πãÈó¥ÁöÑÂØπË±°Á±ªÂûãÊ£ÄÊµã‰∏çËÉΩ‰ΩøÁî® instanceofÔºÅ\n[1, 2] instanceof Array === true new Object() instanceof Array === false     Object.prototype.toString\n ÈÄöËøá {}.toString ÊãøÂà∞ÔºåÈÄÇÂêàÂÜÖÁΩÆÂØπË±°ÂíåÂü∫ÂÖÉÁ±ªÂûãÔºåÈÅáÂà∞ null Âíå undefined Â§±ÊïàÔºàIE6„ÄÅ7„ÄÅ8Á≠âËøîÂõû [object Object]Ôºâ  Object.prototype.toString.apply([]) === \u0026#39;[object Array]\u0026#39;; Object.prototype.toString.apply(function(){}) === \u0026#39;[object Function]\u0026#39;; Object.prototype.toString.apply(null) === \u0026#39;[object Null]\u0026#39;; Object.prototype.toString.apply(Undefined) === \u0026#39;[object Undefined]\u0026#39;;   constructor\n  duck type\n  ‰∫å„ÄÅË°®ËææÂºèÂíåËøêÁÆóÁ¨¶ 2.1 Ë°®ËææÂºè  „Ää JavaScript ÊùÉÂ®ÅÊåáÂçóÔºàÁ¨¨ 6 ÁâàÔºâ„ÄãÔºö\nË°®ËææÂºè (expression) ÊòØ JavaScript ‰∏≠ÁöÑ‰∏Ä‰∏™Áü≠ËØ≠ÔºåJavaScript Ëß£ÈáäÂô®‰ºöÂ∞ÜÂÖ∂ËÆ°ÁÆóÔºàevaluateÔºâÂá∫‰∏Ä‰∏™ÁªìÊûú„ÄÇ\n  ÂéüÂßãË°®ËææÂºèÔºö   Â∏∏Èáè„ÄÅÁõ¥Êé•ÈáèÔºö3.14„ÄÅ‚Äútest‚Äù ÂÖ≥ÈîÆÂ≠óÔºönull„ÄÅthis„ÄÅtrue ÂèòÈáèÔºöi„ÄÅj„ÄÅk  Â§çÂêàË°®ËææÂºèÔºö  ÂàùÂßãÂåñË°®ËææÂºèÔºö  // [1, 2] new Array(1, 2); // [1, , , 4] [1, undefined, undefined, 4]; // { x : 1, y : 2 } var o = new Object; o.x = 1; o.y = 2; ÂáΩÊï∞Ë°®ËææÂºèÔºö  var f = function() {}; (function() { console.log(\u0026#39;hello world\u0026#39;); }); Â±ûÊÄßËÆøÈóÆË°®ËææÂºèÔºö  var o = { x : 1 }; 0.x; 0[\u0026#39;x\u0026#39;]; Ë∞ÉÁî®Ë°®ËææÂºèÔºöfunc(); ÂØπË±°ÂàõÂª∫Ë°®ËææÂºèÔºö  new Func(1, 2); new Object; 2.2 ËøêÁÆóÁ¨¶  ‰∏ÄÂÖÉËøêÁÆóÁ¨¶Ôºö+num ‰∫åÂÖÉËøêÁÆóÁ¨¶Ôºöa + b ‰∏âÂÖÉËøêÁÆóÁ¨¶Ôºöc ? a : b  ÊåâÂäüËÉΩÂàÜÁ±ªÔºö\n ËµãÂÄºÔºöx += 1 ÊØîËæÉÔºöa == b ÁÆóÊï∞Ôºöa - b ‰ΩçÔºöA | B ÈÄªËæëÔºöexp1 \u0026amp;\u0026amp; exp2 Â≠óÁ¨¶‰∏≤Ôºö‚Äúa‚Äù +\u0026quot;b\u0026quot; ÁâπÊÆäÔºödelete obj.x  ÂÖ∂‰∏≠ÔºåÁâπÊÆäËøêÁÆóÁ¨¶ÊúâÔºö\n  Êù°‰ª∂ËøêÁÆóÁ¨¶ c ? a : b\nvar val = true ? 1 : 2; // val = 1 var val = false ? 1 : 2; // val = 2   ÈÄóÂè∑ËøêÁÆóÁ¨¶ a, bÔºö‰ºö‰ªéÂ∑¶Âà∞Âè≥‰æùÊ¨°ËÆ°ÁÆóË°®ËææÂºèÁöÑÂÄºÔºåÊúÄÂêé‰ºöÂèñÊúÄÂè≥ËæπÁöÑÂÄº\nvar val = (1, 2, 3); // val = 3   deleteÔºöÂà†Èô§‰∏Ä‰∏™ configurable ‰∏∫ true ÁöÑÁßÅÊúâÂ±ûÊÄß\nvar obj = { x : 1 }; obj.x;\t// 1 delete obj.x; obj.x;\t// undefined var obj = {}; Object.defineProperty(obj, \u0026#39;x\u0026#39;, { configurable: false, value: 1 }); delete obj.x;\t// false obj.x;\t// 1   inÔºöÂØπË±°ËÉΩÂ§üËÆøÈóÆÂà∞Â±ûÊÄßÊó∂ËøîÂõû true\nwindow.x = 1; \u0026#39;x\u0026#39; in window;\t// true   instanceof, typeof\n{} instanceof Object; // true typeof 100 === \u0026#39;number\u0026#39;;// true   new\nfunction Foo() {}; Foo.prototype.x = 1; var obj = new Foo(); obj.x;\t// 1 obj.hasOwnProperty(\u0026#39;x\u0026#39;);\t// false obj.__proto__.hasOwnProperty(\u0026#39;x\u0026#39;);\t// true // x ÊòØÂ±û‰∫éÂØπË±°ÂéüÂûã‰∏äÁöÑÂ±ûÊÄßÔºåËÄåÈùûÂÖ∂Êú¨Ë∫´ÁöÑÂ±ûÊÄß // Ê∂âÂèäÂà∞ÂéüÂûãÈìæÁöÑÁü•ËØÜÔºå‰πãÂêé‰ºöÁªßÁª≠Â≠¶‰π†   this\nthis;\t// windowÔºàÊµèËßàÂô®Ôºâ var obj = { func: function(){ return this; } }; obj.func();\t// obj   void\nvoid 0;\t// undefined void(0);// undefined   ËøêÁÆóÁ¨¶‰ºòÂÖàÁ∫ßÔºö\n   ‰ºòÂÖàÁ∫ß ËøêÁÆóÁ±ªÂûã ÂÖ≥ËÅîÊÄß ËøêÁÆóÁ¨¶     20 ÂúÜÊã¨Âè∑ n/a (‚Ä¶)   19 ÊàêÂëòËÆøÈóÆ ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ . ‚Ä¶    ÈúÄË¶ÅËÆ°ÁÆóÁöÑÊàêÂëòËÆøÈóÆ ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶[‚Ä¶]    newÔºàÂ∏¶ÂèÇÊï∞ÂàóË°®Ôºâ n/a new ‚Ä¶(‚Ä¶)    ÂáΩÊï∞Ë∞ÉÁî® ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶(‚Ä¶)   18 newÔºàÊó†ÂèÇÊï∞ÂàóË°®Ôºâ ‰ªéÂè≥Âà∞Â∑¶ new ‚Ä¶   17 ÂêéÁΩÆÈÄíÂ¢ûÔºàËøêÁÆóÁ¨¶Âú®ÂêéÔºâ n/a ‚Ä¶ ++    ÂêéÁΩÆÈÄíÂáèÔºàËøêÁÆóÁ¨¶Âú®ÂêéÔºâ  ‚Ä¶ --   16 ÈÄªËæëÈùû ‰ªéÂè≥Âà∞Â∑¶ ! ‚Ä¶    Êåâ‰ΩçÈùû  ~ ‚Ä¶    ‰∏ÄÂÖÉÂä†Ê≥ï  + ‚Ä¶    ‰∏ÄÂÖÉÂáèÊ≥ï  - ‚Ä¶    ÂâçÁΩÆÈÄíÂ¢û  ++ ‚Ä¶    ÂâçÁΩÆÈÄíÂáè  -- ‚Ä¶    typeof  typeof ‚Ä¶    void  void ‚Ä¶    delete  delete ‚Ä¶    await  await ‚Ä¶   15 ÂπÇ ‰ªéÂè≥Âà∞Â∑¶ ‚Ä¶ ** ‚Ä¶   14 ‰πòÊ≥ï ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ * ‚Ä¶    Èô§Ê≥ï  ‚Ä¶ / ‚Ä¶    ÂèñÊ®°  ‚Ä¶ % ‚Ä¶   13 Âä†Ê≥ï ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ + ‚Ä¶    ÂáèÊ≥ï  ‚Ä¶ - ‚Ä¶   12 Êåâ‰ΩçÂ∑¶Áßª ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ \u0026lt;\u0026lt; ‚Ä¶    Êåâ‰ΩçÂè≥Áßª  ‚Ä¶ \u0026gt;\u0026gt; ‚Ä¶    Êó†Á¨¶Âè∑Âè≥Áßª  ‚Ä¶ \u0026gt;\u0026gt;\u0026gt; ‚Ä¶   11 Â∞è‰∫é ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ \u0026lt; ‚Ä¶    Â∞è‰∫éÁ≠â‰∫é  ‚Ä¶ \u0026lt;= ‚Ä¶    Â§ß‰∫é  ‚Ä¶ \u0026gt; ‚Ä¶    Â§ß‰∫éÁ≠â‰∫é  ‚Ä¶ \u0026gt;= ‚Ä¶    in  ‚Ä¶ in ‚Ä¶    instanceof  ‚Ä¶ instanceof ‚Ä¶   10 Á≠âÂè∑ ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ == ‚Ä¶    ÈùûÁ≠âÂè∑  ‚Ä¶ != ‚Ä¶    ÂÖ®Á≠âÂè∑  ‚Ä¶ === ‚Ä¶    ÈùûÂÖ®Á≠âÂè∑  ‚Ä¶ !== ‚Ä¶   9 Êåâ‰Ωç‰∏é ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ \u0026amp; ‚Ä¶   8 Êåâ‰ΩçÂºÇÊàñ ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ ^ ‚Ä¶   7 Êåâ‰ΩçÊàñ ‰ªéÂ∑¶Âà∞Âè≥ `‚Ä¶   6 ÈÄªËæë‰∏é ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ \u0026amp;\u0026amp; ‚Ä¶   5 ÈÄªËæëÊàñ ‰ªéÂ∑¶Âà∞Âè≥ `‚Ä¶   4 Êù°‰ª∂ËøêÁÆóÁ¨¶ ‰ªéÂè≥Âà∞Â∑¶ ‚Ä¶ ? ‚Ä¶ : ‚Ä¶   3 ËµãÂÄº ‰ªéÂè≥Âà∞Â∑¶ ‚Ä¶ = ‚Ä¶      ‚Ä¶ += ‚Ä¶      ‚Ä¶ -= ‚Ä¶      ‚Ä¶ *= ‚Ä¶      ‚Ä¶ /= ‚Ä¶      ‚Ä¶ %= ‚Ä¶      ‚Ä¶ \u0026lt;\u0026lt;= ‚Ä¶      ‚Ä¶ \u0026gt;\u0026gt;= ‚Ä¶      ‚Ä¶ \u0026gt;\u0026gt;\u0026gt;= ‚Ä¶      ‚Ä¶ \u0026amp;= ‚Ä¶      ‚Ä¶ ^= ‚Ä¶      `‚Ä¶   2 yield ‰ªéÂè≥Âà∞Â∑¶ yield ‚Ä¶    yield*  yield* ‚Ä¶   1 Â±ïÂºÄËøêÁÆóÁ¨¶ n/a ... ‚Ä¶   0 ÈÄóÂè∑ ‰ªéÂ∑¶Âà∞Âè≥ ‚Ä¶ , ‚Ä¶     ‰ºòÂÖàÁ∫ß‰ªéÈ´òÂà∞‰ΩéÊéíÂàóÔºåÂèÇËÄÉ developer.mozilla.org\n ‰∏â„ÄÅËØ≠Âè•„ÄÅ‰∏•Ê†ºÊ®°Âºè  JavaScript Á®ãÂ∫èÁî±ËØ≠Âè•ÁªÑÊàêÔºåËØ≠Âè•ÈÅµÂÆàÁâπÂÆöÁöÑËØ≠Ê≥ïËßÑÂàô„ÄÇ\n‰æãÂ¶ÇÔºöif ËØ≠Âè•Ôºåwhile ËØ≠Âè•Ôºåwith ËØ≠Âè•Á≠âÁ≠â„ÄÇ\n ‰∏ãÈù¢‰ªãÁªç‰∏Ä‰∫õ‰∏ªË¶ÅËØ≠Âè•Ôºö\n3.1 block ËØ≠Âè• ÂùóËØ≠Âè•Â∏∏Áî®ËØ≠ÁªÑÂêà 0 ~ n ‰∏™ËØ≠Âè•ÔºåÁî®‰∏ÄÂØπ {} ÂÆö‰πâ\n{ // ËØ≠Âè• 1;  // ËØ≠Âè• 2;  // ‚Ä¶‚Ä¶  // ËØ≠Âè• n; } ËØ∑Ê≥®ÊÑèÔºöÊ≤°ÊúâÂùóÁ∫ß‰ΩúÁî®Âüü\n3.2 var ËØ≠Âè• function foo() { var a = b = 1;\t// b Áõ∏ÂΩì‰∫éÈöêÂºèÂàõÂª∫‰∫ÜÁöÑ‰∏Ä‰∏™ÂÖ®Â±ÄÂèòÈáè } foo(); console.log(typeof a);\t// \u0026#39;undefined\u0026#39; console.log(typeof b);\t// \u0026#39;number\u0026#39;  // Ê≠£Á°ÆÂÜôÊ≥ï var a = 1, b = 1; 3.3 try - catch ËØ≠Âè• Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂºÇÂ∏∏ÊçïËé∑ÁöÑÊú∫Âà∂\ntry {\t// È¶ñÂÖàÊâßË°å try Âùó‰∏≠ÁöÑ‰ª£Á†Å  throw \u0026#34;test\u0026#34;; }\tcatch (ex) {\t// Â¶ÇÊûúÊäõÂá∫ÂºÇÂ∏∏ÔºåÂàôÊâßË°å catch Âùó‰∏≠ÁöÑËØ≠Âè•  console.log(ex);\t// \u0026#34;test\u0026#34; } finally { // Êó†ËÆ∫Â¶Ç‰ΩïÊúÄÂêéÈÉΩ‰ºöÊâßË°å finally  console.log(\u0026#39;finally\u0026#39;); } // ÊâßË°åÁªìÊûú // test // finally try - catch ÁöÑ‰∏Ä‰∏™ÂµåÂ•ó‰æãÂ≠êÔºö\ntry { try { throw new Error(\u0026#39;oops\u0026#39;); } finally { console.log(\u0026#39;finally\u0026#39;); } } catch (ex) { console.error(\u0026#39;outer\u0026#39;, ex.message); } // ÊâßË°åÁªìÊûú‰∏∫Ôºö // finally // outer oops Âèà‰∏Ä‰∏™‰æãÂ≠êÔºö\ntry { try { throw new Error(\u0026#39;oops\u0026#39;); } catch (ex) { console.error(\u0026#39;inner\u0026#39;, ex.message); } finally { console.log(\u0026#39;finally\u0026#39;); } } catch (ex) { console.error(\u0026#39;outer\u0026#39;, ex.message); } // ÊâßË°åÁªìÊûú‰∏∫ // inner oops // finally ÂÜçÊù•‰∏Ä‰∏™Êõ¥Â§çÊùÇÁöÑ‰æãÂ≠êÔºö\ntry { try { throw new Error(\u0026#39;oops\u0026#39;); } catch (ex) { console.error(\u0026#39;inner\u0026#39;, ex.message); throw ex; } finally { console.log(\u0026#39;finally\u0026#39;); } } catch (ex) { console.error(\u0026#39;outer\u0026#39;, ex.message); } // ÊâßË°åÁªìÊûú‰∏∫ // inner oops // finally // outer oops 3.4 function ËØ≠Âè• function ËØ≠Âè•Áî®Êù•ÂÆö‰πâÂáΩÊï∞ÂØπË±°ÔºåÂ¶ÇÔºö\nfunction fd() { // do sth.  return true; } Áî® function ËØ≠Âè•ÂÆö‰πâÁöÑÂáΩÊï∞ÂØπË±°Áß∞‰πã‰∏∫ ÂáΩÊï∞Â£∞ÊòéÔºåËÄå‰∏é‰πãÂØπÂ∫îÁöÑÂè¶‰∏ÄÁßçÂè´ÂÅö ÂáΩÊï∞Ë°®ËææÂºèÔºåÂ¶ÇÔºö\nvar fe = function() { // do sth. }; ÂáΩÊï∞Â£∞Êòé‰∏éÂáΩÊï∞Ë°®ËææÂºèÁöÑÂå∫Âà´ÊúâÂæàÂ§öÔºåÂÖ∂‰∏≠‰∏ªË¶ÅÁöÑ‰∏ÄÁÇπÂú®‰∫éÂú®‰∫éÔºöÂáΩÊï∞Â£∞Êòé‰ºöË¢´È¢ÑÂÖàÂ§ÑÁêÜÔºåÊàñËÄÖËØ¥ ÂáΩÊï∞ÂâçÁΩÆÔºö\nfd();\t// true function fd() { // do sth.  return true; } fe();\t// TypeError var fe = function() { // do sth. } Èô§‰∫ÜËøô‰∏§ÁßçÊñπÂºè‰πãÂ§ñÔºåËøòÂèØ‰ª•ÈÄöËøá new function(){} ÊûÑÈÄ†Âô®ÁöÑÊñπÂºèÂéªÂàõÂª∫ÂáΩÊï∞ÂØπË±°ÔºåÂÆÉ‰ª¨‰πãÈó¥ÁöÑÂå∫Âà´‰ºöÂú®‰πãÂêéÂ±ïÂºÄËÆ®ËÆ∫„ÄÇ\n3.5 for ‚Ä¶ in ËØ≠Âè• ËøîÂõûÊâÄÊúâËÉΩÂ§üËÆøÈóÆÂà∞ÁöÑÂ±ûÊÄß\nvar p; var obj = { x : 1, y : 2 }; for (p in obj) {\t// ÂØπ obj ÂØπË±°‰∏≠ÁöÑÂ±ûÊÄßËøõË°åÈÅçÂéÜ  // 1. È°∫Â∫è‰∏çÁ°ÆÂÆö  // 2. enumerable ‰∏∫ false Êó∂‰∏ç‰ºöÂá∫Áé∞  // 3. for in ÂØπË±°Â±ûÊÄßÊó∂ÂèóÂéüÂûãÈìæÂΩ±Âìç } 3.6 switch ËØ≠Âè• switch(val) { case 1: console.log(1); break; case 2: console.log(2); break; default: console.log(0); break; } switch (val) { case 1: case 2: case 3: console.log(123); break; case 4: case 5: console.log(45); break; default: console.log(0); break; } 3.7 Âæ™ÁéØËØ≠Âè• // while Âæ™ÁéØ while (isTrue) { // do sth. } // do ‚Ä¶ while Âæ™ÁéØ do { // do sth. } while (isTrue) // for Âæ™ÁéØ var i; for (i = 0; i \u0026lt; n; i ++) { // do sth. } 3.8 with ËØ≠Âè• ÂèØ‰ª•‰øÆÊîπÂΩìÂâç‰ΩúÁî®ÁöÑ‰ΩúÁî®Âüü\nwith ({ x : 1 }) { console.log(x); } with (document.forms[0]) { console.log(name.value);\t// Áõ∏ÂΩì‰∫é console.log(document.forms[0].name.value); } ÂΩìÁÑ∂ÔºåËøôÈáåÂÆåÂÖ®ÂèØ‰ª•ÈÄöËøáÂÆö‰πâ‰∏Ä‰∏™ÂèòÈáèÊù•‰ª£ÊõøÔºö\nvar form = document.forms[0]; console.log(form.name.value); JavaScript ‰∏≠Â∑≤Áªè‰∏çÂª∫ËÆÆ‰ΩøÁî® with ‰∫ÜÔºö\n ËÆ© JS ÂºïÊìé‰ºòÂåñÊõ¥Èöæ ÂèØËØªÊÄßÂ∑Æ ÂèØË¢´ÂèòÈáèÂÆö‰πâ‰ª£Êõø ‰∏•Ê†ºÊ®°Âºè‰∏ãË¢´Á¶ÅÁî®  3.9 JavaScript ‰∏•Ê†ºÊ®°Âºè ‰∏•Ê†ºÊ®°ÂºèÊòØ‰∏ÄÁßçÁâπÊÆäÁöÑÊâßË°åÊ®°ÂºèÔºåÂÆÉÈÉ®ÂàÜ‰øÆÂ§ç‰∫ÜËØ≠Ë®Ä‰∏äÁöÑ‰∏çË∂≥ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÈîôËØØÊ£ÄÊü•ÔºåÂπ∂Â¢ûÂº∫ÂÆâÂÖ®ÊÄß„ÄÇ\nËøõÂÖ•‰∏•Ê†ºÊ®°ÂºèÔºö\nfunction func() { \u0026#39;use strict\u0026#39;;\t// ËøõÂÖ•‰∏•Ê†ºÊ®°ÂºèÁöÑÊåá‰ª§ } // or \u0026#39;use strict\u0026#39;; function func() { // do sth. } ‰∏éÊôÆÈÄöÊ®°ÂºèÁöÑ‰∏Ä‰∫õÂå∫Âà´Ôºö\n  ‰∏çÂÖÅËÆ∏‰ΩøÁî® withÔºö\n!function() { \u0026#39;use strict\u0026#39;; with ({ x : 1 }) { console.log(x); } }(); // SyntaxErrorÔºàËØ≠Ê≥ïÈîôËØØÔºâ   ‰∏çÂÖÅËÆ∏Êú™Â£∞ÊòéÁöÑÂèòÈáèË¢´ËµãÂÄºÔºö\n!function() { \u0026#39;use strict\u0026#39;; x = 1; console.log(window.x); }(); // ReferenceError   arguments Âèò‰∏∫ÂèÇÊï∞ÁöÑÈùôÊÄÅÂâØÊú¨Ôºö\n!function(a) { arguments[0] = 100; console.log(a);\t}(1);\t// 1 =\u0026gt; 100; ‰∏ç‰º† =\u0026gt; undefined // 100  !function(a) { \u0026#39;use strict\u0026#39;; arguments[0] = 100;\t// arguments Âèò‰∏∫ÂèÇÊï∞ÁöÑÈùôÊÄÅÂâØÊú¨  console.log(a);\t}(1); // 1  !function(a) { \u0026#39;use strict\u0026#39;; arguments[0].x = 100;\t// Â¶ÇÊûúÂèÇÊï∞ÊòØÂØπË±°ÔºåÈÇ£‰πà‰øÆÊîπÂØπË±°ÁöÑÂ±ûÊÄß‰æùÁÑ∂‰ºöÂΩ±Âìç  console.log(a.x);\t}({ x : 1 }) // 100   delete ÂèÇÊï∞„ÄÅÂáΩÊï∞ÂêçÊä•ÈîôÔºö\n!function(a) { console.log(delete a);\t}(1); // falseÔºàÂà†Èô§Â§±Ë¥•Ôºâ  !function(a) { \u0026#39;use\u0026#39; delete a; } // SyntaxError delete ‰∏çÂèØÈÖçÁΩÆÁöÑÂ±ûÊÄßÊä•ÈîôÔºö\n!function(a) { var obj = {}; Object.defineProperty(obj, \u0026#39;a\u0026#39;, {configurable: false}); console.log(delete obj.a);\t}(1); // falseÔºàÂà†Èô§Â§±Ë¥•Ôºâ  !function(a) { \u0026#39;use strict\u0026#39;; var obj = {}; Object.defineProperty(obj, \u0026#39;a\u0026#39;, {configurable: false}); console.log(delete obj.a);\t}(1); // TypeError   ÂØπË±°Â≠óÈù¢ÈáèÈáçÂ§çÂ±ûÊÄßÂêçÊä•ÈîôÔºö\n!function() { var obj = { x : 1, x : 2 }; console.log(obj.x); }(); // 2  !function() { \u0026#39;use strict\u0026#39;; var obj = { x : 1, x : 2 }; }(); // SyntaxError   Á¶ÅÊ≠¢ÂÖ´ËøõÂà∂Â≠óÈù¢Èáè\n!function() { console.log(0123);\t// ÂÖ´ËøõÂà∂ÂÄº } // 83  !function() { \u0026#39;use strict\u0026#39;; console.log(0123); } // SyntaxError   eval„ÄÅarguments Âèò‰∏∫ÂÖ≥ÈîÆÂ≠óÔºå‰∏çËÉΩ‰Ωú‰∏∫ÂèòÈáè„ÄÅÂáΩÊï∞ÂêçÔºö\n!function() { function eval() {} console.log(eval); }(); // function eval() {}  !function() { \u0026#39;use strict\u0026#39;; function eval() {} }(); // SyntaxError eval Áã¨Á´ã‰ΩúÁî®ÂüüÔºö\n!function() { eval(\u0026#39;var evalVal = 2;\u0026#39;); console.log(typeof evalVal); }(); // number  !function() { \u0026#39;use strict\u0026#39;; eval(\u0026#39;var evalVal = 2;\u0026#39;); console.log(typeof evalVal); }(); // undefined   ‰∏ÄËà¨ÂáΩÊï∞Ë∞ÉÁî®Êó∂Ôºà‰∏çÊòØÂØπË±°ÁöÑÊñπÊ≥ïË∞ÉÁî®Ôºå‰πü‰∏ç‰ΩøÁî® apply / call / bind Á≠â‰øÆÊîπ thisÔºâthis ÊåáÂêë nullÔºåËÄå‰∏çÊòØÂÖ®Â±ÄÂØπË±°\n  Ëã•ÊòØÊúâ apply / callÔºåÂΩì‰º†ÂÖ• null ÊàñËÄÖ undefined Êó∂Ôºåthis Â∞ÜÊåáÂêë null Êàñ undefinedÔºåËÄå‰∏çÊòØÂÖ®Â±ÄÂØπË±°\n  ËØïÂõæ‰øÆÊîπ‰∏çÂèØÂÜôÂ±ûÊÄßwritable = false„ÄÅÂú®‰∏çÂèØÊâ©Â±ïÁöÑÂØπË±°‰∏äÊ∑ªÂä†Â±ûÊÄßÊó∂Êä• TypeErrorÔºåËÄå‰∏çÊòØÂøΩÁï•\n  arguments.caller„ÄÅarguments.callee Ë¢´Á¶ÅÁî®\n  ‚Ä¶‚Ä¶\n  ‰∏•Ê†ºÊ®°ÂºèÊòØÂêë‰∏äÂÖºÂÆπÁöÑÔºåÂØπ‰∫éÁºñÂÜôÈ´òË¥®Èáè„ÄÅÂÅ•Â£ÆÁöÑ‰ª£Á†ÅÊúâÂæàÂ§ßÁöÑÂ∏ÆÂä©„ÄÇ\n","permalink":"https://fang-lansheng.github.io/posts/2019-01-09-javascript-study-notes-1/","summary":"Êú¨ÁØáÂ≠¶‰π†Á¨îËÆ∞ÊòØÂú®ÊÖïËØæÁΩë‰∏äÂ≠¶‰π†JavaScriptÊ∑±ÂÖ•ÊµÖÂá∫Êó∂ÊâÄÂÅöÁ¨îËÆ∞ÔºåÂÜÖÂÆπ‰∏ªË¶ÅÊù•Ëá™ËØæÁ®ãËØæ‰ª∂ÂèäËÄÅÂ∏àËÆ≤Ëß£ÔºåÂêåÊó∂Á©øÊèíËá™Â∑±ÁöÑ‰∏Ä‰∫õÁêÜËß£ÂíåÂ∞ùËØï„ÄÇÈÉ®ÂàÜËµÑÊñôÊï¥ÁêÜËá™ MDN Web Docs„ÄÇ\n ‰∏Ä„ÄÅÊï∞ÊçÆÁ±ªÂûã ÊúâÊÑèÊÄùÁöÑÁâπÊÄßÔºö\n\n1.1 ÂÖ≠ÁßçÊï∞ÊçÆÁ±ªÂûã  ‰∫îÁßçÂéüÂßãÁ±ªÂûãÔºö  number string Boolean null undefined   ‰∏ÄÁßçÂØπË±°Á±ªÂûã  objectÔºö  function Array Date ‚Ä¶      1.2 ÈöêÂºèËΩ¨Êç¢  Â≠óÁ¨¶‰∏≤ÊãºÊé•Ôºö  var x = \u0026#34;The answer is \u0026#34; + 42;\t// \u0026#34;The answer is 42\u0026#34; var y = 42 + \u0026#39; is the answer\u0026#39;;\t// \u0026#34;42 is the answer\u0026#34;  Â∑ßÁî® + / - ËßÑÂàôËΩ¨Êç¢Á±ªÂûãÔºö  num - 0Ôºönum ËΩ¨Êç¢‰∏∫Êï∞Â≠óÁ±ªÂûã num + ''Ôºönum ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤Á±ªÂûã   a == b  Á±ªÂûãÁõ∏ÂêåÔºåÂêå === Á±ªÂûã‰∏çÂêåÊó∂ÔºåÂ∞ùËØïÁ±ªÂûãËΩ¨Êç¢ÂíåÊØîËæÉ  null == undifined Áõ∏Á≠â number == string ËΩ¨ number boolean = ?","title":"JavaScript Â≠¶‰π†Á¨îËÆ∞Ôºà‰∏ÄÔºâÔºöÊï∞ÊçÆÁ±ªÂûã„ÄÅË°®ËææÂºè„ÄÅËøêÁÆóÁ¨¶ÂíåËØ≠Âè•"},{"content":"ÂâçË®Ä ÂçöÂÆ¢Âü∫Êú¨Êê≠Âª∫Â•Ω‰∫ÜÔºå‰∏çËøáËøòÊúâ‰∏çÂ∞ëÁªÜËäÇÈúÄË¶ÅÂÆåÂñÑ‰∏Ä‰∏ã„ÄÇÊµèËßà‰∫Ü‰∏Ä‰∫õ‰ªñ‰∫∫Á≤æËá¥ÁöÑÂçöÂÆ¢ÁΩëÁ´ôÔºåÂèëÁé∞ËøòÊúâÂæàÂ§öÂÄºÂæóÂ≠¶‰π†ÁöÑÂú∞Êñπ„ÄÇÊé•‰∏ãÊù•Â∞±‰ªé ËØÑËÆ∫Á≥ªÁªü „ÄÅ‰æßËæπÊ†èÁõÆÂΩï ‰ª•Âèä ‰∏≠ÊñáÂ≠óÊï∞ÁªüËÆ° ‰∏â‰∏™ÊñπÈù¢ÂÖ•ÊâãÔºåËøõË°å‰∏Ä‰∫õÁæéÂåñ„ÄÇ\nËØÑËÆ∫Á≥ªÁªü  Êê≠Âª∫ÂèÇËÄÉÊïôÁ®ãÔºö‰∏ÄÊ≠•‰∏ÄÊ≠•Êïô‰Ω†Âú®JekyllÂçöÂÆ¢Ê∑ªÂä†ËØÑËÆ∫Á≥ªÁªü - ‰∏Ä‰πãÁ¨îÁöÑÂçöÂÆ¢\n ‰πãÂâçÂú®ÂÖ∂ÂÆû‰ΩøÁî®Ëøá DisqusÔºå‰ΩÜÊòØÂá†ÁªèÊØîËæÉËøòÊòØËßâÂæó Gitalk Êõ¥Âä†ÁæéËßÇ‰∫õÔºåËÄå‰∏î‰ΩøÁî® GitHub Ë¥¶Âè∑ÁôªÂΩï‰πüÊØîËæÉÊñπ‰æøÔºåÊîØÊåÅ markdown ËØ≠Ê≥ïËøô‰∏ÄÁÇπ‰πüÊ∑±ÂæóÊàëÊÑè ÔºàÂÖ∂ÂÆûÊ†πÊú¨Ê≤°Êúâ‰∫∫‰ºöËØÑËÆ∫Ôºâ„ÄÇÊâÄ‰ª•ÊúÄÂêéËøòÊòØÂÖ≥Êéâ‰∫Ü DisqusÔºåÁùÄÊâãÂºÄÂßãÊê≠Âª∫ Gitalk Âï¶ ~\nÁî≥ËØ∑ GitHub OAuth Application Âú® GitHub ‰∏≠ÁÇπÂáªÂ§¥ÂÉè‰∏ãÊãâËèúÂçïÔºåÈÄâÊã© SettingsÔºåÁÑ∂ÂêéÁÇπÂáªÂ∑¶‰æßÁöÑ Developer settings ÔºåÊé•‰∏ãÊù•ÁÇπÂáª OAuth Apps \u0026gt; Register a new applicationÔºåÂºÄÂßãÂ°´ÂÜô‰Ω†ÁöÑ‰ø°ÊÅØÔºö\nÁÇπÂáª Register application ÂêéÔºåÂ∞±‰ºöÂæóÂà∞‰Ω†ÁöÑ Client ID Âíå Client Secret„ÄÇ\nÂú® Jekyll ‰∏≠Ê∑ªÂä† Gitalk Âú®‰Ω†ÈúÄË¶ÅÊ∑ªÂä†ËØÑËÆ∫Á≥ªÁªüÁöÑÂú∞ÊñπÔºå‰∏ÄËà¨ÊòØ _layout/ ÁõÆÂΩï‰∏ãÁöÑ post.htmlÔºö\nÂú® Post Header ‰∏≠ÔºåÂºïÂÖ• gitalk.css Âíå gitalk.min.js Ôºö\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://unpkg.com/gitalk/dist/gitalk.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/gitalk/dist/gitalk.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Âú® Post Container Â∞æÈÉ®ÔºåÊ∑ªÂä†ÂÆπÂô®Âíå JavaScript ‰ª£Á†ÅÔºö\n\u0026lt;!-- Gitalk ËØÑËÆ∫Ê°Ü start --\u0026gt; \u0026lt;div class=\u0026#34;comment\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;gitalk-container\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; var gitalk = new Gitalk({ clientID: \u0026#39;GitHub Application Client ID\u0026#39;, clientSecret: \u0026#39;GitHub Application Client Secret\u0026#39;, repo: \u0026#39;GitHub repo\u0026#39;, owner: \u0026#39;GitHub repo owner\u0026#39;, admin: [\u0026#39;GitHub repo owner and collaborators, only these guys can initialize github issues\u0026#39;], id: location.pathname, // Ensure uniqueness and length less than 50  distractionFreeMode: false // Facebook-like distraction free mode  }) gitalk.render(\u0026#39;gitalk-container\u0026#39;) \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- Gitalk ËØÑËÆ∫Ê°Ü end --\u0026gt; ÂÖ∑‰ΩìËÆæÁΩÆÂèÇËÄÉ Gitalk ÁöÑ‰∏≠ÊñáËØ¥Êòé„ÄÇ\n‰øùÂ≠òÂêéÔºåÊèê‰∫§‰ª£Á†ÅÂ∞±Ë°åÂï¶„ÄÇ\n‰æßËæπÊ†èÁõÆÂΩï Ëøô‰∏™ÈóÆÈ¢òÊúâÁÇπ‰πåÈæôÔºåÂÖ∂ÂÆûÊàë Fork ÁöÑ Hux Blog ‰∏ªÈ¢ò‰∏≠Â∑≤ÁªèËá™Â∏¶ catalogÔºåÊàëÂú® post.html ‰∏≠‰πüÁúãÂà∞‰∫ÜÁõ∏ÂÖ≥ÁöÑ div ÂùóÔºö\n\u0026lt;!-- Side Catalog Container --\u0026gt; {% if page.catalog %} \u0026lt;div class=\u0026#34; col-lg-2 col-lg-offset-0 visible-lg-block sidebar-container catalog-container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;side-catalog\u0026#34;\u0026gt; \u0026lt;hr class=\u0026#34;hidden-sm hidden-xs\u0026#34;\u0026gt; \u0026lt;h5\u0026gt; \u0026lt;a class=\u0026#34;catalog-toggle\u0026#34; href=\u0026#34;#\u0026#34;\u0026gt;CATALOG\u0026lt;/a\u0026gt; \u0026lt;/h5\u0026gt; \u0026lt;ul class=\u0026#34;catalog-body\u0026#34;\u0026gt;\u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% endif %} Âõ†Ê≠§ÔºåÂè™Ë¶ÅÂú® .md Êñá‰ª∂‰∏≠Ê∑ªÂä†‰∏ÄË°å‰ª£Á†ÅÂç≥ÂèØÔºö\n--- layout: post title: \u0026#34;‚Ä¶‚Ä¶\u0026#34; subtitle: \u0026#34;‚Ä¶‚Ä¶\u0026#34; date: ‚Ä¶‚Ä¶ author: \u0026#34;Thistledown\u0026#34; header-img: \u0026#34;‚Ä¶‚Ä¶\u0026#34; # Ê∑ªÂä†‰∏ãÈù¢ËøôË°å‰ª£Á†Å catalog: true tags: ‚Ä¶‚Ä¶ --- Ê∑ªÂä†‰∏≠ÊñáÂ≠óÊï∞ÁªüËÆ° ÊÉ≥Ë¶ÅÁªüËÆ°ÊñáÁ´†ÁöÑÂ≠óÊï∞ÔºåÈ¶ñÂÖàÊàë‰ª¨ÈúÄË¶ÅÂæóÂà∞ÊñáÁ´†ÁöÑÂÜÖÂÆπÔºåÂç≥ page.content„ÄÇ\n Page-Variables - JekyllÔºö\npage.content : The content of the Page, rendered or un-rendered depending upon what Liquid is being processed and what page is.\n ËÄåÊ®°ÊùøËØ≠Ë®Ä Liquid ÊòØ Jekyll ÁöÑÁâπËâ≤‰πã‰∏ÄÔºåÂÆÉÊúâ‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÂØπË±°ÔºåÊ†áÁ≠æÂíåËøáÊª§Âô®„ÄÇ‰∫ÜËß£Êõ¥Â§öÔºöjekyllrb.com„ÄÇ\nËøáÊª§Âô® filter Êõ¥Êîπ Liquid ÂØπË±°ÁöÑËæìÂá∫ÔºåÂπ∂Áî± | ÂàÜÈöî„ÄÇÂÖ∂‰∏≠ÊÅ∞Â•ΩÂ∞±ÊúâÁé∞ÊàêÁöÑÂ≠óÊï∞ÁªüËÆ°ËøáÊª§Âô® number_of_wordsÔºåÁÑ∂ËÄåÔºåÂÆÉÁî®Êù•ÁªüËÆ°Ëã±ÊñáÈùûÂ∏∏Êñπ‰æøÔºåÁªüËÆ°‰∏≠ÊñáÂç¥ÈîôËØØÈ¢ëÂá∫ÔºåÂæÄÂæÄ‰∏ÄÂè•ÊàñËÄÖ‰∏ÄÊÆµËØùÂè™ÁªüËÆ°‰∏∫‰∏Ä‰∏™ËØç„ÄÇÊé®ÊµãËøôÁßçÊñπÊ≥ïÂèØËÉΩÊòØÁî®Á©∫Ê†ºÊù•ÂàÜÂâ≤Â≠óÁ¨¶ËøõË°åÁªüËÆ°ÔºåËøôÊòéÊòæ‰∏çÁ¨¶ÂêàÊàë‰ª¨‰∏≠ÊñáÂÜô‰ΩúÁöÑÂÆûÈôÖÔºåÂõ†Ê≠§ÈúÄË¶ÅÁ®çÂæÆËµ∞ÁÇπÂºØË∑ØÔºåÁî®ÂÖ∂‰ªñÁöÑÊñπÊ≥ïÊù•Ëß£ÂÜ≥„ÄÇ\nÊàë‰ª¨ÈúÄË¶ÅÁöÑ Liquid Filter ÊúâÔºö\n strip_htmlÔºö‰ªéÂ≠óÁ¨¶‰∏≤‰∏≠Âà†Èô§ÊâÄÊúâÁöÑ HTML Ê†áËÆ∞ strip_newlinesÔºö‰ªéÂ≠óÁ¨¶‰∏≤‰∏≠Âà†Èô§ÊâÄÊúâÊç¢Ë°åÁ¨¶ splitÔºöÊòØÁî®ÂèÇÊï∞‰Ωú‰∏∫ÂàÜÈöîÁ¨¶Â∞ÜËæìÂÖ•Â≠óÁ¨¶‰∏≤ÂàÜÂâ≤‰∏∫Êï∞ÁªÑ sizeÔºöËøîÂõûÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑÂ≠óÁ¨¶Êï∞ÊàñÊï∞ÁªÑ‰∏≠ÁöÑÈ°πÊï∞  ÂéªÊéâÊñáÁ´†ÂÜÖÂÆπ‰∏≠ÁöÑ HTML Ê†áÁ≠æÂíåÊç¢Ë°åÁ¨¶ÔºåÂ∞ÜÊâÄÊúâÂ≠óÁ¨¶ÈÉΩÂçï‰∏™Âú∞Â≠òÂÇ®Âú®Êï∞ÁªÑ‰∏≠ÔºåÂè™Ë¶ÅÁªüËÆ°Êï∞ÁªÑÁöÑ sizeÔºåÂ∞±ËÉΩÂ§üÁªüËÆ°Âá∫ÊñáÁ´†Â≠óÊï∞„ÄÇÂõ†Ê≠§Ôºå‰ª£Á†Å‰πüÂ∞±ÈùûÂ∏∏ÁÆÄÂçïÔºö\n\u0026lt;!-- Number of words --\u0026gt; {% raw %}{{ page.content | strip_html | strip_newlines | split: \u0026#34;\u0026#34; | size }}{% endraw %} ÂÖ∂‰ªñ ËøôÂá†Â§©ÊäòËÖæÂçöÂÆ¢‰πüËä±Ë¥π‰∫Ü‰∏çÂ∞ëÊó∂Èó¥ÔºåÊúüÈó¥Ë∏©‰∫Ü‰∏çÂ∞ëÂùëÔºåÂñÑÁî®ÊêúÁ¥¢ÂºïÊìéÂíå GitHub Issues ÂèØ‰ª•Ëß£ÂÜ≥ÂæàÂ§öÈóÆÈ¢òÔºåÂêåÊó∂‰πüÊÑüË∞¢‰∏Ä‰∫õ‰πê‰∫éÂàÜ‰∫´ÁöÑÂçö‰∏ªÁöÑÂ∏ÆÂä©„ÄÇÊúÄÂêéÔºåÊÑüË∞¢ÂºÄÊ∫êÁ§æÂå∫„ÄÇ\n","permalink":"https://fang-lansheng.github.io/posts/2019-01-06-making-blog-look-better/","summary":"ÂâçË®Ä ÂçöÂÆ¢Âü∫Êú¨Êê≠Âª∫Â•Ω‰∫ÜÔºå‰∏çËøáËøòÊúâ‰∏çÂ∞ëÁªÜËäÇÈúÄË¶ÅÂÆåÂñÑ‰∏Ä‰∏ã„ÄÇÊµèËßà‰∫Ü‰∏Ä‰∫õ‰ªñ‰∫∫Á≤æËá¥ÁöÑÂçöÂÆ¢ÁΩëÁ´ôÔºåÂèëÁé∞ËøòÊúâÂæàÂ§öÂÄºÂæóÂ≠¶‰π†ÁöÑÂú∞Êñπ„ÄÇÊé•‰∏ãÊù•Â∞±‰ªé ËØÑËÆ∫Á≥ªÁªü „ÄÅ‰æßËæπÊ†èÁõÆÂΩï ‰ª•Âèä ‰∏≠ÊñáÂ≠óÊï∞ÁªüËÆ° ‰∏â‰∏™ÊñπÈù¢ÂÖ•ÊâãÔºåËøõË°å‰∏Ä‰∫õÁæéÂåñ„ÄÇ\nËØÑËÆ∫Á≥ªÁªü  Êê≠Âª∫ÂèÇËÄÉÊïôÁ®ãÔºö‰∏ÄÊ≠•‰∏ÄÊ≠•Êïô‰Ω†Âú®JekyllÂçöÂÆ¢Ê∑ªÂä†ËØÑËÆ∫Á≥ªÁªü - ‰∏Ä‰πãÁ¨îÁöÑÂçöÂÆ¢\n ‰πãÂâçÂú®ÂÖ∂ÂÆû‰ΩøÁî®Ëøá DisqusÔºå‰ΩÜÊòØÂá†ÁªèÊØîËæÉËøòÊòØËßâÂæó Gitalk Êõ¥Âä†ÁæéËßÇ‰∫õÔºåËÄå‰∏î‰ΩøÁî® GitHub Ë¥¶Âè∑ÁôªÂΩï‰πüÊØîËæÉÊñπ‰æøÔºåÊîØÊåÅ markdown ËØ≠Ê≥ïËøô‰∏ÄÁÇπ‰πüÊ∑±ÂæóÊàëÊÑè ÔºàÂÖ∂ÂÆûÊ†πÊú¨Ê≤°Êúâ‰∫∫‰ºöËØÑËÆ∫Ôºâ„ÄÇÊâÄ‰ª•ÊúÄÂêéËøòÊòØÂÖ≥Êéâ‰∫Ü DisqusÔºåÁùÄÊâãÂºÄÂßãÊê≠Âª∫ Gitalk Âï¶ ~\nÁî≥ËØ∑ GitHub OAuth Application Âú® GitHub ‰∏≠ÁÇπÂáªÂ§¥ÂÉè‰∏ãÊãâËèúÂçïÔºåÈÄâÊã© SettingsÔºåÁÑ∂ÂêéÁÇπÂáªÂ∑¶‰æßÁöÑ Developer settings ÔºåÊé•‰∏ãÊù•ÁÇπÂáª OAuth Apps \u0026gt; Register a new applicationÔºåÂºÄÂßãÂ°´ÂÜô‰Ω†ÁöÑ‰ø°ÊÅØÔºö\nÁÇπÂáª Register application ÂêéÔºåÂ∞±‰ºöÂæóÂà∞‰Ω†ÁöÑ Client ID Âíå Client Secret„ÄÇ\nÂú® Jekyll ‰∏≠Ê∑ªÂä† Gitalk Âú®‰Ω†ÈúÄË¶ÅÊ∑ªÂä†ËØÑËÆ∫Á≥ªÁªüÁöÑÂú∞ÊñπÔºå‰∏ÄËà¨ÊòØ _layout/ ÁõÆÂΩï‰∏ãÁöÑ post.htmlÔºö\nÂú® Post Header ‰∏≠ÔºåÂºïÂÖ• gitalk.css Âíå gitalk.min.js Ôºö\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://unpkg.","title":"Jekyll ÂçöÂÆ¢ÁæéÂåñÔºöÊ∑ªÂä†ËØÑËÆ∫Á≥ªÁªü„ÄÅ‰æßËæπÊ†èÁõÆÂΩï‰∏é‰∏≠ÊñáÂ≠óÊï∞ÁªüËÆ°"},{"content":"ÂâçË®Ä ‰∏ä‰∏Ä‰∏™Âü∫‰∫é WordPress ÁöÑÂçöÂÆ¢Â∑≤ÁªèÂú®ÈáçË£ÖÊúçÂä°Âô®Á≥ªÁªü‰πãÂêéÁÉüÊ∂à‰∫ëÊï£‰∫ÜÔºåÂõ†‰∏∫Êê≠Âª∫ËøáÁ®ãÂÖ®Á®ãÁôæÂ∫¶ÔºåËá™Â∑±ÊØ´Êó†Âç∞Ë±°Ôºå‰πüÂ∞±‰∏çÊÑøÊÑèÂÜçÈáçÂ§ç‰∏ÄÈÅçÊú∫Ê¢∞ÂºèÂú∞ÊäÑÂÜô„ÄÇËøô‰∏ÄÂõûÊâìÁÆóÂèÇËÄÉÁΩë‰∏äÁöÑÊïôÁ®ãÔºåÈáçÊñ∞Êê≠Âª∫‰∏Ä‰∏™Ëá™Â∑± blogÔºåÂπ∂‰∏îÊääËøáÁ®ãËÆ∞ÂΩï‰∏ãÊù•ÔºåÊùÉÂΩìÂÅö‰∏Ä‰∏™Â≠¶‰π†ÁöÑËøáÁ®ãÂêß„ÄÇ\nÊê≠Âª∫ÊïôÁ®ã‰∏ªË¶ÅÂèÇËÄÉÔºö\n Á®ãÂ∫èÂëòÂ¶Ç‰ΩïÊê≠Âª∫Ëá™Â∑±ÁöÑ‰∏™‰∫∫ÂçöÂÆ¢ - ÊñπÂøóÊúãÁöÑÂçöÂÆ¢\nÊÑüË∞¢ @ÊñπÂøóÊúã Âú®ÂçöÂÆ¢Êê≠Âª∫ËøáÁ®ã‰∏≠ÁöÑÊåáÂØº\n ‰ΩøÁî®Ëá™Â∑±ÁöÑÊúçÂä°Âô®ÈÉ®ÁΩ≤ÂçöÂÆ¢ ÂèÇËÄÉÊïôÁ®ãÔºåÊàë‰πüÊòØ‰ΩøÁî® Jekyll Êê≠Âª∫ÂçöÂÆ¢„ÄÇÊúçÂä°Âô®Á≥ªÁªü‰∏∫ Centos 6 x86_64 bbr \nVPS ÂèäÂüüÂêçË¥≠‰π∞ VPS ÊòØ‰π∞ÁöÑÊê¨Áì¶Â∑•Ôºå‰∏ÄÂπ¥‰∏çÂà∞ $20ÔºåÁõ∏ÂΩìÂàíÁÆó„ÄÇÂüüÂêçÂàôÊòØÊù•Ëá™ÈòøÈáå‰∫ë„ÄÇËøô‰∏§ËÄÖÂ∑≤ÁªèÊòØÂæà‰πÖ‰πãÂâçË¥≠‰π∞ÁöÑ‰∫ÜÔºåÁΩëÁªú‰∏äÁõ∏ÂÖ≥ÊïôÁ®ãÂèä‰ªãÁªç‰πüÈùûÂ∏∏ËØ¶Â∞ΩÔºåÊïÖ‰∏çÂú®Ê≠§ËµòËø∞„ÄÇ\nËøûÊé•Âà∞ÊúçÂä°Âô® ËøôÈáåÊàëÊòØÁî®ÁöÑÊòØ XshellÔºåPutty ‰πü‰∏çÈîô„ÄÇÂõ†‰∏∫‰∏ç‰∫ÜËß£ÂéüÁêÜÔºåÊâÄ‰ª•‰πü‰∏çÂ§öËÆ≤ÔºåÂÆûÈôÖÊìç‰Ωú‰∏≠ÂñÑÁî®ÊêúÁ¥¢ÂºïÊìéÂü∫Êú¨Â∞±ËÉΩÊéåÊè°Êìç‰Ωú„ÄÇ\nÂÆâË£Ö Node ‰ΩøÁî® Xshell ËøûÊé•Âà∞ÊúçÂä°Âô®‰πãÂêé„ÄÇÂÆâË£Ö Node ÁéØÂ¢ÉÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\nwget https://nodejs.org/dist/v8.12.0/node-v8.12.0-linux-x64.tar.xz xz -d node-v8.12.0-linux-x64.tar.xz tar -xf node-v8.12.0-linux-x64.tar ln -s ~/node-v8.12.0-linux-x64/bin/node /usr/bin/node ln -s ~/node-v8.12.0-linux-x64/bin/npm /usr/bin/npm node -v npm PSÔºö‰ΩøÁî® xz ÂëΩ‰ª§Êó∂ÔºåÊèêÁ§∫Ôºö\n-bash: xz: command not found ÊçÆ‰∫ÜËß£ÔºåÊòØ xz Ëøô‰∏™Ëß£ÂéãÂ∑•ÂÖ∑Â∞öÊú™ÂÆâË£ÖÔºåËß£ÂÜ≥Ê≠•È™§Â¶Ç‰∏ã\n# 1. ‰∏ãËΩΩ xz ÂåÖ wget https://tukaani.org/xz/xz-5.2.4.tar.bz2 # 2. Ëß£ÂéãÂÆâË£ÖÂåÖ tar -jxvf xz-5.2.4.tar.bz2 # 3. ÈÖçÁΩÆ \u0026amp; ÂÆâË£Ö cd xz-5.2.4 mkdir /opt/gnu mkdir /opt/gnu/xz ./configure --prefix=/opt/gnu/xz\t# ÊåáÂÆöÂÆâË£ÖÁõÆÂΩï make \u0026amp;\u0026amp; make install\t# ÁºñËØëÂπ∂ÂÆâË£Ö ln -s /opt/gnu/xz/bin/xz\t# Âª∫Á´ãËΩØÈìæÊé• # 4. Ëß£Âéã xz ÂåÖ xz -d ***.tar.xz # 5. Ëß£Âéã tar ÂåÖ tar -xvf ***.tar Ëøô‰∏≠Èó¥ÂèàÂá∫Áé∞‰∏Ä‰∏™Â∞èÊèíÊõ≤Ôºå./configure --prefix=/opt/gnu/xz ‰∏ÄÊ≠•Êä•ÈîôÔºöconfigure: error: no acceptable C compiler found in $PATH ÔºåÁªèËøáÊü•ËØÅÔºåËß£ÂÜ≥ÊñπÊ≥ï‰∏∫ÂÆâË£Ö GCC ËΩØ‰ª∂Â•ó‰ª∂Ôºö\nyum install gcc ÂÆâË£Ö Ruby Jekyll ‰æùËµñ‰∫é Ruby ÁéØÂ¢ÉÔºåÈúÄË¶ÅÂÆâË£Ö RubyÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Âç≥ÂèØÔºö\nwget https://cache.ruby-lang.org/pub/ruby/2.4/ruby-2.4.4.tar.gz mkdir -p /usr/local/ruby tar -zxvf ruby-2.4.4.tar.gz cd ruby-2.4.4 ./configure --prefix=/usr/local/ruby make \u0026amp;\u0026amp; make install ÁéØÂ¢ÉÂèòÈáèËÆæÁΩÆÔºö\nvi .bash_profile Âú® .bash_profile Êñá‰ª∂ËÆæÁΩÆ‰ª•‰∏ãÂÜÖÂÆπÔºö\nPATH=/usr/local/ruby/bin:$PATH:$HOME/bin export PATH ‰øùÂ≠òÈÄÄÂá∫ÂêéÔºå‰ΩøÁéØÂ¢ÉÂèòÈáèÁîüÊïàÔºö\nsource .bash_profile ÂÆâË£Ö gcc Âú®ÂâçÈù¢Â∑≤ÁªèË∏©ÂùëË£ÖËøá gcc ‰∫ÜÔºåËøôÈáåÈ™åËØÅ‰∏Ä‰∏ãÔºö\nyum -y update gcc yum -y install gcc+ gcc-c++ ÁÑ∂ÂêéÂá∫Áé∞‰∫ÜËØ°ÂºÇÁöÑÊä•ÈîôÔºö-bash: yum: command not found\nÔºüÔºÅÂàöÂàöÊàë‰∏çÊòØÁî®ËøáËøô‰∏™ÂëΩ‰ª§ÂêóÔºåÊÄé‰πàÁé∞Âú®Â∞±Êâæ‰∏çÂà∞‰∫ÜÔºüÊ≤°ÂäûÊ≥ïÔºåÂ∞èÁôΩÂè™Â•ΩÁªßÁª≠ÂéªË∞∑Ê≠åÔºåÊÄÄÁñëÊòØÂàöÂàö‰øÆÊîπÁéØÂ¢ÉÂèòÈáèÂá∫Áé∞ÁöÑÈóÆÈ¢ò„ÄÇËøûÂ∏¶ÁùÄ ls„ÄÅcd Á≠âÂëΩ‰ª§ÈÉΩÁî®‰∏ç‰∫Ü‰∫Ü„ÄÇÊÑüË∞¢‰∏áËÉΩÁöÑ CSDNÔºåËß£ÂÜ≥ÊñπÊ≥ïÔºöÂú®ÂëΩ‰ª§Ë°å‰∏ãËæìÂÖ•Ôºö\nexport PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin ÂÆâË£Ö Jekyll ÊúÄÂêéÂÆâË£Ö JekyllÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\ngem install jekyll jekyll --version gem update --system ÊØ´‰∏çÊÑèÂ§ñÂú∞ÔºåÊä•Èîô -bash: gem: command not found„ÄÇÊâæ‰∫Ü‰∏Ä‰∏ãÔºåÂèëÁé∞Êú¨Âú∞Ê†πÊú¨Ê≤°Êúâ gemÔºö\n[root@host ~]# which gem /usr/bin/which: no gem in (/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin) Â•ΩÂêßÔºåÁúãÊù•Âè™ËÉΩÊâãÂä®ÊëòÂæóËøôÂùó‚ÄúÂÆùÁü≥‚Äù‰∫ÜÔºö\nwget https://rubygems.org/rubygems/rubygems-3.0.2.tgz\t# ‰∏ãËΩΩÊ∫êÁ†Å tar -zxvf rubygems-3.0.2.tgz\t# Ëß£ÂéãÊ∫êÁ†ÅÊñá‰ª∂ cd rubygems-3.0.2 ruby setup.rb\t# ÂÆâË£Ö Ë°åÂêßÔºåÂÆâË£ÖÂÆå‰πãÂêéÔºåËøêË°å gem ÂëΩ‰ª§Ôºö\n[root@host /]# gem install jekyll ERROR: Loading command: install (LoadError) cannot load such file -- zlib ERROR: While executing gem ... (NoMethodError) undefined method `invoke_with_build_args\u0026#39; for nil:NilClass ËøôÊòØÁº∫Â∞ë zlib ‰æùËµñÔºåÈúÄË¶ÅÂÆâË£Ö zlib Â∫ìÔºåÂèØ‰ª•ÈÄöËøá yum ÂÆâË£ÖÔºå‰ΩÜÊòØÂÆâË£Ö‰πãÂêéË¶ÅËÆ∞ÊàêÂà∞ ruby ‰∏≠Ôºö\n# ÂÆâË£Ö zlib Â∫ì yum install zlib-devel # ÈõÜÊàê zlib Â∫ìÂà∞ ruby ÁéØÂ¢É cd root/ruby-2.4.4 cd ext/zlib ruby extconf.rb # Âú®Êìç‰Ωú‰∏ã‰∏ÄÊ≠•‰πãÂâçÈúÄË¶Å‰øÆÊîπ Makefile Êñá‰ª∂‰∏≠ÁöÑ  # zlib.oÔºö$(top_srcdir)/include/ruby.h ÔºàÂ§ßÊ¶ÇÂú®ÂÄíÊï∞Âá†Ë°åÔºâ # Êîπ‰∏∫Ôºö # zlib.o: ../../include/ruby.h # ÔºàËøô‰∏ÄÊ≠•Â¶ÇÊûú‰∏ç‰øÆÊîπÔºåmake Êó∂‰ºöÁàÜÂá∫Âè¶‰∏Ä‰∏™ÈóÆÈ¢òÔºâ make \u0026amp;\u0026amp; make install ËøòÊúâ‰∏Ä‰∏™ÈîôËØØÔºö\n[root@host /]# gem install jekyll ERROR: While executing gem ... (Gem::Exception) Unable to require openssl, install OpenSSL and rebuild Ruby (preferred) or use non-HTTPS sources ËøôÈáåÊèêÁ§∫Áº∫Â∞ë openssl Â∫ìÔºåÂÆâË£ÖÊñπÊ≥ïÁ±ª‰ºº‰∏ä‰∏ÄÊ≠•Ôºö\n# ÂÆâË£Ö OpenSSL Â∫ì yum install openssl-devel # ÈõÜÊàê OpenSSL Â∫ìÂà∞ Ruby cd root/ruby-2.4.4 cd ext/openssl ruby extconf.yb # ÂêåÊ†∑‰øÆÊîπ Makefile ‰∏≠ÁöÑ $(top_srcdir) ‰∏∫ ../.. # Â§ßÊ¶ÇÊï∞ÂçÅÂ§ÑÔºåÊîπÂà∞ÂêêË°Ä==ÔºÅ‰∏çÁü•ÈÅì Vim ÊúâÊ≤°ÊúâÂÖ®Â±ÄÊü•Êâæ/ÊõøÊç¢ÂäüËÉΩ‚Ä¶ make \u0026amp;\u0026amp; make install Áªà‰∫éÔºåÂá∫Áé∞‰∫Ü installing default openssl librariesÔºåÊÑüË∞¢ÂçöÊñá blog.csdn ÁöÑÊåáÂØºÔºåÁªà‰∫éÂèØ‰ª•ÁªßÁª≠ÂÆâË£Ö Jekyll‰∫ÜÔºö\ngem install jekyll jekyll --version gem update --system ÂÆâË£ÖËøáÁ®ãÁï•ÈïøÔºå‰ΩÜÊòØ‰∏ÄÊ∞îÂëµÊàêÔºåËàíÊúçÔºÅ\n[root@host /]# jekyll --version jekyll 3.8.5 [root@host /]# gem update --system Latest version already installed. Done. ÁºñËØëÂçöÂÆ¢ ÈúÄË¶Å git Â∑•ÂÖ∑Ôºå‰∏ãËΩΩÂú® github ‰∏äÁöÑ‰ª£Á†ÅÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\ngit clone https://github.com/Fang-Lansheng/my-thistledown.git cd my-thistledown jekyll server ÈÉ®ÁΩ≤Âà∞ Nginx ÊúçÂä°Âô®‰∏ä ÈÄöËøá Jekyll ÁºñËØëÂêéÁöÑÈùôÊÄÅÊñá‰ª∂ÈúÄË¶ÅÊåÇËΩΩÂà∞ Nginx ÊúçÂä°Âô®ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÆâË£Ö Nginx ÊúçÂä°Âô®„ÄÇÂÆâË£ÖËøáÁ®ãÂèÇÁÖßÂÆòÊñπÊïôÁ®ãÔºö\ncd /etc/yum.repos.d/ # Êñ∞Âª∫Êñá‰ª∂ vi nginx.repo Âú®Êñá‰ª∂‰∏≠ÁºñËæë‰ª•‰∏ãÂÜÖÂÆπÔºåÂπ∂‰øùÂ≠òÔºö\n[nginx] name=nginx repo baseurl=http://nginx.org/packages/mainline/centos/6/$basearch/ gpgcheck=0 enabled=1 ÁÑ∂ÂêéÊâßË°åÂÆâË£Ö Nginx ÂëΩ‰ª§ÔºåÂ¶Ç‰∏ãÔºö\nyum install nginx ÊûúÁÑ∂‰∏çÂ§±ÊâÄÊúõÔºåÂÆâË£ÖËøáÁ®ã‰∏≠ÔºåÊâæ‰∏çÂà∞ÂåÖ‰∫ÜÔºåÂú®ÁΩë‰∏äÁöÑÊïôÁ®ã‰∏≠ËΩ¨‰∫ÜÂèàËΩ¨ÔºåÊï≤‰∫ÜÂá†Ë°åÁ´üÁÑ∂ÂèàÊàê‰∫ÜÔºö\nyum clean all yum makecache yum -y install nginx Â§ßÂäüÂëäÊàêÔºÅ\nNginx ÈÖçÁΩÆÊàêÂäüÂêéÔºåÈúÄË¶ÅËÆæÁΩÆ Nginx ÁöÑÈÖçÁΩÆÔºåÈÖçÁΩÆË∑ØÂæÑ‰∏∫ /etc/nginx/conf.d/default.confÔºåÈÖçÁΩÆÂÜÖÂÆπÂ¶Ç‰∏ãÔºö\nserver { listen 80; server_name localhost my-thistledown.com www.my-thistledown.com;\t# Èùû www ÂüüÂêçÁöÑÈáçÂÆöÂêëÂà∞ www if ( $host != \u0026#34;www.my-thistledown.com\u0026#34; ) { rewrite \u0026#34;^/(.*)$\u0026#34; http://www.my-thistledown.com/$1 permanent; } #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache\u0026#39;s document root # concurs with nginx\u0026#39;s one # #location ~ /\\.ht { # deny all; #} } ÂÆâË£Ö Nginx ÊúçÂä°ÊàêÂäüÂêéÔºåÂ∞Ü Jekyll ÁºñËØëÁöÑÂçöÂÆ¢ÈùôÊÄÅ html Êñá‰ª∂ËæìÂá∫Âà∞ Nginx ÊúçÂä°Âô®‰∏äÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\njekyll build --destination=/root/blog/html ÂêØÂä® Nginx ÊúçÂä°Âô®ÔºåÂ∞±ÂèØ‰ª•Ê≠£Â∏∏ËÆøÈóÆÂçöÂÆ¢ÁΩëÈ°µ„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂú®ÊµèËßàÂô®‰∏äËÆøÈóÆÔºåÈúÄË¶ÅÂú®ÈòøÈáå‰∫ë ECS ÊéßÂà∂Âè∞ÁöÑÂÆâÂÖ®ÁªÑ‰ª∂Êö¥Èú≤ 80 Á´ØÂè£„ÄÇÂ¶ÇÊûúÊÉ≥ÈÄöËøáÂüüÂêçËÆøÈóÆÔºåÈúÄË¶ÅÂ∞ÜÂüüÂêçËß£ÊûêËÆæÁΩÆÊåáÂêë‰Ω†ÁöÑÊúçÂä°Âô®Âú∞ÂùÄ„ÄÇ\nÊØ´Êó†ÁñëÈóÆÔºåÊ≤°ÊúâÊàêÂäü‚Ä¶‚Ä¶ÂèØËÉΩÊòØÁî® Jekyll ËæìÂá∫Êñá‰ª∂Âà∞ Nginx ÊúçÂä°Âô®‰∏äÂá∫Èîô‰∫ÜÂêßÔºåÊ≤°ÂÖ≥Á≥ªÔºåËøôÈáå‰øÆÊîπ‰∏Ä‰∏ãÈÖçÁΩÆÊñá‰ª∂Ôºö\nvi /etc/nginx/nginx.conf ÊâìÂºÄÈÖçÁΩÆÊñá‰ª∂ÔºåÊòæÁ§∫Ôºö\nuser nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; } http Êã¨Âè∑‰∏ãÊúâ include /etc/nginx/conf.d/*.conf ÔºåÂõ†Ê≠§Êàë‰ª¨Ëá™ÂÆö‰πâÁöÑÈÖçÁΩÆÊñá‰ª∂ÊòØÊàêÂäüÂØºÂÖ•ÁöÑÔºö\nvi /etc/nginx/conf.d/default.conf ÈáçÊñ∞‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÔºåÈùôÊÄÅÊñá‰ª∂ÁõÆÂΩï‰∏∫Êàë‰ª¨ÁΩëÈ°µÊñá‰ª∂ÊâÄÂú®ÁöÑÁõÆÂΩïÔºö\n# location / {} ‰∏≠‰øÆÊîπËøô‰∏§Ë°åÔºö root /root/myblog/my-thistledown; index index.html; ÁÑ∂ÂêéÂ∞Ü nginx.conf ‰∏≠Á¨¨‰∏ÄË°å user nginx ‰øÆÊîπ‰∏∫Ôºö\nuser root;\t# ËøôÈáåÂèØËÉΩÂ≠òÂú®ÊùÉÈôêÈóÆÈ¢ò ÈáçÂêØÊúçÂä°ÔºåÊàêÂäüËøêË°å„ÄÇÂú®ÊµèËßàÂô®‰∏≠ËæìÂÖ•ÂüüÂêçÔºåÂá∫Áé∞‰∫Ü index.html ÁöÑÂÜÖÂÆπ„ÄÇ\nÂêØÂä® Nginx ÊúçÂä°Âô®\nnginx -c nginx.conf Â¶ÇÊûúÈÅáÂà∞ nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) ËøôÊ†∑ÁöÑÈîôËØØÔºåÂèØËÉΩÊòØÁ´ØÂè£Ë¢´Âç†Áî®„ÄÇ\n# ÊñπÊ≥ï‰∏Ä # Linux ÈÄöËøáÁ´ØÂè£Êü•ÁúãËøõÁ®ã netstat -nap | grep Á´ØÂè£Âè∑ # ÊñπÊ≥ï‰∫å # ÂÖàÊü•ÁúãËøõÁ®ã PID ps -ef | grep ËøõÁ®ãÂêç # ÈÄöËøá PID Êü•ÁúãÂç†Áî®Á´ØÂè£ netstat -nap | grep ËøõÁ®ãPID ÂÅúÊ≠¢ Nginx ÊúçÂä°Âô®\nps -ef | grep nginx # ‰ªéÂÆπÂÅúÊ≠¢ Nginx kill -QUIT ‰∏ªËøõÁ®ãÂè∑ # Âø´ÈÄüÂÅúÊ≠¢ Nginx kill -TERM ‰∏ªËøõÁ®ãÂè∑ # Âº∫Âà∂ÂÅúÊ≠¢ Nginx pkill -9 nginx ÈáçÂêØ Nginx ÊúçÂä°Âô®\nsudo nginx -s reload ËÆøÈóÆÈ¶ñÈ°µÊó∂Áõ¥Êé•ÊòæÁ§∫ÂüüÂêçÔºå‰∏çÊòæÁ§∫ ***/index.html ËøôÁÆóÊòØ‰∏Ä‰∏™Â∞èÊèíÊõ≤ÔºåÊàëÂèëÁé∞ËÆøÈóÆÊàëÁöÑÈ¶ñÈ°µ http://www.my-thisdown.com Êó∂Âπ∂‰∏çÊòØÊòæÁ§∫index.htmlÔºåËÄåËÆøÈóÆ http://www.my-thisdown.com/index.html Êó∂ÊâçË∑≥ËΩ¨Âà∞ÊàëÁöÑ‰∏ªÈ°µ„ÄÇËøôÊòæÁÑ∂‰∏é‰∏çÂ§™Á¨¶ÂêàÈ¢ÑÊúüÔºåÂú®ÁΩëÁªú‰∏äÊü•Êâæ‰∏ÄÁï™ÔºåÂèëÁé∞Êúâ‰ª•‰∏ãËß£ÂÜ≥ÊñπÊ≥ïÔºö\nÈ¶ñÂÖàÔºåËøòÊòØÊâìÂºÄÈÖçÁΩÆÊñá‰ª∂Ôºö\nvi /etc/nginx/conf.d/default.conf Âú®ÂÖ∂‰∏≠Ê∑ªÂä†‰ª•‰∏ãÂá†Ë°å‰ª£Á†ÅÂç≥ÂèØÔºö\nlocation / { root /root/myblog/my-thistledown; index index.html index.htm; # ‰ª•‰∏ã‰∏∫Êñ∞Ê∑ªÂä†ÂÜÖÂÆπ # Ê≠§ if ‰∏∫Âà§Êñ≠ÊòØÂê¶ËÆøÈóÆ‰∫Ü root ÁõÆÂΩïÔºå‰∏∫ÊòØÊó∂ÔºåË∑≥ËΩ¨Âà∞ index ÈÖçÁΩÆÈ°πÔºåÂç≥ËÆøÈóÆÂà∞ index.html if (!-e $request_filename) { proxy_pass http://index; } } Ëá™Âä®ÂåñÈÉ®ÁΩ≤ ÈÄöËøáËÆæÁΩÆ GitHub ÁöÑ Webhooks ÂèØ‰ª•ÂÆûÁé∞Ëá™Âä®ÂåñÊûÑÂª∫ÂíåÈÉ®ÁΩ≤„ÄÇ\nËøáÁ®ã‰∏∫ÔºöÊèê‰∫§ÂçöÊñáÊàñËÄÖÈÖçÁΩÆÂà∞ GitHub ‰ªìÂ∫ìÔºå‰ªìÂ∫ì‰ºöËß¶Âèë‰Ω†ËÆæÁΩÆÁöÑ WebhookÔºå‰ºöÂêë‰Ω†ËÆæÁΩÆÁöÑ Webhooks Âú∞ÂùÄÂèëÈÄÅ‰∏Ä‰∏™ post ËØ∑Ê±ÇÔºåÊØîÂ¶ÇÊàëËÆæÁΩÆÁöÑËØ∑Ê±ÇÊòØÂú®ÊúçÂä°Âô®Ë∑ëÁöÑ‰∏Ä‰∏™ Node.js Á®ãÂ∫èÔºåÁõëÂê¨ GitHub Webhooks ÁöÑËØ∑Ê±ÇÔºåÊé•Êî∂Âà∞ËØ∑Ê±ÇÂêéÔºå‰ºöÊâßË°å shell ÂëΩ‰ª§„ÄÇ\nÈ¶ñÂÖàËÆæÁΩÆ GitHub ÁöÑ WebhooksÔºåÂú® GitHub ‰ªìÂ∫ìÁöÑÈ°πÁõÆÁïåÈù¢ÔºåÁÇπÂáª Settings‚ÜíWebhooks‚ÜíAdd webhookÔºåÊ∑ªÂä†ÈÖçÁΩÆ‰ø°ÊÅØÔºåÁ§∫‰æãÂ¶Ç‰∏ãÔºö\nÁÇπÂáª Add webhook ÊåâÈíÆÔºåÂ∞±ËÆæÁΩÆÊàêÂäü‰∫Ü„ÄÇ\nÁé∞Âú®ÂéªÊúçÂä°Âô®Á´ØÁõëÂê¨ GItHub Webhook ÂèëÈÄÅÁöÑËØ∑Ê±ÇÔºåÊàë‰πüË∑üÈöèÊïôÁ®ãÔºåÈááÁî®‰∫ÜÂºÄÊ∫êÁªÑ‰ª∂ github-webhook-handler ÂéªÁõëÂê¨„ÄÇÈ¶ñÂÖàÂÆâË£ÖÔºö\nnpm i github-webhook-handler ÂÆâË£ÖÊàêÂäüÂêéÔºåÂú® /root/node-v8.12.0-linux-x64/lib/node_modules/github-webhook-handler Êñá‰ª∂Â§π‰∏ãÊñ∞Âª∫ deploy.js Êñá‰ª∂Ôºö\nvar http = require(\u0026#39;http\u0026#39;); var createHandler = require(\u0026#39;github-webhook-handler\u0026#39;); var handler = createHandler({ path: \u0026#39;/webhook\u0026#39;, secret: \u0026#39;******\u0026#39; }); function run_cmd(cmd, args, callback) { var spawn = require(\u0026#39;child_process\u0026#39;).spawn; var child = spawn(cmd, args); var resp = \u0026#34;\u0026#34;; child.stdout.on(\u0026#39;data\u0026#39;, function(buffer) { resp += buffer.toString(); }); child.stdout.on(\u0026#39;end\u0026#39;, function() { callback (resp) }); } http.createServer(function (req, res) { handler(req, res, function (err) { res.statusCode = 404 res.end(\u0026#39;no such location\u0026#39;) }) }).listen(3001) handler.on(\u0026#39;error\u0026#39;, function (err) { console.error(\u0026#39;Error:\u0026#39;, err.message) }) handler.on(\u0026#39;push\u0026#39;, function (event) { console.log(\u0026#39;Received a push event for %s to %s\u0026#39;, event.payload.repository.name, event.payload.ref); run_cmd(\u0026#39;sh\u0026#39;, [\u0026#39;./start_blog.sh\u0026#39;], function(text){ console.log(text) }); }) ‰∏äËø∞‰ª£Á†Å‰∏≠ÔºåÂà∂ÂÆö‰∫Ü Node.js ÊúçÂä°ÁöÑÁ´ØÂè£‰∏∫ 3001ÔºåÁõëÂê¨‰∫Ü path/webhookÔºåsecret ‰∏∫ ******ÔºåËøôÂíå‰πãÂâçÂú® GitHub Webhook ‰∏≠ÁöÑËÆæÁΩÆÈúÄË¶Å‰∏ÄËá¥„ÄÇ\n‰ª£Á†Å run_cmd('sh', ['./start_blog.sh']Ôºå¬∑¬∑¬∑)ÔºåÊåáÂÆö‰∫ÜÊé•Êî∂Âà∞ËØ∑Ê±ÇÂêéÊâßË°å ./start_blog.shÔºåÂÖ∂‰ª£Á†ÅÂ¶Ç‰∏ãÔºö\necho `date` cd /root/myblog/my-thistledown echo start pull from github git pull echo start build.. jekyll build --destination=/usr/share/nginx/html ÁÑ∂ÂêéÈúÄË¶Å‰ΩøÁî® forover Êù•ÂêØÂä® deploy.js ÁöÑÊúçÂä°ÔºåÊâßË°åÂëΩ‰ª§Â¶Ç‰∏ãÔºö\nsudo npm install forever -g #ÂÆâË£Ö forever start deploy.js #ÂêØÂä® forever stop deploy.js #ÂÖ≥Èó≠ forever start -l forever.log -o out.log -e err.log deploy.js #ËæìÂá∫Êó•ÂøóÂíåÈîôËØØ ÊúÄÂêé‰∏ÄÊ≠•ÔºåÈúÄË¶ÅÂú® Nginx ÊúçÂä°Âô®ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂ∞ÜÁõëÂê¨ÁöÑ /webhook ËØ∑Ê±ÇËΩ¨ÂèëÂà∞ Node.js ÊúçÂä°‰∏äÔºåÈÖçÁΩÆ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö\nlocation = /webhook { proxy_pass http://127.0.0.1:3001/webhook; } ËøôÊ†∑ÔºåÂΩì‰Ω†Êèê‰∫§‰∫ÜÊñáÁ´†ÊàñËÄÖ‰øÆÊîπÁöÑÈÖçÁΩÆÂà∞ GItHub ‰∏äÔºåGitHub ÈÄöËøá webhook Âêë‰Ω†ÊâÄÂú®ÁöÑÊúçÂä°Âô®ÂèëÈÄÅËØ∑Ê±ÇÔºåÊúçÂä°Âô®Êé•Êî∂Âà∞ËØ∑Ê±ÇÂêéÊâßË°å sh ÂëΩ‰ª§Ôºåsh ÂëΩ‰ª§ÂåÖÊã¨‰∫ÜÈáçÊñ∞ pull ‰ª£Á†ÅÂíåÁºñËØë‰ª£Á†ÅÁöÑËøáÁ®ãÔºåËøôÊ†∑Ëá™Âä®ÂåñÈÉ®ÁΩ≤Â∞±ÂÆåÊàê‰∫ÜÔºå‰Ω†Âè™ÈúÄÊèê‰∫§‰ª£Á†ÅÔºåÊúçÂä°Âô®Â∞±Ëß¶Âèë pull ‰ª£Á†ÅÂíåÈáçÊñ∞ÁºñËØëÁöÑÂä®‰Ωú„ÄÇ\nÁî®ÈòøÈáå‰∫ëÁöÑÂÖçË¥π SSL ËØÅ‰π¶ËÆ©ÁΩëÁ´ô‰ªé HTTP Êç¢Êàê HTTPS  Êú¨ÊñáÂèÇËÄÉÔºöhttps://ninghao.net/blog/4449\n Áî≥ËØ∑ËØÅ‰π¶\n ÁôªÂΩïÔºöÈòøÈáå‰∫ëÊéßÂà∂Âè∞‚Üí‰∫ßÂìÅ‰∏éÊúçÂä°‚ÜíËØÅ‰π¶ÊúçÂä°‚ÜíË¥≠‰π∞ËØÅ‰π¶ Ë¥≠‰π∞ÔºöËØÅ‰π¶Á±ªÂûãÈÄâÊã©‚ÜíÂÖçË¥πÂûãDV SSLÔºåÁÑ∂ÂêéÂÆåÊàêË¥≠‰π∞ Ë°•ÂÖ®ÔºöÂú® ÊàëÁöÑËØÅ‰π¶ ÊéßÂà∂Âè∞ÔºåÊâæÂà∞Ë¥≠‰π∞ÁöÑËØÅ‰π¶ÔºåÂú®Êìç‰ΩúÊ†èÈáåÈÄâÊã©Ë°•ÂÖ®ÔºåÂ°´ÂÜôËØÅ‰π¶Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ ÂüüÂêçÈ™åËØÅÔºöÂèØ‰ª•ÈÄâÊã© DNSÔºåÂ¶ÇÊûúÂüüÂêçÁî®‰∫ÜÈòøÈáå‰∫ëÁöÑ DNS ÊúçÂä°ÔºåÂÜçÂãæÈÄâ‰∏Ä‰∏ã ËØÅ‰π¶ÁªëÂÆöÁöÑÂüüÂêçÂú® ÈòøÈáå‰∫ëÁöÑ‰∫ëËß£Êûê ‰∏ä‰º†ÔºöÁ≥ªÁªüÁîüÊàê CSRÔºåÁÇπ‰∏Ä‰∏ãÂàõÂª∫ Êèê‰∫§ÂÆ°Ê†∏  ‰∏ãËΩΩËØÅ‰π¶\nËØÅ‰π¶ÂÆ°Ê†∏ÈÄöËøáÂêéÔºåÂç≥ÂèØ‰∏ãËΩΩ„ÄÇÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÊúçÂä°Âô®Á±ªÂûãÈÄâÊã©ËØÅ‰π¶‰∏ãËΩΩÔºåÊîØÊåÅ Tomcat„ÄÅApache„ÄÅNginx„ÄÅIIS Á≠â„ÄÇ‰∏ãËΩΩÂØπÂ∫îÁöÑËØÅ‰π¶ÔºåËß£ÂéãÂêéÂæóÂà∞‰∏§‰∏™Êñá‰ª∂Ôºå‰∏Ä‰∏™ÊòØ *.keyÔºå‰∏Ä‰∏™ÊòØ *.pem„ÄÇ\nÈÖçÁΩÆ Nginx ÁöÑ HTTPS\nÊúâ‰∫ÜËØÅ‰π¶ÔºåÂ∞±ÂèØ‰ª•ÂéªÊúçÂä°Âô®‰ΩøÁî®‰∫Ü„ÄÇ\nÁ¨¨‰∏ÄÊ≠•Ôºö‰∏ä‰º†ËØÅ‰π¶\nÂàõÂª∫‰∏Ä‰∏™Â≠òÂÇ®ËØÅ‰π¶ÁöÑÁõÆÂΩïÔºö\nmkdir /etc/nginx/ssl/my-thistledown.com ÊääÁî≥ËØ∑Âπ∂‰∏ãËΩΩ‰∏ãÊù•ÁöÑËØÅ‰π¶Ôºå‰∏ä‰º†Âà∞‰∏äÈù¢ÂàõÂª∫ÁöÑÁõÆÂΩï‰∏≠ÔºåËøôÈáåÊàë‰ΩøÁî®ÁöÑÊòØ Xftp ËøõË°åÁöÑÊñá‰ª∂‰º†Ëæì„ÄÇËØÅ‰π¶ÁöÑÂÆûÈôÖ‰ΩçÁΩÆ‰∏∫Ôºö\n/etc/nginx/ssl/my-thistledown.com/1692217_my-thistledown.com.key /etc/nginx/ssl/my-thistledown.com/1692217_my-thistledown.com.pem Á¨¨‰∫åÊ≠•ÔºöNginx ÈÖçÁΩÆÊñá‰ª∂\n‰Ω†ÁöÑÁΩëÁ´ôÂèØ‰ª•ÂêåÊó∂ÊîØÊåÅ HTTP ‰∏é HTTPSÔºåHTTP ÁöÑÈªòËÆ§Á´ØÂè£Âè∑ÊòØ 80ÔºåHTTPS ÁöÑÈªòËÆ§Á´ØÂè£Âè∑ÊòØ 443„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂ¶ÇÊûú‰Ω†ÁöÑÁΩëÁ´ôË¶Å‰ΩøÁî® HTTPSÔºå‰Ω†ÈúÄË¶ÅÈÖçÁΩÆÁΩëÁ´ôÊúçÂä°Âô®ÔºåËÆ©ÂÖ∂ÁõëÂê¨ 443 Á´ØÂè£Ôºå‰πüÂ∞±ÊòØÁî®Êà∑‰ΩøÁî® HTTPS ÂèëÂá∫ÁöÑËØ∑Ê±Ç„ÄÇ\nÊñ∞Â¢ûÈÖçÁΩÆÊñá‰ª∂Ôºö\nvi /etc/nginx/conf.d/https.conf Â¢ûÂä†Â¶Ç‰∏ãÈÖçÁΩÆÔºö\nserver { listen 443;\t# ÈªòËÆ§ 443 Á´ØÂè£ server_name my-thistledown.com; ssl on; root /usr/share/nginx/html; index index.html; ssl_certificate /etc/nginx/ssl/my-thistledown.com/1692217_my-thistledown.com.pem; ssl_certificate_key /etc/nginx/ssl/my-thistledown.com/1692217_my-thistledown.com.key; ssl_session_timeout 5m;\t# ÂÆ¢Êà∑Á´ØÂèØ‰ª•ÈáçÁî®‰ºöËØùÂèÇÊï∞ÁöÑÊó∂Èó¥ ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\t# ‰ΩøÁî®ÁöÑÂçèËÆÆ ssl_ciphers AESGCM:ALL:!DH:!EXPORT:!RC4:+HIGH:!MEDIUM:!LOW:!aNULL:!eNULL;\t# ÈÖçÂ•óÂä†ÂØÜÂ•ó‰ª∂ ssl_prefer_server_ciphers on; } ÁÑ∂ÂêéÈáçÊñ∞Âä†ËΩΩ Nginx ÊúçÂä°Ôºö\nsudo service nginx reload # Êàñ sudo systemctl reload nginx # Êàñ nginx -s reload ÈÅáÂà∞ÁöÑÈóÆÈ¢ò\nÈÖçÁΩÆÊàêÂäüÂêéÔºåÊó†Ê≥ïÊàêÂäüËÆøÈóÆÔºåÊâßË°å nginx -t Ê£ÄÊü•Ôºö\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful Ê≤°ÊúâÂá∫ÈîôÔºåÁúãÊù•ÈóÆÈ¢òÂá∫Âú®ÂÖ∂‰ªñÂú∞Êñπ„ÄÇ\nCentos 6 ÁöÑÈò≤ÁÅ´Â¢ôÈÖçÁΩÆÊúâÈóÆÈ¢òÔºö\n# ÈÖçÁΩÆ iptables Á´ØÂè£ vi /etc/sysconfig/iptables Ê∑ªÂä†‰∏ãÈù¢Âá†Ë°å\n-A INPUT -p tcp -m state --state NEW -m tcp --dport 21 -j ACCEPT\t-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 8080 -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT # Âú® COMMIT ‰∏ÄË°å‰πãÂâç  21Á´ØÂè£ÔºöÈªòËÆ§Ôºå‰æõ ssh ËÆøÈóÆ 80„ÄÅ8080 Á´ØÂè£ÔºöHTTP ÊúçÂä°ËÆøÈóÆ 445 Á´ØÂè£ÔºöHTTPS ÊúçÂä°ËÆøÈóÆÔºàÈªòËÆ§ 443Ôºâ  ÁÑ∂ÂêéÈáçÂêØ iptables ÊúçÂä°Ôºö\nservice iptables restart\t# ÈáçÂêØ iptables ÊúçÂä° service iptables enable\t# ËÆæÁΩÆ iptables ÊúçÂä°ÂºÄÊú∫ÂêØÂä® service iptables status\t# Êü•ËØ¢Èò≤ÁÅ´Â¢ôÁä∂ÊÄÅ ÂÜç‰øÆÊîπ‰∫Ü‰∏ãÈÖçÁΩÆÊñá‰ª∂ÔºåËøô‰∏ãÂ∫îËØ•ÊòØÊ≤°ÈóÆÈ¢ò‰∫ÜÔºö\nserver { listen 80 ; server_name _ localhost my-thistledown.com www.my-thistledown.com ; location = /webhook { proxy_pass http://127.0.0.1:3001/webhook; } # return 301 https://www.my-thistledown.com$request_uri; } server { listen 443; server_name my-thistledown.com; return 301 https://www.my-thistledown.com$request_uri; } server { listen 443 default_server ssl; server_name www.my-thistledown.com; ssl on; ssl_certificate /etc/nginx/ssl/my-thistledown.com/1692217_my-thistledown.com.pem; ssl_certificate_key /etc/nginx/ssl/my-thistledown.com/1692217_my-thistledown.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers AESGCM:ALL:!DH:!EXPORT:!RC4:+HIGH:!MEDIUM:!LOW:!aNULL:!eNULL; ssl_prefer_server_ciphers on; root /usr/share/nginx/html; index index.html index.htm; location / { root /usr/share/nginx/html; index index.html; } error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } ÂêéËÆ∞ ÂçöÂÆ¢ÁöÑÂü∫Êú¨ÈÖçÁΩÆÂ∑≤ÁªèÁÆÄÂçïÂÆåÊàê„ÄÇÊé•‰∏ãÊù•ÂºïÁî®‰∏Ä‰∫õ‰ºòÁßÄÁöÑ Jekyll Ê®°ÊùøÔºåÂÜç‰∏ÄÁØáÁØáÂú∞Ê∑ªÂä†ÂçöÊñáÔºå‰∏Ä‰∏™Á≤æÂ∑ßÁöÑÂçöÂÆ¢‰πüÂ∞±ÂèØ‰ª•ÊÖ¢ÊÖ¢ÊàêÂûã‰∫Ü„ÄÇ\n‰∫ãÊÉÖËøòÊ≤°Âëä‰∏ÄÊÆµËêΩÔºåÊàëÂèà‰∏çËá™‰∏ªÂú∞ËÄÉÈáèÔºöËøêËê•ÂçöÂÆ¢‰ºöËä±Ë¥πÊàëÂ§öÂ∞ëÊó∂Èó¥ÔºåÊàëËÉΩÂÜôÂá∫‰ªÄ‰πàÊ†∑ÁöÑÂÜÖÂÆπÔºå‰ª•ÂèäÔºåÊàë‰∏∫‰ªÄ‰πàË¶ÅÂÜôÂçöÂÆ¢Ôºü\nÈïø‰πÖ‰ª•Êù•ÔºåÊàëÈÉΩÁº∫‰πèËæìÂÖ•ÔºåÁé∞Âú®ÁöÑÊàëÂ∑≤ÁªèÂæàÈöæËØªÂÆå‰∏ÄÊú¨‰π¶„ÄÅÊéåÊè°‰∏ÄÈ°πÁâπÈïø‰πÉËá≥ÂùöÊåÅÊüê‰∏™‰π†ÊÉØ„ÄÇ‰∏ÄÁØáÂçöÊñáÔºåÈô§Âç¥‰∏úÊãºË•øÂáë„ÄÅÂØªÁ´†ÊëòÂè•ÁöÑÈÉ®ÂàÜÔºåÊúâÂ§öÂ∞ëÂÜÖÂÆπÊ∫êËá™Ëá™Â∑±ÁöÑÊÄùËÄÉÔºüÈîÆÂ∏ΩÂäàÂï™‰ΩúÂìçÔºåÂÇ¨ÊàëËµ∂Âø´Êå§Âá∫ÊúÄÂêéÂá†Ë°åÊñáÂ≠ó„ÄÇÁÑ¶ËôëÁöÑÂà∫ÈÄºÁùÄÊàë‰∏ÄÂ§¥ÊâéËøõËøôËå´Ëå´Ëø∑ÈõæÔºåÁõ¥Ëá≥Â¶Ç‰ªä‰æùÁÑ∂ÊôïÂ§¥ËΩ¨Âêë„ÄÇ\nÊàë‰ªé‰∏çÊãíÁªùÊñ∞‰∫ãÁâ©ÔºåÊØîÂ¶ÇÂÜô‰ΩúÔºåËøôÁßçÊñ∞Â•áÊÑü‰ºöÊöÇÊó∂ÂÆâÊäöÊàëÁöÑÁÉ¶Ë∫Å„ÄÇÊàëÊõæÁªèÂäüÂà©Âú∞ÁÆóËÆ°ÔºåÂßãÁªàÊ≤°Áê¢Á£®Âá∫Êç∑ÂæÑÊòØÂì™‰∏™ÊñπÂêëÔºåËøôËÆ©Êàë‰∏áÂàÜÊ≤Æ‰∏ß„ÄÇÂ¶ÇËã•Ê≤°ÊúâÂØπÁîüÊ¥ªÁöÑÂêëÂæÄÔºå‰πü‰∏çËá≥Ê≠§Á≠âÂøÉÁÉ¶ÊÑè‰π±„ÄÇÊàñËÆ∏ÊñáÂ≠óÁöÑÂàÜÈáèÔºåË∂≥‰ª•ËÆ©ÊàëÊ≤â‰∏ãÂøÉÊù•Âêß„ÄÇ\n","permalink":"https://fang-lansheng.github.io/posts/2019-01-03-hello-my-blog/","summary":"ÂâçË®Ä ‰∏ä‰∏Ä‰∏™Âü∫‰∫é WordPress ÁöÑÂçöÂÆ¢Â∑≤ÁªèÂú®ÈáçË£ÖÊúçÂä°Âô®Á≥ªÁªü‰πãÂêéÁÉüÊ∂à‰∫ëÊï£‰∫ÜÔºåÂõ†‰∏∫Êê≠Âª∫ËøáÁ®ãÂÖ®Á®ãÁôæÂ∫¶ÔºåËá™Â∑±ÊØ´Êó†Âç∞Ë±°Ôºå‰πüÂ∞±‰∏çÊÑøÊÑèÂÜçÈáçÂ§ç‰∏ÄÈÅçÊú∫Ê¢∞ÂºèÂú∞ÊäÑÂÜô„ÄÇËøô‰∏ÄÂõûÊâìÁÆóÂèÇËÄÉÁΩë‰∏äÁöÑÊïôÁ®ãÔºåÈáçÊñ∞Êê≠Âª∫‰∏Ä‰∏™Ëá™Â∑± blogÔºåÂπ∂‰∏îÊääËøáÁ®ãËÆ∞ÂΩï‰∏ãÊù•ÔºåÊùÉÂΩìÂÅö‰∏Ä‰∏™Â≠¶‰π†ÁöÑËøáÁ®ãÂêß„ÄÇ\nÊê≠Âª∫ÊïôÁ®ã‰∏ªË¶ÅÂèÇËÄÉÔºö\n Á®ãÂ∫èÂëòÂ¶Ç‰ΩïÊê≠Âª∫Ëá™Â∑±ÁöÑ‰∏™‰∫∫ÂçöÂÆ¢ - ÊñπÂøóÊúãÁöÑÂçöÂÆ¢\nÊÑüË∞¢ @ÊñπÂøóÊúã Âú®ÂçöÂÆ¢Êê≠Âª∫ËøáÁ®ã‰∏≠ÁöÑÊåáÂØº\n ‰ΩøÁî®Ëá™Â∑±ÁöÑÊúçÂä°Âô®ÈÉ®ÁΩ≤ÂçöÂÆ¢ ÂèÇËÄÉÊïôÁ®ãÔºåÊàë‰πüÊòØ‰ΩøÁî® Jekyll Êê≠Âª∫ÂçöÂÆ¢„ÄÇÊúçÂä°Âô®Á≥ªÁªü‰∏∫ Centos 6 x86_64 bbr \nVPS ÂèäÂüüÂêçË¥≠‰π∞ VPS ÊòØ‰π∞ÁöÑÊê¨Áì¶Â∑•Ôºå‰∏ÄÂπ¥‰∏çÂà∞ $20ÔºåÁõ∏ÂΩìÂàíÁÆó„ÄÇÂüüÂêçÂàôÊòØÊù•Ëá™ÈòøÈáå‰∫ë„ÄÇËøô‰∏§ËÄÖÂ∑≤ÁªèÊòØÂæà‰πÖ‰πãÂâçË¥≠‰π∞ÁöÑ‰∫ÜÔºåÁΩëÁªú‰∏äÁõ∏ÂÖ≥ÊïôÁ®ãÂèä‰ªãÁªç‰πüÈùûÂ∏∏ËØ¶Â∞ΩÔºåÊïÖ‰∏çÂú®Ê≠§ËµòËø∞„ÄÇ\nËøûÊé•Âà∞ÊúçÂä°Âô® ËøôÈáåÊàëÊòØÁî®ÁöÑÊòØ XshellÔºåPutty ‰πü‰∏çÈîô„ÄÇÂõ†‰∏∫‰∏ç‰∫ÜËß£ÂéüÁêÜÔºåÊâÄ‰ª•‰πü‰∏çÂ§öËÆ≤ÔºåÂÆûÈôÖÊìç‰Ωú‰∏≠ÂñÑÁî®ÊêúÁ¥¢ÂºïÊìéÂü∫Êú¨Â∞±ËÉΩÊéåÊè°Êìç‰Ωú„ÄÇ\nÂÆâË£Ö Node ‰ΩøÁî® Xshell ËøûÊé•Âà∞ÊúçÂä°Âô®‰πãÂêé„ÄÇÂÆâË£Ö Node ÁéØÂ¢ÉÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\nwget https://nodejs.org/dist/v8.12.0/node-v8.12.0-linux-x64.tar.xz xz -d node-v8.12.0-linux-x64.tar.xz tar -xf node-v8.12.0-linux-x64.tar ln -s ~/node-v8.12.0-linux-x64/bin/node /usr/bin/node ln -s ~/node-v8.12.0-linux-x64/bin/npm /usr/bin/npm node -v npm PSÔºö‰ΩøÁî® xz ÂëΩ‰ª§Êó∂ÔºåÊèêÁ§∫Ôºö\n-bash: xz: command not found ÊçÆ‰∫ÜËß£ÔºåÊòØ xz Ëøô‰∏™Ëß£ÂéãÂ∑•ÂÖ∑Â∞öÊú™ÂÆâË£ÖÔºåËß£ÂÜ≥Ê≠•È™§Â¶Ç‰∏ã\n# 1.","title":"Hello, Blog!"}]