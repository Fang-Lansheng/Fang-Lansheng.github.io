<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ML/DL on Jifan&#39;s Blog</title>
    <link>https://jifan.tech/tags/ml/dl/</link>
    <description>Recent content in ML/DL on Jifan&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://jifan.tech/tags/ml/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>机器学习基石-数学篇：Why Can Machines Learn?</title>
      <link>https://jifan.tech/posts/2020-07-02-mlf-mf-ntu-2/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-07-02-mlf-mf-ntu-2/</guid>
      <description>Course Link ：
 Week 5 - Training versus Testing Week 6 - Theory of Generalization Week 7 - The VC Dimension Week 8 - Noise and Error   5 Training versus Testing  What we pay in choosing hypotheses during training: the growth function for representing effective number of choices &amp;hellip;
 5.1 Recap and Preview   Recap: the statistical learning flow
 if $\mathcal{H} = M$ finite, $N$ large enough  for whatever $g$ picked by $\cal A$, $E_{out}(g) \approx E_{in}(g)$   if $\cal A$ finds one $g$ with $E_{in}(g) \approx 0$  PAC guarantee for $E_{out}(g) \approx 0$ $\Rightarrow$ learning possible      Two central questions</description>
    </item>
    
    <item>
      <title>机器学习基石-数学篇：When Can Machines Learn?</title>
      <link>https://jifan.tech/posts/2020-06-30-mlf-mf-ntu-1/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-06-30-mlf-mf-ntu-1/</guid>
      <description>Course Link ：
 Week 1 - The Learning Problem Week 2 - Learning to Answer Yes/No Week 3 - Types of Learning Week 4 - Feasibility of Learning   1 The Learning Problem  What machine learning is and its connection to applications and other fields &amp;hellip;
 1.1 Course Introduction   Machine Learning: a mixture of theoretical and practical tools
 theory oriented  derive everything deeply for solid understanding less interesting to general audience   techniques oriented  flash over the sexiest techniques broadly for shiny converge too many techniques, hard to choose, hard to use properly   **Our approach: **foundation oriented    Foundation Oriented ML Course</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（10）</title>
      <link>https://jifan.tech/posts/2020-06-25-ml-ng-10/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-06-25-ml-ng-10/</guid>
      <description>Course Link ：Week10 - Large Scale Machine Learning &amp;amp; Week11 - Application Example: Photo OCR
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 34 Gradient Descent with Large Datasets 34.1 Learning With Large Datasets   If you look back at 5-10 year history of machine learning, ML is much better now because we have much more data
 However, with this increase in data comes great responsibility? No, comes a much more significant computational cost New and exciting problems are emerging that need to be dealt with on both the algorithmic and architectural level    One of best ways to get high performance is take a low bias algorithm and train it on a lot of data</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（9）</title>
      <link>https://jifan.tech/posts/2020-06-24-ml-ng-9/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-06-24-ml-ng-9/</guid>
      <description>Course Link ：Week9 - Anomaly Detection &amp;amp; Recommender Systems
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 28 Density Estimation 28.1 Problem Motivation   Anomaly detection is a reasonably commonly used type of machine learning application
 Can be thought of as a solution to an unsupervised learning problem But, has aspects of supervised learning    What is anomaly detection?
 Imagine you&amp;rsquo;re an aircraft engine manufacturer As engines roll off your assembly line you&amp;rsquo;re doing QA  Measure some features from engines (e.</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（8）</title>
      <link>https://jifan.tech/posts/2020-06-22-ml-ng-8/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-06-22-ml-ng-8/</guid>
      <description>Course Link ：Week8 - Unsupervised Learning &amp;amp; Dimensionality Reduction
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 24 Clustering 24.1 Unsupervised Learning: Introduction  Unsupervised learning: learning from unlabeled data Supervised learning &amp;amp; unsupervised learning: Compare and contrast:  Supervised learning:  Given a set of labels, fit a hypothesis to it   Unsupervised learning:  Try and determining structure in the data Clustering algorithm groups data together based on data features     What is clustering good for  Market segmentation: group customers into different market segments Social network analysis: Facebook &amp;ldquo;smartlists&amp;rdquo; Organizing computer clustering and data centers for network layout and location Astronomical data analysis: understanding galaxy formation    24.</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（7）</title>
      <link>https://jifan.tech/posts/2020-06-20-ml-ng-7/</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-06-20-ml-ng-7/</guid>
      <description>Course Link ：Week7 - Support Vector Machines
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 21 Large Margin Classification 21.1 Optimization Objective  So far, we&amp;rsquo;ve seen a range of different algorithms  With supervised learning algorithms - performance is pretty similar  What matters often is:  The amount of training data Skill of applying algorithms       One final supervised learning algorithms that is widely used - Support Vector Machine (SVM)  Compared to both logistic regression and neural networks, a SVM sometimes gives a cleaner way of learning non-linear functions Later in the course we&amp;rsquo;ll do a survey of different supervised learning algorithms      Start with logistic regression, see how we can modify it to get the SVM</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（6）</title>
      <link>https://jifan.tech/posts/2020-03-22-ml-ng-6/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-03-22-ml-ng-6/</guid>
      <description>Course Link ：Week6 - Advice for Applying Machine Learning &amp;amp; Machine Learning System Design
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 16 Evaluating a Learning Algorithm 16.1 Deciding What to Try Next   Debugging a learning algorithm:
  Suppose you have implemented regularized linear regression to predict housing prices.
$$ J(\theta) = \frac{1}{2m}\left[\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^n\theta_j^2 \right] $$
    However, when you test your hypothesis on a new set of houses, you find that it makes unacceptably large errors in its predictions.</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（5）</title>
      <link>https://jifan.tech/posts/2020-03-14-ml-ng-5/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-03-14-ml-ng-5/</guid>
      <description>Course Link ：Week 5 - Neural Networks: Learning
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 14 Cost Function and Backpropagation 14.1 Cost Function   Neural Network (Classification)
 Training Set: $\lbrace \left( x^{(1)}, y^{(1)} \right), \left( x^{(2)}, y^{(2)} \right), \dots, \left( x^{(m)}, y^{(m)} \right) \rbrace$  $L$ = total number of layers in the network $s_l$ = number of units (not counting bias unit) in layer $l$ $K$ = number of output units/classes   Binary classification: $y = 0 \text{ or } 1$ (1 output unit) Multi-class classification (K classes): $y \in \mathbb{R}^K$ (K output units)    Cost function</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（4）</title>
      <link>https://jifan.tech/posts/2020-03-10-ml-ng-4/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-03-10-ml-ng-4/</guid>
      <description>Course Link ：Week 4 - Neural Networks: Representation
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 11 Motivations 11.1 Non-linear Hypotheses Neural Networks is a much better way to learn complex non-linear hypotheses even when your input feature space $(n)$ is large.
11.2 Neurons and the Brain  Origins: Algorithms that try to mimin the brain. Was very widely used in 80s and early 90s; popularity diminished in late 90s. Recent resurgence: State-of-the-art technique for many applications.</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（3）</title>
      <link>https://jifan.tech/posts/2020-03-04-ml-ng-3/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-03-04-ml-ng-3/</guid>
      <description>Course Link ：Week 3 - Logistic Regression
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 7 Classification and Representation 7.1 Classification   The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.
  For now, we will focus on the binary classification problem.
  $y \in {0,1}$ - the variable that we&amp;rsquo;re trying to predict</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（2）</title>
      <link>https://jifan.tech/posts/2020-02-26-ml-ng-2/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-02-26-ml-ng-2/</guid>
      <description>Course Link ：Week 2 - Linear Regression with Multiple Variables
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 5. Multivariate Linear Regression 5.1 Multiple Features  Training set:  Notation:  $m$ = the number of training examples $n$ = number of features $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$). $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$).</description>
    </item>
    
    <item>
      <title>机器学习-吴恩达：学习笔记及总结（1）</title>
      <link>https://jifan.tech/posts/2020-02-26-ml-ng-1/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jifan.tech/posts/2020-02-26-ml-ng-1/</guid>
      <description>Course Link ：Week 1 - Introduction
Code : Fang-Lansheng/Coursera-MachineLearning-Python
 0. 写在课前  作为机器学习入门课程中的经典，吴恩达教授的课程具有如下特点：  数学推导过程少，偏重基础，门槛较低 课程短小精悍，讲解清晰，覆盖面广 主要用 Matlab 完成课后程序作业   课程学习目标：  理解机器学习的基本思想和方法 用 Python 实现课程要求掌握的算法    1. Introduction 1.1 Welcome   Machine Learning
 Grew out of work in AI New capability for computers    Examples:
  Database mining
  Large datesets from growth of automation/web
E.g., Web click data, medical records, biology, engineering</description>
    </item>
    
  </channel>
</rss>
